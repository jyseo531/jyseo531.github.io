[
  
  {
    "title": "Graphics-Ch8. Graphics Pipeline",
    "url": "/posts/ch8-Graphics_pipeline/",
    "categories": "Graphics, Study",
    "tags": "Graphics",
    "date": "2025-02-15 12:04:00 +0900",
    





    
    "snippet": "Intro  Graphics Pipeline : object에서 시작해서 pixel이 업데이트되기까지의 일련의 operations sequence  object-order rendering = rendering by “rasterization”  그래픽스 파이프라인 과정들 중에서 “vertex processing”, “Rasterization”, “F...",
    "content": "Intro  Graphics Pipeline : object에서 시작해서 pixel이 업데이트되기까지의 일련의 operations sequence  object-order rendering = rendering by “rasterization”  그래픽스 파이프라인 과정들 중에서 “vertex processing”, “Rasterization”, “Fragment Processing”, “Blending” 이렇게 핵심 네 스텝에 대해서 이번 단원에 다루도록 하겠다          Vertex processing : vertices(꼭짓점)들이 연산되는 과정      Rasterziation stage : 그 꼭짓점들을 이용한 primitives가 보내져서 연산되고, 각각의 Primitive를 fragment로 쪼갬      Fragment processing : 쪼개진 fragment들이 연산되는 과정      Blending : 다양한 Fragment들이 대응되는 픽셀에 따라서 혼합됨      1. Rasterization  그래픽스 파이프라인 중 가장 핵심이 되는 과정  rasterizer는 다음의 두 가지 일을 수행함          픽셀을 순회함, 각 픽셀은 primitive로 덮여있음      그 primitive들을 보간(Interpolate)함      1.1. Line Drawing  오든 그래픽스 패키지들은 line drawing관련된 명령을 포함하고 있음  시작점 $(x_0, y_0)$과 끝점 $(x_1, y_1)$을 연결하는 직선 그리기  크게 implicit line equation을 이용하는 방법과 parametric한 line equation을 이용하는 방법으로 2가지 방법이 있음1.1.1. Implicit Line Equations  Midpoint alogrithm 이용: 직선의 방정식 $f(x,y)$가 이 midpoint보다 위에 있는지 아래에 있는지에 따라서 픽셀 선택하는 알고리즘  시작점 끝점을 잇는 직선의 방정식 $f(x,y)$일 떄, 기울기 $m = \\frac{y_1 - y_o}{x_1 - x_0}$  직선 위에 있는 점들에 대해서는 $f(x,y)=0$을 만족한다는 성질 이용  $(x, y)$에서 다음으로 이동할 픽셀의 후보군(아래 그림에서 주황색 픽셀들)을 다음의 두 가지로 설정함 : $(x+1, y)$ 또는 $(x+1, y+1)$          평행하게 오른쪽 한 칸으로 움직이거나, 대각선 방향으로 이동하는 경우의 수        Midpoint = $(x+1, y+0.5)$  $f$가 midpoint보다 above한지, below한지 분류하는 방법       : $d = f(x+1, y+0.5)$라고 할 때,        1. if $d &lt; 0$이라면          대각선 방향으로 즉, $(x+1, y+1)$ 으로 픽셀 결정       2. else $d &gt; 0$이라면,           평행한 방향으로 즉, $(x+1)$ 으로 픽셀 결정1.2. Triangle Rasterization  2D points로 2D 삼각형 그리는 방법          Given points $p_0 = (x_0, y_0)$ , $p_1 = (x_1, y_1)$, $p_2 = (x_2, y_2)$ in screen space      각 꼭짓점이 color $c_0, c_1, c_2$를 저장하고 있다는 가정하에, 삼각형 위의 한 점을 $(\\alpha, \\beta, \\gamma)$계수르 이용하여 표현 가능 ( barycentric 좌표계를 이용):     \\[\\begin{align} c = \\alpha c_0 + \\beta c_1 + \\gamma c_2. \\end{align}\\]          위 방법의 interpolation을 “Gouraud interpolation“(고러드 보간, 고러드 쉐딩이랑 차이점 공부하기)이라고도 부름        line drawing이랑 삼각형 rasterization의 미묘한 차이점중에 하나는 vertices and edges에 있다.          “Hole problem”: 인접한 삼각형을 그릴 떄, hole이 생기면 안됨      “Order problem” : 만약 인접한 삼각형들이 다른 색상을 갖고 있다면 그려지는 순서에 따라서 이미지 색상이 달라짐      이 두가지 문제를 완화하기 위해, 삼각형의 center가 삼각형 내부에 존재할때만 픽셀을 그리는 성질을 이용함 (당연한거 아님?)  같은 말로, barycentric coordinate상에서 삼각형의 중심점(center)이 (0,1)간격 사이에 존재해야 한다는 의미      만약, center가 삼각형의 edge에 정확하게 존재하면 어떻게 하는가?? 이 질문에 대한 내용은 후에 다룬다    Barycentric Coordinate을 통해 삼각형같은 다면체의 “꼭짓점”이 주어지는 상황에서 어떤 위치의 픽셀에 그릴지와 그 픽셀의 색상또한 결정하는 key가 된다.  모든 픽셀에 대해서 고러드 보간을 이용하면 계산 복잡성이 크기 때문에, 아래의 간단한 방법으로 효율성 증가시킬 수 있음          삼각형의 세 꼭짓점을 기준으로 한 bounding rectangle을 찾아서 이 직사각형 내부에 존재하는 후보 픽셀들에 대해서만 계산 Looping을 진행한다      알고리즘x_min = floor(x_i)x_max = ceiling(x_i)y_min = floor(y_i)y_max = ceiling(y_i)for y in range(y_min, y_max + 1):    for x in range(x_min, x_max + 1):        α = f_12(x, y) / f_12(x_0, y_0)        β = f_20(x, y) / f_20(x_1, y_1)        γ = f_01(x, y) / f_01(x_2, y_2)        if α &gt; 0 and β &gt; 0 and γ &gt; 0:s            c = α * c_0 + β * c_1 + γ * c_2            drawpixel(x, y, color=c)      여기서 $f_{ij}$는 꼭짓점을 잇는 직선의 방정식(line drawing 섹션)이고 아래처럼 표현함\\(\\begin{align} f_{01}(x,y) &amp;= (y_0 - y_1)x + (x_1 - x_0)y + x_0y_1 - x_1y_0 , \\\\                f_{12}(x,y) &amp;= (y_1 - y_2)x + (x_2 - x_1)y + x_1y_2 - x_2y_1 , \\\\                f_{20}(x,y) &amp;= (y_2 - y_0)x + (x_0 - x_2)y + x_2y_0 - x_0y_2 . \\end{align}\\)      Dealing with Pixels on Triangle Edges  삼각형의 중심(center)이 Edge에 정확하게 존재하는 경우의 픽셀은 어떻게 결정할 것인지에 대한 내용  삼각형 중심이 삼각형 변에 위치하는 대표적인 예시가 직각-삼각형임 \"off-screen point\"는 두 삼각형의 공유된 변의 정확히 한쪽에 위치하며,  우리가 그릴 edge는 바로 그 변임,  공유 변 위에 있지 않은 정점들은 해당 변 기준으로 서로 반대쪽에 위치한다.   따라서, 그림 상 a 또는 b 정점 중 하나는 off-screen point와 shared edge기준 같은 부호면을 가진다는 성질이용  $pq &gt; 0$알고리즘y_max = ceiling(y_i)f_α = f_12(x_0, y_0)f_β = f_20(x_1, y_1)f_γ = f_01(x_2, y_2)for y in range(y_min, y_max + 1):    for x in range(x_min, x_max + 1):        α = f_12(x, y) / f_α        β = f_20(x, y) / f_β        γ = f_01(x, y) / f_γ        if α &gt;= 0 and β &gt;= 0 and γ &gt;= 0:            if (α &gt; 0 or f_α * f_12(-1, -1) &gt; 0) and \\  # 부호               (β &gt; 0 or f_β * f_20(-1, -1) &gt; 0) and \\  # 부호               (γ &gt; 0 or f_γ * f_01(-1, -1) &gt; 0):       # 부호                c = α * c_0 + β * c_1 + γ * c_2                drawpixel(x, y, color=c)1.3. CLipping  눈의 뒤(behind)에 존재하는 공간 혹은 view volume의 바깥에 존재하는 primitive를 clipping해주는 과정들이 있어야 정확한 Rasterizing을 수행할 수 있다.  삼각형에서 두 정점은 view volume 내부에 존재하지만, 하나의 정점이 behind the eye에 있다는 상황을 생각해보자.          perspective transformation을 하게 되면, depth $z$가 $z’$로 변환되고 부호도 반대로 바뀜      이떄 모든 정점이 view volume에 있지 않는다면 제대로 transformation이 이루어지지 않아서 삼각형 모양도 변형되는 incorrect result가 초래됨        Clipping : removes part of primitives that could extend behind the eye (시야에 보이지 않는 primitives들을 제거하는 역할)  transform되기 이전 단계의 6개의 평면을 이용하여 표현된 world coordinate들에서 수행되는 clipping module  homogeneous coordinate을 이용하여 4D transform된 space에서 수행되는 clipping module          위의 두 가지 옵션을 통해 clipping 작업이 수행된다.      1.4. Clipping against a Plane      평면의 방정식 : $f(\\mathrm{p}) = \\mathrm{n} \\cdot (\\mathrm{p}-\\mathrm{q}) = 0$ 이 implicit equation을 다시 쓰면 아래처럼 재표현 가능:\\[\\begin{align} f(\\mathrm{p}) = \\mathrm{n} \\cdot \\mathrm{p} + D = 0, \\end{align}\\]    points $a$와 $b$를 잇는 line segment가 있는 상황에서, 이 선분은 평면에 의해 clipping되어짐  평면 $f(\\mathrm{p}) &gt; 0$ 이면 평면의 내부(inside)에 존재하는 것이고, $f(\\mathrm{p}) &lt; 0$이면 평면의 외부(outside)에 존재하는 것임          clipping된 선분의 양끝 지점에 대한 부호가 다르다는 성질        선분과 평면이 만나는 intersection point $\\mathrm{p}$ ($\\mathrm{p} = a + t(b-a)$)에서의 $f(\\mathrm{p}) = 0$이라는 성질 이용해서 아래 그림처럼 교차점에서의 $t$값을 도출할 수 있다.2. Operations Before and After Rasterization  Graphics pipeline을 recall          Rasterization 이전에 “Vertex processing” 단계를 통해 geometry들이 적절하게 transformed되어 primitives로 도출되어야 함                  이 때, viewing transformations을 이용해서 world coordinate에서 screen space로 변환되는 정보들 + colors, surface normals, texture coordinate 같은 정보들도 변환되어야함          전자는 chapter 7에서 다루었기 때문에 후자의 내용을 이번 챕터에서 어떻게 변환시키는지 다루도록 한다.                    Rasterization 이후에는 “Fragment processing” 단계를 통해 rasterization으로 나온 interpolated color와 depth를 그대로 통과시키던가,아니면 복잡한 shading operation으로 각각의 fragment에 대한 color와 depth를 계산한다.      그 후, “Blending” 단계를 통해 각 픽셀마다 겹쳐있는 primitive에서 final color를 혼합시켜 결정한다.                  가장 쉽고 흔한 방법으로는 depth가 가장 작은, 즉 eye와 가장 가까운 fragment의 color로 선택하는 것                    2.1. Simple 2D Drawing  가장 간단한 파이프라인은, vertex와 fragments stage, 그리고 blending stage 에서 아무것도 수행되지 않고 오로지 rasterization에서 pixel coordinate에 직접적으로 연산들이 모두 수행되는 것2.2. A Minimal 3D Pipeline  3D 공간에 물체를 그리기 위해서는 2D Drawing pipeline에서 matrix transformation이 포함되어 vertex processing이 이루어진다.          vertex-processing에서 인풋으로 들어오는 vertex position에다가 camera, projection, 그리고 viewport transformation같은 행렬을 곱함으로써 행렬 변환이 수행됨            이 결과로 삼각형이 screen space에 띄워질 수 있고, 이것은 2D상에 직접적으로 그려지는 것과 같은 결과임    Occlusion problem : back-to-front 순서로 물체들을 그리지 않으면, 더 가까이 있는 물체가 멀리 있는 물체에 가려지는 incorrect result가 초래됨          이 back-to-front 순서로 물체 그리는 걸 Painter’s algorithm이라고도 부름      hidden surface를 제거하는 가장 타당한 방법임      하지만, Occlusion cycle같은 어떤 물체가 더 앞에 있고 뒤에있는지 이 상대적인 깊이를 평가할 수 없는 경우가 존재함                  이럴때는 painter’s algorithm으로 back-to-front order를 설정할 수가 없다.          또한, depth 기준으로 primitives를 정렬하는 것은 시간소요적이라서 큰 씬에 대해서는 애초에 painter’s algorithm을 사용하기가 거의 어려움                     2.3. Using a Z-Buffer for Hidden Surfaces  앞선 occlusion problem에서 필요한 depth sorting이 시간소요적이라는 측면에서, painter’s algorithm이 거의 사용되지 않기 떄문에 나온 방법  아이디어 = 각 픽셀에서 지금까지 렌더링된 가장 가까운 표면까지의 거리(depth)를 기록하고, 그 거리보다 더 멀리 있는 fragment는 폐기한다.          이 거리를 RGB color 세 값에 추가적으로 buffer를 두어 z값을 저장하고 z차원에서 grid를 생성함        buffer algorithm은 Fragment blending phase에서 구현됨  현재 z-buffer에 저장된 depth value랑 각 fragment별로 depth를 비교하면서, 만약 해당 fragment value값이 더 작다면 color와 depth value를 덮어씌운다.2.4. Per-vertex Shading  rasterization으로 interpolated된 color와 계산된 depth로만 3D 공간에서 object를 나타내는 것으로 충분할 수도 있지만, 대부분은 objects with shading을 구현하고 싶어함  light direction, eye direction, surface normal 같은 조명과 관련한 변수들 필요로 함  vertex stage에서 shading computation을 수행하는 방법 소개          “고러드 쉐이딩”      카메라 poistion, lights, vertex에 의해서 빛의 방향과 viewer의 gaze direction이 계산되는 방법      이 shading equation을 통해 계산된 vertex color는 rasterizer에게 넘어감 (즉, rasterizer 이전 단계에서 수행되는 shading equation이다)        각각의 vertex에서 shading을 다루고 vertex끼리는 다루지 않아서 아무래도 디테일이 좀 떨어진다는 단점이 있다.2.5. Per-fragment Shading  Rasterization 이후에 나온 interpolated color를 이용해서 fragment stage에서 수행되는 shading  “Phong shading” (Phong illumination model이랑 다른 개념임)  shading equation자체는 똑같지만, 정점 각각에 행해지는 것이 아니라 rasterziation으로 나온 보간된 색상을 이용한 fragment각각에 수행된다는 점이 다름          이것을 위해서 vertex stage 좌표계가 fragment stage와 일련되게 존재해야 데이터가 적절하게 사용될 수 있음      2.6. Texture Mapping  Texture에 대한 디테일은 chapter-11에서 더 자세하게 다룰 예정  Textures : 표면의 음영(shading)에 추가적인 디테일을 더하기 위해 사용되는 이미지로, 추가되지 않으면 지나치게 균일하고 인공적으로 보이게 될 수 있음2.7. Shading Frequency  shading computation들을 어떤 스텝에 위치시킬지를 color change가 얼마나 빠른지에 따라 결정할 수 있다.  이 변하는 정도를 “scale”로 확인할 수 있음          large-scale features(e.g., diffuse shading(난반사된 빛) on curved surfaces)                  low shading frequency로 계산되어야 함          vertex stage                    small-scale features(e.g., sharp highlightsor detailed textures)                  high shading frequency로 계산됨          vertex stage  &amp; fragment stage 모두 가능                    simple anti-aliasing, culling primitives 내용 교재 참고"
  },
  
  {
    "title": "Graphics-Ch7. Viewing",
    "url": "/posts/ch7-Viewing/",
    "categories": "Graphics, Study",
    "tags": "Graphics",
    "date": "2025-02-13 12:04:00 +0900",
    





    
    "snippet": "Intro  3D to 2D mapping을 통해 object를 3d location과 2D view 사이에서 이동시키는 것을 “viewing Transformation“이라고 함  object-order rendering에서 중요함  ch4(Ray-Tracing)단원에서 orthographic(paralled) view와 perspective vie...",
    "content": "Intro  3D to 2D mapping을 통해 object를 3d location과 2D view 사이에서 이동시키는 것을 “viewing Transformation“이라고 함  object-order rendering에서 중요함  ch4(Ray-Tracing)단원에서 orthographic(paralled) view와 perspective view에서 vieweing ray가 어떻게 생성되는지에 대해 간단하게 다루었음          ray-tracer는 ray와 만나는 가장 가까운 교차점에 대한 surface를 찾는 것이고, object-order renderer는 solid-looking object에서 어떤 표면이 screen space(pixcel space)의 어떤 점들에 매칭되어 그려지는지에 대해 다루는 것      1. Viewing Transformations  Viewing Transformation을 통해 Canonical Coordinate System에서의 $(x, y, z)$로 표기되는 3D location을 이미지 픽셀 단위의 screen space 매핑하는 것이 필요함          고려되어야 할 것 : projection의 종류, FOV(Field of View, 시야각), 이미지 해상도 등        아래 세 가지의 transformation 과정으로 World Space에 있는 물체가 Screen Space로 View Transformed된다.          Camera Transformation (Eye Transformation) :                  camera의 pose(position, oriendtation)에만 의존          World Coordinate -&gt; Camera Coordinate                    Projection Tranformation :                  projection 종류에 의해서만 의존          Camera space -&gt; Canonical View Volume          -1에서 +1 범위의 points                    Viewpoint Transformation (Windowing Transformation) :                  output 이미지의 size와 position에 의존          Canonical view volume -&gt; Screen space                    1.1. The Viewport Transformation  canonical view volume에 있는 view를 가정해아 함, input처럼 생각      $(x, y, z) \\in [-1, +1]^3$\\[\\begin{bmatrix} x_{\\text{screen}} \\\\ y_{\\text{screen}} \\\\ 1 \\end{bmatrix} =  \\begin{bmatrix} \\frac{n_x}{2} &amp; 0 &amp; \\frac{n_x - 1}{2} \\\\ 0 &amp; \\frac{n_y}{2} &amp; \\frac{n_y - 1}{2} \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix}  \\begin{bmatrix} x_{\\text{canonical}} \\\\ y_{\\text{canonical}} \\\\ 1 \\end{bmatrix} .\\]    Viewport Transformation Matrix :          앞선 식의 변환행렬에다가 $z$ coordinate의 행과 열에 $ [0 \\quad 0 \\quad 0 \\quad 1]$ 를 추가시킴      \\[\\begin{align}M_{\\text{vp}} = \\begin{bmatrix}  \\frac{n_x}{2} &amp; 0 &amp; 0 &amp; \\frac{n_x - 1}{2} \\\\  0 &amp; \\frac{n_y}{2} &amp; 0 &amp; \\frac{n_y - 1}{2} \\\\  0 &amp; 0 &amp; 1 &amp; 0 \\\\  0 &amp; 0 &amp; 0 &amp; 1 \\end{bmatrix}.\\end{align}\\]1.2. Orthographic Projection Transformation  canonical view volume 좌표계로 옮기는 변환 과정  input : camera coordinate (orthographic view)  orthographic view volume을 아래의 3D space로 정의할 수 있음   = $[l,r]$ x $[b,t]$ x $[f,n]$          좌우, 상하, 앞뒤 절단면을 정의하는 클리핑 파라미터        아래 그림처럼 orthorgraphic view volume에서 $-z$방향으로 바라보고 있다고 가정하기 때문에(상황마다 다르게 설정가능하긴 하다) $f$(far)가 $n$(near)보다 작은 값임  \\[\\begin{align}M_{\\text{orth}} =  \\begin{bmatrix}  \\frac{2}{r - l} &amp; 0 &amp; 0 &amp; -\\frac{r + l}{r - l} \\\\  0 &amp; \\frac{2}{t - b} &amp; 0 &amp; -\\frac{t + b}{t - b} \\\\  0 &amp; 0 &amp; \\frac{2}{n - f} &amp; -\\frac{n + f}{n - f} \\\\  0 &amp; 0 &amp; 0 &amp; 1  \\end{bmatrix}.\\end{align}\\]      camera space(orthographic view volume) =&gt; screen space(image pixel)로의 최종 변환 수식은 아래와 같음:\\[\\begin{align}  \\begin{bmatrix}  x_{\\text{pixel}} \\\\ y_{\\text{pixel}} \\\\ z_{\\text{canonical}} \\\\ 1 \\end{bmatrix} = (M_{\\text{vp}}M_{\\text{orth}}) \\begin{bmatrix}x \\\\ y \\\\ z \\\\ 1\\end{bmatrix}.\\end{align}\\]  1.3. The Camera Transformation  World(object) space 에서 Camera Space($uvw$-space)로의 변환 과정새롭게 정의되는 변수 3가지 = {eye position $e$, gaze direction $g$, view-up vector $t$} + 기저 벡터 ${uvw}$  이 정보들로 왼쪽처럼 coordinate system 세팅하는데 충분한 정보를 제공함    arbitrary view(world space coordinate $xyz$)으로부터 origin $e$와 기저벡터 $u,v,w$로 표현된 카메라 좌표 시스템으로 저장하는 것이 목표          origin $e$로의 translation &amp; u,v,z로의 scaling matrix 처럼 생각하면 됨 (왼에서 오로 곱해지는 그거)            아래의 변환 수식을 통해 수행됨:\\[\\begin{align} M_{\\text{cam}} =\\begin{bmatrix}\\mathbf{u} &amp; \\mathbf{v} &amp; \\mathbf{w} &amp; \\mathbf{e} \\\\0 &amp; 0 &amp; 0 &amp; 1\\end{bmatrix}^{-1}=\\begin{bmatrix}x_u &amp; y_u &amp; z_u &amp; 0 \\\\x_v &amp; y_v &amp; z_v &amp; 0 \\\\x_w &amp; y_w &amp; z_w &amp; 0 \\\\0 &amp; 0 &amp; 0 &amp; 1\\end{bmatrix}\\begin{bmatrix}1 &amp; 0 &amp; 0 &amp; -x_e \\\\0 &amp; 1 &amp; 0 &amp; -y_e \\\\0 &amp; 0 &amp; 1 &amp; -z_e \\\\0 &amp; 0 &amp; 0 &amp; 1\\end{bmatrix}. \\end{align}\\]    최종적인 Viewing Transformation 알고리즘 :            Matrix Name      Description                  M_vp      Viewport Matrix              M_orth      Orthographic Matrix              M_cam      Camera Matrix              M      $( M = M_{\\text{vp}} M_{\\text{orth}} M_{\\text{cam}} )$      Algorithm  Construct ( $M_{\\text{vp}}$ )  Construct ( $M_{\\text{orth}}$ )  Construct ( $M_{\\text{cam}}$ )  Compute ( $M = M_{\\text{vp}} M_{\\text{orth}} M_{\\text{cam}}$ )  For each line segment ( (a_i, b_i) ):          Compute ( $p = M_{a_i}$ )      Compute ( $q = M_{b_i}$ )      Draw line from ( $(x_p, y_p)$ ) to ( $(x_q, y_q)$ )      2. Projective Transformations  앞선 1.2.단계의 orthographic veiw volume(camera space)에서 canonical view volume으로 변환 하는 단계에서 사용하는 transformation 종류들에 대해서 알아보자.  주요 특성(key property) :          screen에 보이는 object의 size는 $-z$축 방향으로 보여지는 camera space와의 거리(depth) $z$의 역수에 비례함            Affine Transformation(어파인 변환)으로는 분모로 z(input vector의 한 요소)를 나누는 이러한 연산은 구현할 수가 없다    Homogeneous Coordinate 메커니즘을 활용함          마지막 원소를 1로 두고 차원을 하나 확장시키는 좌표계 $(x,y,z) -&gt; (x,y,z,1)$      여기서 1을 $(x,y,z)$좌표의 분포로 생각할 수 있다.        Affine 변환을 통해 변환된 point $x’ = ax + by + cz +d$3. Perspective Projection      matrix transformation : \\(\\begin{bmatrix} y_s \\\\ 1 \\end{bmatrix}  \\sim \\begin{bmatrix} d &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\end{bmatrix}  \\begin{bmatrix} y \\\\ z \\\\ 1 \\end{bmatrix}.\\)        Perspective Matrix $P$ : \\(\\begin{align} \\mathbf{P} =\\begin{bmatrix}n &amp; 0 &amp; 0 &amp; 0 \\\\0 &amp; n &amp; 0 &amp; 0 \\\\0 &amp; 0 &amp; n + f &amp; -f n \\\\0 &amp; 0 &amp; 1 &amp; 0\\end{bmatrix}.\\end{align}\\)    perspective matrix를 orthograpic 시스템에 병합하기 위해서, $M_{per} = M_{orth} P.$를 이용하여 아래와 같은 transforamtion matrix 로 분해할 수 있다:  \\[\\begin{align} M = M_{vp} (M_{orth} P) M_{cam} \\end{align}\\]          \\[\\begin{align} M_{\\text{per}} =  M_{orth} P =  \\begin{bmatrix}  \\frac{2n}{r - l} &amp; 0 &amp; \\frac{l + r}{l - r} &amp; 0 \\\\  0 &amp; \\frac{2n}{t - b} &amp; \\frac{b + t}{b - t} &amp; 0 \\\\  0 &amp; 0 &amp; \\frac{f + n}{n - f} &amp; \\frac{2fn}{f - n} \\\\  0 &amp; 0 &amp; 1 &amp; 0  \\end{bmatrix}. \\end{align}\\]            5. Field-of-View(FOV)  한국말로 시야각이라는 의미  $\\theta$로 표현 = the angle from the bottom of the screen to the top of the screen as measured from the eye\\[\\begin{align} tan\\frac{2}{\\theta} = \\frac{t}{|n|}  \\end{align}\\]"
  },
  
  {
    "title": "Mip-NeRF 360, CVPR 2022",
    "url": "/posts/MipNerf360/",
    "categories": "Paper Review",
    "tags": "Surface reconstruction, Graphics",
    "date": "2025-02-10 12:30:00 +0900",
    





    
    "snippet": "Mip-NeRF 360: Unbounded Anti-Aliased Neural Radiance Fields  depth distortion loss의 시초          현재 GS기반 Surface Reconstruction 정규화 텀들에 많이 사용함      Abstract  “unbounded scene” : 학습된 데이터의 방향(directio...",
    "content": "Mip-NeRF 360: Unbounded Anti-Aliased Neural Radiance Fields  depth distortion loss의 시초          현재 GS기반 Surface Reconstruction 정규화 텀들에 많이 사용함      Abstract  “unbounded scene” : 학습된 데이터의 방향(direction)과 거리(distance)와 무관하게 모든 ray상에 존재하는 point에 대해 잘 합성된 scene          주로 전경 물체 뿐만아니라 배경까지 있는 scene을 의미함        NeRF(Neural Radiance Fields)          volumentric density 랑 color를 MLP를 통해 합성함      MLP : ray상의 작은 3D point들을 이용        Critical Issues          Parameterization      Efficiency      Ambiguity      Preliminaries : mip-NeRF  ray : $\\mathrm{r}(t) = \\mathrm{o} + t\\mathrm{d}$ 가 있을 때,  distance $t$에 의해서 정렬된 벡터가 정의되고 ray가 t에 대한 구간의 집합으로 쪼개진다 $T_i = [t_i, t_{i+1}]$  각 구간마다 원뿔대(conical frustum)의 평균과 공분산$(\\mu, \\Sigma)=\\mathrm{r}(T_i)$을 계산함          ray의 focal length와 image plane의 픽셀사이즈에 의해서 결정됨              featureize 단계(IPE, Integratged positional Encoding)\\[\\gamma(\\mu, \\Sigma) =\\left\\{\\begin{bmatrix}    \\sin(2\\ell \\mu) \\exp\\left(-2^ {2\\ell-1} \\operatorname{diag}(\\Sigma)\\right) \\\\    \\cos(2\\ell \\mu) \\exp\\left(-2^ {2\\ell-1} \\operatorname{diag}(\\Sigma)\\right)    \\end{bmatrix}    \\right\\}_{\\ell=0}^{L-1}\\]              위 값들이 MLP의 input으로 들어가고, MLP weight인 $\\Theta_{NeRF}$와 같이 들어가서 density $\\tau$와 color $\\mathrm{c}$를 출력함 :          \\[\\forall T_i \\in t, \\quad (\\tau_i, c_i) = \\text{MLP}(\\gamma(r(T_i)); \\Theta_{\\text{NeRF}})\\]              view direction $\\mathrm{d}$도 MLP에 같이 들어감      Volume rendering을 통해 렌더링되는 최종 pixel color $\\mathrm{C}(\\mathrm{r}, t)$ :\\[\\begin{align} \\mathrm{C}(\\mathrm{r}, t) = \\sum_limits_i w_i c_i \\\\     w_i = (1-e^{-\\tau_i(t_{i+_1} - t_i)})e^{-\\sum_{i' &lt; i}\\tau_{i'}(t_{i'+1}-t_{i'})} \\end{align}\\]    NeRF는 두 개의 서로 다른 MLP (“coarse” MLP와 “fine” MLP)를 사용하는 계층적 샘플링 절차를 사용 :          $t^c \\sim U[t_n, t_f], \\quad t^c = \\operatorname{sort}({t^c}) $      $t^f \\sim hist[t^c, w^c], \\quad t^f = \\operatorname{sort}({t^f}) $            final loss (combination of coarse and fine reconstruction loss) :\\[\\sum_{\\mathbf{r} \\in \\mathcal{R}} \\frac{1}{10}   \\mathcal{L}_{\\text{recon}} \\left( \\mathbf{C}(\\mathbf{r}, t^c), \\mathbf{C}^*(\\mathbf{r}) \\right)   + \\mathcal{L}_{\\text{recon}} \\left( \\mathbf{C}(\\mathbf{r}, t^f), \\mathbf{C}^*(\\mathbf{r}) \\right)\\]  1. Scene and Ray Parameterization1.1. 3D coordinate $x$에 대한 Parameterization  기존의 연구들은 unbounded scene에 대해 point를 파라미터화했지만, 본 논문에서는 Gaussian을 재파라미터화하였다  $f(x)$ : smooth coordinate transformation function 또는 state transition model이라고 정의  $f$ 에 대한 선형 근사 = $f(x)≈f(μ)+J_f(μ)(x−μ)$          where, $J$: $\\mu$에서의 f함수에 대한 쟈코비안 행렬    \\[\\begin{align} f(μ,Σ)=(f(μ),J_f(μ)ΣJ_f(μ)⊤) \\end{align}\\]    Extended Kalman Filter이랑 비슷하게 작동함                  Contract 함수 : point x를 입력받아 특정 반지름(1,2)를 기준으로 변형하는 연산\\[\\text{contract}(x) =\\begin{cases} x, &amp; \\text{if } \\|x\\| \\leq 1 \\\\\\left(2 - \\frac{1}{\\|x\\|} \\right) \\left(\\frac{x}{\\|x\\|} \\right), &amp; \\text{if } \\|x\\| &gt; 1\\end{cases}\\]              의미          x가 1보다 큰 경우(그림에서 주황색 범위인 경우)에는 벡터를 방향은 유지하면서 특정 크기로 축소(수축, contract)      변형된 크기는 $2−1∥x∥2−∥x∥1​$ 로 조정됨 , 벡터를 단위 벡터로 변환한 후 크기를 다시 스케일링하는 방식      =&gt; 거리가 멀리 떨어져 있을수록 disparity(시점 차) 즉, distance의 역수만큼 비율적으로 분포하게 해줌      NDC(normalzed device coordinate)와 같은 motivation        본 논문에서는 아래의 식에 따라 유클리드 공간에서 mip-NeRF의 IPE feature encoding($\\gamma$) input으로 축소된 공간에서의 contract함수 반환 값($\\gamma(\\text{contract}(x))$)을 감마 함수에 집어넣는다2. 구간을 나눌 광선 거리 $t$에 대한 Parameterization :  일반적인 NeRF에서는 uniform distribution에서 $t_n$(근거리)과 $t_f$(원거리)사이 구간에서 샘플링한 광선거리 $t^c$를 정렬하여 사용          하지만, Scene parameterization에서 NDC처럼 역할하는 축소된 공간에서의 state transition 과정으로 인해서 실제로는 깊이의 역수(disparity)간격으로 균일하게 샘플들이 배치된다.      이 상황은 카메라 방향이 하나만 존재하는 unbounded scene에는 적합하겠지만, 방향이 다른 모든 뷰에 대한 unbounded scene을 복원하는 데에는 적합하지 않다            유클리디안 공간에 있는 ray distance $t$를 “normalized” ray distance $s$로 역매핑해주는 것이 필요 :\\[s \\triangleq \\frac{g(t) - g(t_n)}{g(t_f) - g(t_n)}, \\quad t \\triangleq g^{-1} \\left( s \\cdot g(t_f) + (1 - s) \\cdot g(t_n) \\right), \\quad (11)\\]          여기서 g는 가역(정규화된 값을 출력해주는)함수      2. Coarse-to-Fine Online Distillation  Mip-NeRF :          MLP에서 “coarse” ray interval $t^c$을 사용하여 한 번 평가하고, 다시 “fine” ray interval $t^f$를 사용하여 평가되며, 두 수준 모두에서 image reconsruction loss를 사용        Mip-NeRF 360 :          두 가지의 MLP학습(NeRF MLP $\\Theta_{NeRF}$ &amp; Proposal MLP $\\Theta_{prop}$)                  NeRF MLP $\\Theta_{NeRF}$ : 기존 Mip-NeRF에서 사용하던 MLP          Proposal MLP $\\Theta_{prop}$ : volumetric density만 에측함, color는 예측하지 않음                          output의 volumetric density가 다시 weight vector $\\hat{w}$로 반환됨              proposal 가중치 $\\hat{w}$는 자체 가중치 벡터 $w$를 예측하는 NeRF MLP에 제공되는 $s$-간격을 샘플링하는 데 사용              입력 이미지를 재현하도록 학습되지 않고 대신 NeRF MLP에 의해 생성된 가중치 $w$를 제한하도록 학습              두 MLP 모두 랜덤하게 초기화되고 공동으로 학습되므로 이 supervision은 NeRF MLP 지식을 proposal MLP에 대한 일종의 “online distillation”로 생각함                                          proposal MLP $(\\hat{\\mathrm{t}}, \\hat{\\mathrm{w}})$와 NeRF MLP $(\\mathrm{t},\\mathrm{w})$의 히스토그램이 일관되도록 장려하는 loss function이 필요                  이 때 두 히스토그램 x축 bin길이가 동일할 필요가 없음, 아니 오히려 달라져야 proposal MLP가 잘 학습되었다는 근거임          따라서 bin이 다를 때의 히스토그램 유사성을 평가하는 통계적 방법론 연구들이 상대적으로 부족하기 때문에 꽤 어려운 문제임                          $\\text{bound}(\\hat{\\mathrm{t}}, \\hat{\\mathrm{w}}, T) = \\sum\\limits_{j : T \\cap \\hat{T}_j neq \\emptyset}\\hat{w_j} .$              만약 두 히스토그램이 서로 일관(일치)하면, $(\\mathrm{t},\\mathrm{w})$의 모든 간격 $(T_i, w_i)$에 대해 $w_i \\leq \\text{bound}((\\hat{\\mathrm{t}}, \\hat{\\mathrm{w}}), T_i)$가 모든 구간에 대해 성립해야 함              이 성질 이용해서 $L_{prop}$ 설계 :            \\[\\begin{align} \\mathcal{L}_{\\text{prop}} (\\mathbf{t}, \\mathbf{w}, \\hat{\\mathbf{t}}, \\hat{\\mathbf{w}}) =\\sum_i \\frac{1}{w_i} \\max \\left( 0, w_i - \\text{bound} (\\hat{\\mathbf{t}}, \\hat{\\mathbf{w}}, T_i) \\right)^2,\\end{align}\\]                                  3. Regularization for Interval-Based Models (Depth distortion loss)  “floater”가 발생하거나 “back-ground collapse”가 발생하는 결함 상황에서 잘 해결할 수 있음      정규화된 ray distance $s$와 blending weight $w$간의 step function으로 구성됨\\[\\begin{align}  \\mathcal{L}_{\\text{dist}} (\\mathbf{s}, \\mathbf{w}) =\\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} w_{\\mathbf{s}}(u) w_{\\mathbf{s}}(v) |u - v| \\, d u \\, d v, \\end{align}\\]          where, w_s(u)는 $u$에서 $(s,w)$에 의해 정의된 step function에 대한 보간이다.      $ w_s(u)=\\sum_i w_i 𝟙[s_i,s_{i+1})(u) $            위 적분 식은 정의는 직관적이지만 계산하는 것이 어렵기 때문에, 아래 식으로 rewritten할 수 있음\\[\\begin{align} \\mathcal{L}_{\\text{dist}} (\\mathbf{s}, \\mathbf{w}) =  \\sum_{i,j} w_i w_j \\left| \\frac{s_i + s_{i+1}}{2} - \\frac{s_j + s_{j+1}}{2} \\right|  + \\frac{1}{3} \\sum_i w_i^2 (s_{i+1} - s_i)  \\end{align}\\]    첫 번째 항은 모든 interval에 대한 midpoint사이의 weighted distances 최소화 term, 두 번째 항은 각각의 interval에 대한 weighted size를 최소화하는 텀이다.느낀점  이전에 나온 모델로 Mip-NeRF(ICLR 2021)가 있는데 ray tracing based volume rendering이 아니라, “Cone” tracing한다는 점이 가장 큰 특징인 것만 알고 읽은 상태라 모든 수식이 매끄럽게 이해되진 않았다  NeRF자체를 제대로 공부해본적이 없었어서 잘 이해가 안되는 게 많긴 하는데 언젠가 공부 급한건 아님"
  },
  
  {
    "title": "Gaussian Opacity Fields, SIGGRAPH Asia 2024",
    "url": "/posts/GOF/",
    "categories": "Paper Review",
    "tags": "Surface reconstruction, Graphics",
    "date": "2025-02-08 11:30:00 +0900",
    





    
    "snippet": "Gaussian Opacity Fields: Efficient Adaptive Surface Reconstruction in Unbounded ScenesAbstract  본 논문의 GOF는 ray-tracing 기반 3D Gaussian의 volume rendering을 통해 직접적으로 geometry를 추출하여 level-set을 확인하는 과정을 ...",
    "content": "Gaussian Opacity Fields: Efficient Adaptive Surface Reconstruction in Unbounded ScenesAbstract  본 논문의 GOF는 ray-tracing 기반 3D Gaussian의 volume rendering을 통해 직접적으로 geometry를 추출하여 level-set을 확인하는 과정을 수행          SuGaR처럼 포아송 재건(Poisson reconstruction)을 사용하지 않으며, 2DGS와 GS2Mesh처럼 TSDF fusion을 사용하지 않음      surface normal을 ray-Gaussian intersection plane(ray와 교차하는 가우시안의 평면)을 이용하여 근사함      geometry extraction method로는 “Marching Tetrahedra“(마칭 큐브아니고 마칭-사면체)방법 사용함      Introduction  NeRF 기반          SDF(Signed Distance Function)와 occupancy 네트워크를 사용하여 surface reconstruction을 수행      foreground object 복원에만 제한적임      e.g., Neuralangelo      NeRF’s opacity Field로부터 real-time rendering 및 surface 추출하는 연구도 진행됨 (e.g., Binary Opacity Grids-BOG)        Marching Cube Algorithm          mesh extraction Algorithm      surface recon자체보다 NVS(novel view synthesis)에 초점이 맞춰져서 고안된 방법이라, 정규화 텀이 부족하고 다소 noisy함        3DGS 기반          SuGaR : surface-align Gaussian들을 얻기 위해 정규화 텀 추가 &amp; depth map으로부터 Poisson reconstruction기반 메쉬추출      2DGS : image plane에 projected된 2D Gaussian의 property이용 + TSDF fusion이용      - 이 방법들은 surface reconstruction 성능 향상이 되었지만, fine-grained geometry를 잡는 것과 background region을 복원하는 것에는 아직 부족하다.        Poisson-Reconstruction 과 TSDF Fusion의 결함          포아송 재건은 Gaussian primitive에 대한 opacity(투명도), scale, rendered depth같은 정보들을 고려하지 않음      TSDF fusion은 얇은 구조물이나 Unbounded scene에 대한 복원 성능이 정확하지 않음      Contributions  Volume Rendering 측면 :          projection-based 방법이랑 다르게, explicit한 ray-Gaussian intersection을 이용해서 volume rendering할 때의 Gaussian의 “기여도”를 결정함      이러한 ray-tracing 으로부터 기반된 formula는 ray상에 존재하는 모든 Point에 존재하는 Gaussian의 opacity를 결정짓게 할 수 있음 //  (여기까진 RaDe-GS에서도 ray-tracing intersection을 이용해서 rasterized method로 가우시안의 primitive들을 풀어냈다는 점이 유사하다.)      “view independence” = 모든 뷰에 대해 opacity가 최소인 값을 취하면 view에 대해 독립적이다(??) : opacity field가 Poisition에 대한 함수를 전적으로 책임짐        Surface normal 측면 :          ray와 Gausssian사이의 intersection plane(교차 평면)에 대한 normal        Surface extraction technique(alogirthm) 측면 :          poisson recon, marching cube와는 다른 메소드      tetrahedra-grids(다면체 그리드)에 기반한 ‘Marching tetrahedra’ 사용함                  3D Gaussian primitive중에서 3D bounding box의 코너값과 중앙값을 이용하여 다면체 메쉬의 꼭짓점 집합으로 구성하는 방법임                    Methods1. Modeling  Given multiple posed + calibrated images      3D scene은 3D Gaussian의 집합 $\\mathcal{G}_k$은 중심점 $\\mathrm{p}_k$, scaling matrix $\\mathrm{S}_k$, 그리고 rotation matrix $\\mathrm{R}_k$으로 파라미터화되고, 이는 쿼터니언(quaternion, 복소수의 확장형태)로 아래처럼 나타내짐 :\\[\\begin{align} \\mathcal{G}_k(\\mathrm{x}) = e^{-\\frac{1}{2}(\\mathrm{x}-\\mathrm{p}_k)\\Sigma_k^{-1}(\\mathrm{x}-\\mathrm{p}_k)} \\end{align}\\]  Ray Gaussian Intersection  ray와 Gaussian이 만나는 intersection의 정도를 이용한다.  RaDe-GS에 있던 내용이랑 겹침, 그래서 어떤 부분이 어디서부터 노벨티인지 분간이 안된다(ray tracing chapter advanced ver공부 더 해야겠다)  “ray intersection” : 1-D Gaussian Function이 최대화되는 점          point $\\mathrm{x} = \\mathrm{o} + t\\mathrm{r}$ , r은 ray direction, t는 ray의 depth            local coordinate system으로 point $\\mathrm{x}$ 을 변환 &amp; scale로 normalize한다\\[\\begin{align} \\mathrm{o}_g &amp;= \\mathrm{S}_k^{-1}\\mathrm{R}_k(\\mathrm{o}-\\mathrm{p}_k) \\\\               \\mathrm{r}_g &amp;= \\mathrm{S}_k^{-1}\\mathrm{R}_k\\mathrm{r} \\\\               \\mathrm{x}_g &amp;= \\mathrm{o}_g + t\\mathrm{r}_g \\end{align}\\]        ray 선 상에 존재하는 depth t에서의 1-D Gaussian value :\\[\\begin{align} \\mathcal{G}_k(\\mathrm{t}) = e^{-\\frac{1}{2}\\mathrm{x}_g^T\\mathrm{x}_g} = e^{-\\frac{1}{2}(\\mathrm{r}_g^T\\mathrm{r}_gt^2 + 2\\mathrm{o}_g^T\\mathrm{r}_gt + \\mathrm{o}_g^T\\mathrm{o}_g)} \\end{align}\\]        위의 가우시안 value가 t에 대한 quadratic term(이차형식)을 포함하고 있으므로 미분 이용해서 최대화되는 지점의 $t^*$을 유도 가능(RaDe-GS review에서 했음) :\\[\\begin{align} t^* = -\\frac{B}{A} \\end{align}\\]    where, $A =\\mathrm{r}_g^T\\mathrm{r}_g ,$  $B = \\mathrm{o}_g^T\\mathrm{r}_g$ 이렇게 구한 ray-Gaussian intersection은 world space에서 바로(directly) 구해질 수 있다는 점에서 surface normal구할 때도 유용한 특성임        “Gaussian $\\mathcal{G}_k$에 대한 기여도(Contribution) $\\mathcal{E}$ 라고 정의함 :\\[\\begin{align} \\mathcal{E}(\\mathcal{G}_k, \\mathrm{o}, \\mathrm{r}) = \\mathcal{G}_k^{1D}(t^*) \\end{align}\\]      Volume Rendering      camera ray상의 pixel color는 Gaussian primitive의 depth로 정렬된 순서에 따라서 alpha blending을 통해 렌더링됨\\[\\begin{align} \\mathrm{c}(\\mathrm{o}, \\mathrm{r}) = \\sum\\limits_{k=1}^{K}\\mathrm{c}_k\\alpha_k\\mathcal{E}(\\mathcal{G}_k, \\mathrm{o}, \\mathrm{r})\\prod\\limits_{i=1}^{k-1}(1 - \\alpha_j \\mathcal{E}(\\mathcal{G}_j, \\mathrm{o}, \\mathrm{r})) \\end{align}\\]  where, $c_k$ : view-dependent color modeled with spherical harmonics and $\\alpha_k$ : additional parameter that influences the opacity of Gaussian $k$.      (tile-based rendering process) 즉 standard 3DGS와 같이 depth 기반 alpha blending방식으로 pixel color rendering하는 방식 사용함      2. Gaussian Opacity Fields      projected 2D Gaussian대신 ray-Gaussian Intersection을 이용하기 때문에, ray에 존재하는 어떠한 점이라도 opacity value($\\mathrm{O}_k(\\mathcal{G}_k, \\mathrm{o}, \\mathrm{r}, t)$)를 구할 수 있다는 장점\\[\\begin{align} \\mathrm{O}_k(\\mathcal{G}_k, \\mathrm{o}, \\mathrm{r}, t) = \\begin{cases}\\mathcal{G}_k^{1D}(t) &amp; \\text{if } t \\leq t^* \\\\\\mathcal{G}_k^{1D}(t^*),  &amp; \\text{if } t  &gt; t^*\\end{cases} \\end{align}\\]        이렇게 구해진 ray상의 opacity value를 이용하여 volume rendering process는 아래 수식처럼 이루어짐 :\\[\\begin{align} \\mathrm{O}(\\mathrm{o}, \\mathrm{r}, t) = \\sum\\limits_{k=1}^{K}\\alpha_k \\mathrm{O}_k(\\mathcal{G}_k, \\mathrm{o}, \\mathrm{r}, t) \\prod\\limits_{i=1}^{k-1}(1 - \\alpha_j \\mathrm{O}_k(\\mathcal{G}_k, \\mathrm{o}, \\mathrm{r}, t)) \\end{align}\\]        이 때 3D point는 모든 뷰에서 보여지는데, 3D point $\\mathrm{x}$의 opacity는 이 모든 학습 뷰에 대한 opacity value 중 최솟값으로 정의한다.\\[\\begin{align} \\mathrm{O(\\mathrm{x})} = \\min\\limits_{(o, r)}\\mathrm{O}(\\mathrm{o}, \\mathrm{r}, t) \\end{align}\\]        위의 $\\mathrm{O(\\mathrm{x})}$를 \"Gaussian Opacity Fields(GOF)\"라고 언급한다.          이 GOF를 이용하면 poisson reconstruction이나 TSDF Fusion없이도 surface를 바로 추출할 수 있다.      by identifying their level sets      본 논문에서 만든 Tetrahedral 기반 메쉬 추출 방법과 연결되어 사용함, 4.섹션에 더 자세히 수록      3. Optimization  기본적으로 2DGS에서 언급된 loss들(depth distortion, normal consistency loss)를 이용하여 정규화함3.1. Depth Distortion loss  Mip-NeRF360 논문에서 처음으로 제안됨      ray-Gaussian intersection들이 더 밀집되고 집중되게 해주는 것 촉진함\\[\\begin{align} L_d = \\sum\\limits_{i,j}w_iw_j|t_i-t_j| \\end{align}\\]                  where $w_i$ : i번째 가우시안의 blending weight \\[w_i = \\alpha_k\\mathcal{E}(\\mathcal{G}_k, \\mathrm{o}, \\mathrm{r})\\prod_{j=1}^{k-1}(1-\\alpha_j\\mathcal{E}(\\mathcal{G}_k, \\mathrm{o}, \\mathrm{r}))\\]              하지만, depth distortion만으로는 Gaussian마다의 거리와 weight를 모두 감소시키는 정규화 텀이기 때문에, 이것은 alpha values가 증가하는 결과가 초래될 수 있다.          alpha blending에서 섞여지는 초기 가우시안의 alpha value값이 지나치게 크다면, 과장된 Gaussian이 초래되고 이것은 \"floater\"를 유발하는 원인이 됨        따라서, 가우시안끼리의 거리에 대해서만 최소화하고, blending weight $w_i$는 최소화 텀에서 뗴어냄3.2. Normal Consistency loss\\[\\begin{align} L_n = \\sum\\limits_{i}w_i(1-n_i^TN) \\end{align}\\]  2DGS의 normal consistency regularization을 바로 3D normal로 적용하는 것이 챌린지  2D Gaussian의 gradient는 항상 투영된 image plane에서의 가우시안 중심점(center, mu)에서 바깥쪽으로 위치하는 특성이 있음          투영된 2D 가우시안 중심에서 픽셀 좌표까지의 방향이 다르면, 두 개의 다른 픽셀에서 렌더링된 노멀은 서로 다르게 나타난다 =&gt; 정확성이 떨어지는 모호함이 발생함            이 문제 완화를 위해서 3D Gaussian의 normal을 ray direction r이 주어졌을 때, intersection plane의 normal로 정의함.      Final Loss\\[\\begin{align} L = L_c + \\alpha L_d + \\beta L_n \\end{align}\\]  where,  $L_c$ : RGB reconstruction loss with combining $L_1$ with the D-SSIm term4. Surface Extraction (Marching-Tetrahedral)  tetrahedral(다면체) 기반으로 그리드를 생성하여 메쉬 추출하는 알고리즘을 이용  학습 이후, surface 또는 triangle mesh extraction 단계  전통적인 메쉬 추출 방법들은 DTU dataset처럼 작은 물체단위의 베경없는 foreground object(regions of interest)영역은 비교적 잘 되지만, large-scale의 unbounded dataset에는 성능 좋게 나오는게 어려운 문제였음          dense evaluation으로 하는 기존 방법 -&gt; 그리드의 해상도에 따라 computation complexity가 증가한다는 점에서, large scale mesh에 적합하지 않고 시간이 매우 많이 걸리는 문제        본 논문에서 novel method 소개 : “Tetrahedral grid”를 이용한 “Marching Tetrahedra“4.1. Tetrahedral Grids Generation  3D Gaussian의 primitive에서 position과 scale value는 surface의 존재에 유의미한 정보를 주는 역할  각각의 가우시안을 감싸는 3D Bounding box를 정의 :          3d box의 중심점에서 가장 높은 opacity를 가지고, 가장자리 꼭짓점(corner)에서 가장 작은 opacity를 가진다.      이 opacity자체를 고려하는 것은 아님, 낮은 opacity value를 가지는 가우시안을 filter out(pruning)하긴 함        bbox의 center와 corner들로 [사면체 그리드]를 생성함          CGAL 라이브러리 이용(Tetra-NeRF에서 영감받아 사용)해서 [Delaunay triangulation] 수행      들로네 삼각법이란? : 2D 평면의 점 집합을 삼각형들로 연결하는 방법 중 하나로, 삼각형의 내접원의 원 안에 다른 점이 포함되지 않도록 삼각형을 구성하는 기법      refer : 들로네-삼각분할        생성된 사면체 그리드에서 filtering step을 통해 겹쳐지지 않은 가우시안과 연결된 edge를 포함하는 사면체 cell들을 제거함          겸쳐지지 않은 가우시안으로 판별하는 과정 : 사면체의 edge 길이가 그것에 대한 maximum scale(평균에서 3σ(표준편차)의 최대 범위까지 확장된 차원)의 합을 초과할 때      4.2. Efficient Opacity Evaluation  앞서 구해진 사면체 그리드의 vertices 즉, 꼭짓점에서의 opacity를 측정하기 위해, 3DGS의 rasterized method처럼 tile-based evaluation algorithm 설계함          꼭짓점들을 image space로 projection한 후, 타일로 쪼개져있을 때 그 대응되는 타일 ID를 확인한다.      각각의 타일에 대해서 projection으로 들어가져있는 point 리스트를 얻을 수 있다. 그 후에 점들을 다시 pixel space로 projection해서 대응되어 떨어지는 pixel을 구할 수 있다.      따라서 해당 pixel에 기여하는 가우시안들을 추적할 수 있고 이 과정은 모든 학습 이미지들에 대해서 수행된다.      그 후, pre-filtered된(pruning) Opcaity를 가지는 가우시안들중에서 minimum값을 Tetrahedral grid 꼭짓점의 opacity로 삼는다.      4.3. Binary Search of level Set  traingle mesh 추출 단계 via. Marching Tetrahedral method  “marching tetrahedral” :           선형 보간(linear interpolation)이 level set구분하는 것에 의존하기 때문에, Opacity Field라는 가우시안의 비선형적 특성에는 misalign되어서 성능이 불완전함      선형 추정(linear assumption)을 늘려가면서 non-linear한 opacity field로 level set을 정확하게 확인      Binary Search(이진 탐색)알고리즘으로 구현함                  8 iteration binary search한 것이 dense evaluation 256번 한 것이랑 같은 시뮬레이션 효과나오는 것 확인함                            Experiments  custom CUDA kernel  DTU, TnT 같은 foreground simple dataset뿐만 아니라 Mip-Nerf360 dataset처럼 unbounded된 scene에서 background영역까지 복원 잘 됨  느낀점 &amp; Future work  ray-Gaussian Intersection(GOF)과 ray-tracing based rasterized method(RaDe-GS)에 나오는 공통 방법론이 어디서부터 시작된 이론인지 레퍼런스 찾기  (mip-nerf 360) 논문에서 depth distortion loss 부분 중심으로 읽기 (완)  들로네 삼각분할 « 대충 훑기만 해선 이해안됨  포아송 재건, 마칭큐브// 마칭-사면체 같은 메쉬추출 알고리즘만 정리돼있는 내용 어디없는지 확인"
  },
  
  {
    "title": "RaDe-GS, arXiv preprint",
    "url": "/posts/RaDe-GS/",
    "categories": "Paper Review",
    "tags": "Surface reconstruction, stereo vision, Graphics",
    "date": "2025-02-07 19:34:00 +0900",
    





    
    "snippet": "RaDe-GS: Rasterizing Depth in Gaussian Splatting  인용도 많이 됐고 나온지 꽤 됐는데 아직 아카이브이길래 아마 cvpr2025 이번에 내지 않았을까 싶은데…  GOF(Gaussian Opacity Fields, SIGGRAPH Asia 2024) 다음에 읽기Abstract  3DGS의 이산적이고 비구조적인 특성때...",
    "content": "RaDe-GS: Rasterizing Depth in Gaussian Splatting  인용도 많이 됐고 나온지 꽤 됐는데 아직 아카이브이길래 아마 cvpr2025 이번에 내지 않았을까 싶은데…  GOF(Gaussian Opacity Fields, SIGGRAPH Asia 2024) 다음에 읽기Abstract  3DGS의 이산적이고 비구조적인 특성때문에 shape accurac가 떨어지는 문제 있음  최근 관련 연구 중, 2D-GS에서는 shape reconstruction 성능을 올리기 위해, Gaussian primitives들을 이용했지만 이것은 렌더링 퀄리티랑 계산적인 효율성은 감소시키는 효과가 있음  그래서 본 논문에서는 rasterized 접근법을 통해, 3DGS의 depth map과 surface normal map을 렌더링하는 Shape reconstruction 정확성 보장  DTU dataset에서 NeuraLangelo와 비교했을 때 CD(Chamfer Distance) error가 좋게 나옴Introduction  multi-view 이미지들로부터 3D Reconstruction하는 태스크에서는 multi-view stereo 알고리즘을 통해 depth map을 얻는 것을 포함한다.  이렇게 얻어진 depth map으로 완전한 triangle mesh를 만드는 모델로 통합되는 것이 가능하다      전통적인 image-based 모델링 접근법은 꽤 정확한 결과로 depth map rendering + mesh recon이 가능하나, 고질적으로 빛나는 표면(shiny surface)이나 reflective한 유리같은 transparent surface에서 특히나 robustness가 떨어지는 한계점이 존재한다.    explicit한 3DGS에서 surface 잘 뽑아내는 것은 어려운 문제임          2DGS[Huang et al.2024] : PSNR 수치 줄어듦      GOF[Yu et al.2024] : ray-tracing 기반 방법을 통해 광선(light ray)에 따른 Gaussian opacity를 계산하는 방법으로 high-quality surface를 뽑아냄      Contributions  본 논문에서는 raasterized 기반 방법으로 general Gaussian Splatting에서 정확한 depth map을 얻는 것을 발전시켰다.  standard GS만큼의 computation efficiency를 가진다.  DTU dataset에서 shape reconstruction accuracy를 Chamfer Distance 0.69mm 달성, 5분의 학습시간          이 수치는 implicit한 representation 기반 모델인 NeuraLangelo(0.69mm)와 비슷      GS-based 방법론들(sota는 GOF)에서는 당연히 성능 더 좋았음      Methods  scene에 splatting된 Gaussian들과 광선에 의한 intersection point를 통한 솔루션을 찾음  💡Key idea💡   &gt; camera center로부터 각각의 Light ray에 의해 Gaussian value가 최대화되는 지점이 intersection point이다   &gt; intersection point들에서 Affine-projection을 시키면 co-planar (동일 평면상에 존재)하다   &gt; 그리고 이 intersection point들을 Image plane에 projection 된 depth로 정의하여 curved surface를 구할 수 있다   &gt; 따라서, planar equation으로 projected depth를 구하는데 이것은 rasterization에 의해 효율적으로 계산될 수 있다.     &gt; final depth map은 투명도(translucency)를 고려하여 투영된 Gaussian들 중에서 중간값 깊이(median depth)로 계산1. Rasterizing Depth for Gaussian Splats      Gaussian Splatting에서는 아래 사진처럼 Perspective camera projection을 Affine transformation을 통해 근사시켜서 더 효율적인 rendering가능하다는 것 사전에 알아두고 시작        standard 3DGS에서는 알파 블렌딩을 위해서 image plane에 projected된 2D Gaussian의 중심점(center)을 depth로 설정한다.          shape detail 포착 불가능        그러므로, 본 논문에서는 spatially varying depth를 rasterized method기반 방법으로 계산한다.1.1. Depth Under Perspective Projection  planar equation을 얻는 본격적인 과정 이전에, perspective projection으로 camera coordinate에 대한 기본 concepts부터 먼저 이해해보자.          Figure 4.              위 그림에서 왼쪽의 (a)에서 보여진 것 처럼, camera center $\\mathrm{o}$와 unit direction $\\mathrm{v}$가 주어졌을 때, ray 상에 $o$로부터 거리 $t$만큼 떨어져 있는 point $\\mathrm{x}$는, \\(\\begin{align} \\mathrm{x}= \\mathrm{o} + t\\mathrm{v} \\end{align}\\)으로 구할 수 있다.        이 ray에 존재하는 Gaussian value $\\mathrm{G}^1(t)$는,\\(\\begin{align} \\mathrm{G}^1(t)= e^{-(\\mathrm{o} + t\\mathrm{v}-\\mathrm{x}_c)^T\\Sigma^{-1}(\\mathrm{o} + t\\mathrm{v}-\\mathrm{x}_c)} \\end{align}\\)으로 구할 수 있다.    이 때, Gaussian value는 1-D function이고, 이 값이 최대화되는 지점이 3D Gaussian과 ray가 만나는 “intersection point” 라고 정의한다.      그 후 distance $t^*$ : intersection point와 camera center간의 거리는 위의 Gaussian value가 최대화되는 t를 찾으면 되고 구한 식은 아래처럼 됨\\[\\begin{align} t^* = \\frac{\\mathrm{v}^T\\Sigma^{-1}(\\mathrm{x}_c - \\mathrm{o})}{\\mathrm{v}^T\\Sigma^{-1}\\mathrm{v}} \\end{align}\\]    (유도 과정) :        이렇게 유도된 distance $t^*$이 의미하는 바는, 3D Gaussian들과 광선들 집합간의 intersection들이 “curved surface”(연두색 곡선) 를 나타낸다고 해석할 수 있음          different pixels have different depth values of $t^*$ and different viewing direction $\\mathrm{v}$.      1.2. Depth Under Local Affine Projection  이제 가우시안말고, 픽셀의 depth를 “local affine projection”으로 구해보자.      위의 Figure4.(b)상황 =&gt; “ray space” 좌표계    [1.2.1. Transformation from Camera to Ray space]          (a) 의 파란 점 $\\mathrm{x} = (x, y, z)^T$ 가 (b)의 파란 점 $\\mathrm{u} = (u,v,t)$ 로 변환      $(u,v,t)$에서 $(u,v)$는 image plane coordinate(이미지 평면에서의 좌표계)이고, $(t)$는 point와 $uv-plane$사이의 거리를 나타낸다      즉, $t=\\sqrt{x^2+y^2+z^2}$              ray space에서 light direction $\\mathrm{v}$ 는 단위고유벡터 $(0,0,1)^T$임                    가우시안들도 ray space로 변환되어야 하는데, 변환된 Gaussian Function은 아래와 같다\\[\\begin{align} \\mathrm{G}'(\\mathrm{u}) = e^{-(\\mathrm{u}-\\mathrm{u_c})^T\\Sigma^{'-1}(\\mathrm{u}-\\mathrm{u_c})} \\end{align}\\]        Gaussian center $\\mathrm{u_c}=(u_c, v_c, t_c)^T$로, (b)그림에서 빨간 점                  [1.2.2. Intersection in Ray Space]    기본 과정은 perpective projection때와 같음     1) ray space 상에 존재하는 point $\\mathrm{u}$  :\\[\\begin{align} \\mathrm{u} = \\mathrm{u_o} + t\\mathrm{v'} \\end{align}\\]    where , $\\mathrm{u_o} = (u,v,0)^T \\quad and \\quad \\mathrm{v’} = (0,0,1)^T $    2) 1D Gaussian function $\\mathrm{G^{‘1}}(t)$ :\\[\\begin{align} \\mathrm{G^{'1}}(t)=  e^{-(\\mathrm{u_o} + t\\mathrm{v'}-\\mathrm{u_c})^T\\Sigma^{'-1}(\\mathrm{u_o} + t\\mathrm{v'}-\\mathrm{u_c})} \\end{align}\\]    3) maximum point가 위치한 distance $t^*$ :\\[\\begin{align} t^* = \\frac{\\mathrm{v'}^T\\Sigma^{'-1}(\\mathrm{u}_c - \\mathrm{u_o})}{\\mathrm{v'}^T\\Sigma^{'-1}\\mathrm{v'}} \\end{align}\\]    간단하게 표현하기 위해서 $(\\mathrm{u}_c - \\mathrm{u_o})$ 벡터 빼고 나머지 항들을 $\\hat{q}$로 정의하여 간단하게 아래처럼 표현 :\\[\\begin{align} t^* = \\hat{q}(\\mathrm{u}_c - \\mathrm{u_o}) \\end{align}\\]        [1.2.3. Depth of Intersection]    1) $t$는 3D point $x$와 camera center $o$사이의 거리이므로 x에 대한 깊이는, 간단하게 삼각비 성질을 이용해서 (a)그림의 $z^*$을 아래처럼 구할 수 있다:\\[\\begin{align} d = cos\\theta t^* \\end{align}\\]    2) (b), affine projection인 상황에 대입해서 depth구하면,\\[\\begin{align} d = cos\\theta_c t^* = \\frac{x_c}{t_c}t^* = \\frac{x_c}{t_c}\\hat{q}(\\mathrm{u}_c - \\mathrm{u_o}) = \\hat{p}(\\mathrm{u}_c - \\mathrm{u_o})    \\end{align}\\]    여기서 $\\hat{p} = \\frac{z_c}{t_c}\\hat{q}$로 정의    3) 이어서 위의 식을 아래처럼 나타낼 수 있는데,\\[\\begin{align} d = \\hat{p}(\\mathrm{u}_c - \\mathrm{u_o}) =  \\hat{p}\\begin{pmatrix} u_c - u \\\\ v_c-v \\\\ t_c \\end{pmatrix} = \\hat{p}\\begin{pmatrix} 0 \\\\ 0 \\\\ t_c \\end{pmatrix} + \\hat{p}\\begin{pmatrix} \\Delta{u} \\\\ \\Delta{v} \\\\ 0 \\end{pmatrix}.  \\end{align}\\]    4) 위의 최종 분해된 식 중, 첫 번째 항 $\\begin{pmatrix} 0 \\ 0 \\ t_c \\end{pmatrix}$에 대한 성질 :\\[\\begin{align}       \\hat{p} \\begin{pmatrix} 0 \\\\ 0 \\\\ t_c \\end{pmatrix}       &amp;= \\frac{z_c}{t_c} \\hat{q} \\begin{pmatrix} 0 \\\\ 0 \\\\ t_c \\end{pmatrix} \\\\      &amp;= \\frac{z_c}{t_c} \\frac{\\mathrm{v^{'T}}\\Sigma^{'-1}}{\\mathrm{v^{'T}}\\Sigma^{'-1}\\mathrm{v'}} \\begin{pmatrix} 0 \\\\ 0 \\\\ t_c \\end{pmatrix} \\\\      &amp;= \\frac{z_c}{t_c} \\frac{\\mathrm{v^{'T}}\\Sigma^{'-1}}{\\mathrm{v^{'T}}\\Sigma^{'-1}\\mathrm{v'}} (t_c\\mathrm{v'}) \\\\      &amp;= \\frac{z_c}{t_c} \\frac{\\mathrm{v^{'T}}\\Sigma^{'-1}\\mathrm{v'}}{\\mathrm{v^{'T}}\\Sigma^{'-1}\\mathrm{v'}} (t_c) \\\\      &amp;= z_c .  \\end{align}\\]       5) 두 번째 항의 $\\hat{p}$ 는 $p$ 로 근사됨          따라서, 최종적으로 depth를 구하는 rasterized method수식으로 유도된다.       Recall. $d = z_c + p(\\begin{pmatrix} \\Delta{u} \\ \\Delta{v} \\end{pmatrix})^T$      2. Rasterizing Normal for Gaussian Splats  느낀점  이 분야에서는 살짝 뭐 바꿨더니 소타다 이런건 절대 안먹히겠다  완전히까진 아니여도 적당히 창의적으로 메소드를 짜야되겠구나라는 생각이 들었다. 이런 면이 재밌지만서도 다소 막막하긴 하다  Affine aprroximation projection기반으로 spatial varying depth 구하는 방법이 창의적이다  2DGS에서 쓴 loss 가져다 쓴 건 아쉬운데 이거까지 새로 짰으면 걍 씨그라프 오랄 찍었을 것 같다  이건 어셉 왜 안되지          GOF(SIGGRAPH Asia 2024)가 작년 9월에 나와서 sota인 것 같다. 이어서 이거 읽기      "
  },
  
  {
    "title": "Graphics-Ch6. Transformation Matrices",
    "url": "/posts/ch6-Transformation_Matrices/",
    "categories": "Graphics, Study",
    "tags": "Graphics",
    "date": "2025-02-07 13:04:00 +0900",
    





    
    "snippet": "Introduction  Chapter 5는 선형대수학(Linear Algebra)이라 아는 내용이 많아서 정리 생략  Transformation matrix는 왜 필요한가 : Rotation, Translation, Scaling, Projection같은 기하학적 변환에서 행렬 곱을 통해 변환되는데 이때 변환 행렬이 필요함1. 2D Linear Tr...",
    "content": "Introduction  Chapter 5는 선형대수학(Linear Algebra)이라 아는 내용이 많아서 정리 생략  Transformation matrix는 왜 필요한가 : Rotation, Translation, Scaling, Projection같은 기하학적 변환에서 행렬 곱을 통해 변환되는데 이때 변환 행렬이 필요함1. 2D Linear Transformations      2D vector를 2x2 행렬로 변환하는 과정은 아래처럼 전개됨 : \\[\\begin{bmatrix} a_{11} &amp; a_{12} \\\\ a_{21} &amp; a_{22} \\end{bmatrix}  \\begin{bmatrix} x \\\\ y \\end{bmatrix} = \\begin{bmatrix} a_{11}x &amp; a_{12}y \\\\ a_{21}x &amp; a_{22}y \\end{bmatrix}\\]        여기서, 벡터 $(x y)^T$에 곱해지는 행렬 $A$를 변환행렬이라고 한다  1.1. 2D Scaling\\[\\begin{align} scale(s_x, s_y) = \\begin{bmatrix} s_x &amp; 0 \\\\ 0 &amp; s_y \\end{bmatrix} \\end{align}\\]=&gt; Cartesian components와 결합되면 아래와 같이 전개됨 \\[\\begin{bmatrix} s_x &amp; 0 \\\\ 0 &amp; s_y \\end{bmatrix}  \\begin{bmatrix} x \\\\ y\\end{bmatrix} = \\begin{bmatrix} s_xx \\\\ s_yy \\end{bmatrix}\\]1.2. Shearing  물체에 평행한 방향으로 힘이 가해져 변형되는 현상      얼만큼($=s$만큼) x또는 y방향으로 물체를 밀 건지\\[shear-x(s) = \\begin{bmatrix} 1 &amp; s \\\\ 0 &amp; 1 \\end{bmatrix} , shear-y(s) = \\begin{bmatrix} 1 &amp; 0 \\\\ s &amp; 1 \\end{bmatrix}\\]        만약, 시계 방향으로 각도 $\\phi$만큼 회전시키는 shear transformation matrix :\\(\\begin{bmatrix} 1 &amp; tan\\phi \\\\ 0 &amp; 1 \\end{bmatrix}\\)    만약, 반시계 방향으로 각도 $\\phi$만큼 회전시키는 shear transformation matrix :\\(\\begin{bmatrix} 1 &amp; 0 \\\\ tan\\phi &amp; 1 \\end{bmatrix}\\)1.3. Rotationvector $\\mathrm{a}$가 x축으로부터 각도 $\\alpha$만큼 떨어진 길이가 r인 벡터라는 상황 가정 반시계방향(counterclockwise)으로 벡터 a를 회전시킨 벡터를 $\\mathrm{b}$라고 두자.   $$ \\begin{align} \\mathrm{a} = (x_a, y_a) , \\mathrm{b} = (x_b, y_b) \\end{align} $$  $$ \\begin{align} x_a = rcos\\alpha \\\\ y_a = rsin\\alpha \\end{align} $$    $$ \\begin{align} x_b = rcos(\\alpha+\\phi)=rcos\\alpha cos\\phi - rsin\\alpha sin\\phi \\\\ y_b = rsin(\\alpha+\\phi) = rsin\\alpha cos\\phi + rcos\\alpha sin\\phi \\end{align} $$  위 식에서, $x_a =rcos\\alpha$ 그리고 $y_a = rsin\\alpha$ 를 대입하면 아래처럼 정리된다. \\[\\begin{align} x_b = x_a cos\\phi - y_a sin\\phi \\\\ y_b = x_bcos\\phi + x_a sin\\phi \\end{align}\\]따라서, vector $\\mathrm{a}$에서 vector $\\mathrm{b}$로 가는 Rotation matrix term은 아래와 같다 :\\[\\begin{align} rotate(\\phi) = \\begin{bmatrix} cos\\phi &amp; -sin\\phi \\\\ sin\\phi &amp; cos\\phi \\end{bmatrix} \\end{align}\\]  각 행에 대한 원소 norm $(sin^2\\phi + cos^2\\phi = 1)$ 이기 때문에, 행들은 서로 직교한다(orthogonal)  따라서, Rotation matrix를 직교행렬(orthogonal matrix)로 볼 수 있다.1.4. Reflection  대칭이동(축을 기준으로 뒤집는 변환) \\[\\begin{align} reflect-y = \\begin{bmatrix}  -1 &amp; 0 \\\\ 0 &amp; 1 \\end{bmatrix}  ,\\quad reflect-x = \\begin{bmatrix}  - &amp; 0 \\\\ 0 &amp; -1 \\end{bmatrix}  \\end{align}\\]1.5. Composition and Decomposition of Transformations      처음 상태 : 2D vector $\\mathrm{v_1}$        scale transformation $S$ 적용 이후, Rotation transformation $R$을 적용한다면 : \\[\\begin{align} \\mathrm{v_2} = S\\mathrm{v_1}, \\quad then, \\mathrm{v_3} = R\\mathrm{v_2} \\end{align}\\]\\[\\begin{align}  \\mathrm{v_3} = R(S\\mathrm{v_1}) . \\\\ \\mathrm{v_3}=(RS)\\mathrm{v_1} \\end{align}\\]  처럼 나타낼 수 있다.따라서, $M = RS$행렬을 변환 행렬로 취급한다.  중요한 것은, 이러한 변환들은 오른쪽 변환부터 적용된다는 것을 혼동하면 안됨, 즉, RS에서 scaling먼저 적용하는 의미  이 순서가 바뀌면 결과도 달라짐1.6. Decomposition of Transformations  Compoisition말고 곱해진 상태의 변환 행렬을 “분해”하는 방법  크게 고유값 분해(Eigenvalue Deomposition) 기반과 특이값 분해(Singular Value Decomposition)기반 방법이 있다.1. Symmetric Eigenvalue Deomposition      행렬이 symmetric(대칭)행렬일 때, \\(\\mathrm{A} = \\mathrm{R}\\mathrm{S}\\mathrm{R}^T\\) 으로 분해됨      1) $\\mathrm{R}^T$ 변환 : 행렬 A의 eigen vector(고유벡터)인 $\\mathrm{v_1}$과 $\\mathrm{v_2}$를 x 또는 y축 기반으로 회전시킴 2) $\\mathrm{S}$ 변환 : x와 y를 $(\\lambda_1, \\lambda_2)$만큼 scaling 시킴 3) $\\mathrm{R}$ 변환 :  $\\mathrm{v_1}$과 $\\mathrm{v_2}$를 x 또는 y축 기반으로 다시 역회전시킴   2. Singular Value Deomposition  전체적으로 과정은 같음, 대신 전체 변환행렬 A가 대칭행렬이 아님  고유값 대신 특이값사용2. 3D Linear Transformations  2D 변환의 확장된 ver.3. Translation and Affine Transformations[1] 변환 행렬 M이 곱해졌을 때, \\[\\begin{align} x' = m_{11}x + m_{12}y \\\\ y' = m_{21}x + m_{22}y \\end{align}\\][2] Translation 이 있을 때, \\[\\begin{align} x' = x + x_t \\\\ y' = y + y_t \\end{align}\\]위의 두 가지 변환 연산을 합친 Transformation matrix M을 single하게 어떻게 나타낼 것인가, trick : Homogeneous coordinate을 이용함 :  $(x \\quad y) -&gt; [x \\quad y \\quad 1]^T $\\[M = \\begin{bmatrix}  m_{11} &amp; m_{12} &amp; x_t \\\\ m_{21} &amp; m_{22} &amp; y_t \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix}\\]이렇게 곱해지는 형태의 변환을 “Affine Transformation” 이라고 한다.… 이어서"
  },
  
  {
    "title": "Graphics-Ch4. Ray Tracing",
    "url": "/posts/ch4-Ray-Tracing/",
    "categories": "Graphics, Study",
    "tags": "Graphics",
    "date": "2025-02-04 17:34:00 +0900",
    





    
    "snippet": "Rendering  Object based Rendering          각각의 object단위로 고려됨      물체가 존재하는 모든 픽셀들이 찾아지고 업데이트 됨        Image based Rendering          각각의 pixel단위로 고려됨      물체에 영향을 주는 각 pixel들이 찾아지고 업데이트 됨둘의 차이점에 대해...",
    "content": "Rendering  Object based Rendering          각각의 object단위로 고려됨      물체가 존재하는 모든 픽셀들이 찾아지고 업데이트 됨        Image based Rendering          각각의 pixel단위로 고려됨      물체에 영향을 주는 각 pixel들이 찾아지고 업데이트 됨둘의 차이점에 대해서는 ch8에서 더 자세하게 다룬다      Ray Tracing이란  3D Scene을 렌더링하는데에 사용되는 image-order 알고리즘  object-order rendering에서 사용되어지는 수학적 방법1. Basic Ray-Tracing Algorithm  image상의 Pixel에서 ray tracing을 했을 때 보여지는 물체를 찾는 것이 목표  각각의 픽셀은 다른 방향을 “바라본다”          이 보여지는 물체들은 모두 viewing ray 상에 존재한다        카메라에 가장 가깝게 존재하는 viewing ray에 대해 교차하고 있는 특정 Object를 찾는 것이 목표      그 물체가 찾아지면, shading 계산을 통해 intersection point, surface normal, 그리고 다른 정보들을 구해서 pixel color를 결정할 수 있음    Basic Ray Tracer의 세 가지 흐름 :          Ray Generation : Camera geometry에 기반하여, origin과 각 픽셀마다 vieweing ray의 direction을 구하는 것      Ray Intersection : viewing ray와 교차하는 가장 가까운 물체(Object)를 찾는 것      Shading : ray-intersection 과정을 통해 구해진 결과를 기반으로 Pixel color를 계산하는 것        기본 내용을 다룬다, 더 발전된 내용은 chapter 10, 12, 13 등등에 나옴2. Perspective projection  how to mapping 3D objects into 2D image plane?                            Linear perspecrive          straight line =&gt; straight line                                      Parallel projection          projection direction에 따라서 움직여짐                          parallel projection을 통해 보이는 뷰는 orthographic view라고도 부른다          장점 : 변환되어도 size랑 shape은 변화시키지 않고 일정하게 유지한다          단점 : 물체가 view point기준으로 멀어질수록 작게 보인다 =&gt; vanishing point(소실점)과 연관 있음                          this is because eyes and cameras don’t collect light from a single viewing direction(뷰 하나 보이는 것으로 눈과 카메라에서 모든 빛을 수집할 수 없기 때문임)                                           3. Computing Viewing Rays (Ray Generation)\\(p(t) = e + t(s-e)\\)  $e$ : eye(origin point, view point)  $s$ : end point of the ray      $t$ : 시간 축    vector $(s-e)$ 를 view direction로 해석할 수 있다  만일 $t$가 0보다 작다면, view behind에 있다고 해석할 수 있다  “Ray Generation” 단계의 origin과 view direction을 구하는 과정에서 camera frame으로 알려진 orthonormal 좌표계에서 시작해야 한다.          orthonormal coordinate frame : 세 가지 기저 벡터 $u, v, w$으로 구성되어 있음      3.1. Orthographic (Parallel) Views  이미지를 표현하는 4가지 차원 = ${l, r, b, t}$[Orthographic viewing rays 만드는 방법]  ray Starting point(Origin)로 pixel의 image-plane position을 그대로 사용할 수 있다 = $e + u\\mathrm{u} + v\\mathrm{v}$  ray’s Direction으로는 view direction을 가져다가 사용할 수 있다 = $-w$3.2. Perspective Views  각 픽셀마다의 ray들은 같은 origin$(e)$를 공유함          Image Plane이 더이상 $e$ 점에 위치하는 것이 아니라, 거리 $d$만큼 떨어져서 존재함      $d$ : image plane distance 또는 focal length 라고 불린다        view direction은 모두 다름          어떻게 구해지는가? -&gt; image planedml pixel위치와 viewpoint에 의해서 정의된다 = $-dw + u\\mathrm{u} + v\\mathrm{v}$      4. Ray-Object Intersection  앞선 Ray Generation단계를 통해 ray $\\mathrm{e} + t\\mathrm{d}$를 만들었다면, $t &gt; 0$ 인 구간에서 ray와 처음으로 교차되는(만나는) 물체를 찾는 것이 필요하다.  General problem = find the first intersection between the ray &amp; a surface that occurs at a $t$ in the interval $[t_0, t_1]$          sphere과 traingles 물체(surface)를 먼저 다루고, 다른 다면체들은 다음 섹션에 다루겠음      4.1. Ray-Sphere Intersection다음의 ray와 surface가 만나는 교차점을 구한다고 생각해봅시다,  ray $p(t) = \\mathrm{e} + t\\mathrm{d}$  implicit surface $f(p) = 0$\\[\\begin{align} f(p(t)) = 0 \\\\  f(\\mathrm{e} + t\\mathrm{d}) = 0 \\end{align}\\]      그리고 sphere는 중심점 $c = (x_c, y_c, z_c)$와 반지름 $R$을 이용해서 아래처럼 implicit equation을 나타낼 수 있음\\(\\begin{align} (x - x_c)^2 + (y - y_c)^2 + (z - z_c)^2 - R^2 = 0 \\\\  (p-c) \\cdot (p-c) - R^2 = 0    \\end{align}\\)              이 vector form 구의 방정식에서 p에다가 ray $p(t)$를 집어넣어도 식이 성립해야 함, intersection point이기 때문      $(\\mathrm{e} + t\\mathrm{d} - c) \\cdot (\\mathrm{e} + t\\mathrm{d} - c) - R^2 = 0$                  식을 풀면 아래와 같이 $t$에 대한 2차방정식 term으로 풀어짐      $(d \\cdot d)t^2 + 2d \\cdot (e-c)t + (e-c) \\cdot (e-c) - R^2 = 0$            근의 공식을 이용하여 $t$를 구한다\\(t = \\frac{-d \\cdot (e-c) \\pm \\sqrt{(d \\cdot (e-c))^2 - (d \\cdot d)( (e-c) \\cdot (e-c) - R^2)}}{(d \\cdot d)}\\)        근의 공식 풀기 위해서는  판별식 ($B^2 - 4AC$) 이거를 통해 근이 몇 개 나오는지 먼저 판단한다, 이 값이 음수면 허수라서 교차점 없다고 판단하면 됨              section 2.5.4에서 다루었던 것처럼, point $p$에 있는 normal vector는 gradient를 통해 아래처럼 계산할 수 있음      \\mathrm{n} = 2(p-c)        unit normal = (p-c) / R4.2. Ray-Traingle Intersection  barycentric 좌표계 사용함 =&gt; 삼각형을 포함하는 parametric plane을 표현할 때 삼각형의 꼭짓점만 이용하면 다른 storage가 필요없는 효율적인 좌표계표현이기 때문  parameteric surface와 ray간의 intersection point구하는 방법 :          Cartesian coordinate (데카르트 좌표계)를 이용한 다음 연립방정식으로 푼다=&gt; 식이 세 개이고, 구해야 되는 미지수도 $(t, u, v)$로 세 개이기 때문에 계산 가능      왼쪽 그림과 같이, $a,b,c$ 의 vertices로 이루어진 삼각형이 존재하고 ray $p(t) = \\mathrm{e} + t\\mathrm{d}$가 존재하는 상황을 가정할 때, intersection point인 $p$는 그림과 같이 ray 연장선 위에 존재한다. $$ \\mathrm{e} + t\\mathrm{d} = \\mathrm{a} + \\beta(\\mathrm{b} - \\mathrm{a}) + \\gamma(\\mathrm{c}-\\mathrm{a}) $$ 이 식에서 $t, \\beta, \\gamma$를 구해야 함위의 식을 vector form으로 아래처럼 식을 확장할 수 있음 :\\[\\begin{align}x_e  + tx_d = x_a + \\beta(x_b - x_a) + \\gamma(x_c - x_a), \\\\y_e  + ty_d = y_a + \\beta(y_b - y_a) + \\gamma(y_c - y_a), \\\\z_e  + tz_d = z_a + \\beta(z_b - z_a) + \\gamma(z_c - z_a). \\\\\\end{align}\\]      vector form을 행렬로 바궈서 standard linear system(선형 결합 형태)로 아래처럼 바꿔 표현 가능:        그 후, 크래머 규칙(Cramer’s rule)을 이용해서 $t, \\beta, \\gamma$ 값을 도출할 수 있다 (자세한 풀이 생략 책 참고)  5. Shading  픽셀에 대한 intersection point, 즉 visible surface가 구해졌다면, 광원을 고려해서 pixel color(intensity) 결정하는 단계  Light Reflection(반사) 관련 모델링을 사용한다          중요한 변수들                  light direction $\\mathrm{l}$          view direction $\\mathrm{v}$          surface normal $\\mathrm{n}$                    5.1. Lambertian Shading  가장 간단한 shading modeling 을 위한 가정**Lambertian Shading**  : 왼쪽 그림에서 표면에 떨어지는 광원으로부터 나오는 빛의 양은  빛에 대한 입사각 $\\theta$에 의해서만 결정된다. (View-Independent) 이를 이용해서 lambertian Shading model식을 아래처럼 설계  $$ L = k_d I max(0, \\mathrm{n} \\cdot \\mathrm{l}) $$  $k_d$ : diffuse coefficient (난반사 계수) 또는 surface color = 표면이 입사된 빛을 얼마나 균일하게(난반사로) 반사하는지를 나타내는 계수  $I$ : intensity of the light source  여기서 n과 l은 크기가 1인 단위벡터이기 때문에, $\\mathrm{n} \\cdot \\mathrm{l}$을 $cos\\theta$ 로 계산 가능하다.          즉, 내적은 cosine 유사도를 의미하므로 빛이 들어오는 방향에 대해서 얼마나 반사되는지 그 강도(intensity)가 결정된다는 의미      =&gt; 위의 모델링 수식은 RGB 채널 3개에 대해서 각각 적용되어 pixel value가 구해진다  $\\mathrm{v}, \\mathrm{l}, \\mathrm{n}$이 모두 단위벡터(크기가 1)인 것을 잊지 말기!5.2. Blinn-Phong Shading  모든 광원이 난반사(diffuse)로만 구성되지는 않는다,  specular component (정반사되는 성질) 빛을 모델링하기 위한 모델  Idea : $\\mathrm{v}$랑 $\\mathrm{l}$ 이 surface normal $\\mathrm{n}$을 가로지르는 상황에서 반사가 가장 잘된다는 것          mirror reflection이 발생할때 반사율이 가장 크다.      오른쪽 그림처럼, $\\mathrm{v}$와 $\\mathrm{l}$ 각도 중간에 위치한 half vector $\\mathrm{h}$가 있다고 가정해보자, 이 h가 surface noraml n과 가까울수록, specular component가 증가한다. (밝기가 증가) $\\mathrm{h}$와 $\\mathrm{n}$사이의 유사도는 **dot product(내적)**으로 생각할 수 있다. *Phong exponent*라고 불리는 power $p$가 surface에 대한 광택의 정도를 조절하는 역할임\\[\\begin{align}h = \\frac{\\mathrm{v} + \\mathrm{l}}{||\\mathrm{v} + \\mathrm{l}||}, \\\\L = k_d  I  max(0, \\mathrm{n} \\cdot \\mathrm{l}) + k_s I max(0, \\mathrm{n} \\cdot \\mathrm{h})^p \\\\\\end{align}\\]where, $k_s$ is the specular coefficient, or the specular color of the surface5.3. Ambient Shading      조명이 도달하지 않는 표면에서는 완전하게 black으로 보이기 때문에, 이런 현상을 줄이기 위해서 constant component를 shading model에 추가시킴        Full version of a simple and useful shading model(Ambident shading components &amp; Blinn-Phong model) :\\[\\begin{align}l = k_aI_a + k_dImax(0, \\mathrm{n} \\cdot \\mathrm{l}) + k_s I Max(0, \\mathrm{n} \\cdot \\mathrm{h})^n\\\\  \\end{align}\\]  "
  },
  
  {
    "title": "2D Gaussian Splatting, SIGGRAPH 2024",
    "url": "/posts/2DGS/",
    "categories": "Paper Review",
    "tags": "Geometry Reconstruction, Mesh extraction, Novel View Synthesis, Graphics",
    "date": "2025-02-03 16:34:00 +0900",
    





    
    "snippet": "Intro  3DGS : Redering결과 Inconsistency한 depth발생, 왜냐하면 pixel ray사이의 intersection을 통해 gaussian value를 구하기 때문  2DGS : explicit ray-splat intersection 이용  surface normal 정확하게 추출하면 mesh도 잘 구해짐          ...",
    "content": "Intro  3DGS : Redering결과 Inconsistency한 depth발생, 왜냐하면 pixel ray사이의 intersection을 통해 gaussian value를 구하기 때문  2DGS : explicit ray-splat intersection 이용  surface normal 정확하게 추출하면 mesh도 잘 구해짐                  depth distortion        : 2D primitives에 집중해서 더 좁은 범위의 ray에 가우시안들이 분포하게 해줌                    normal consistency        : rendered normal map과 rendered depth의 gradient 사이의 discrepancies 를 최소화함              두 개 regularaization term을 이용해서 smoother surface 구함  Contributions  efficient differentiable 2D Gaussian Renderer  surface recon을 위한 2가지 정규화 텀 소개 (depth distortion, normal consistency)  sota Reconstruction &amp; NVS result1. Modeling  normal = the steepest change of density          better alignment with thin surfaces        input = only a sparse calibration point cloud &amp; photometric supervision2D Gaussian Explicit Properties  central point $p_k$  two principal tangential vector $t_u, t_v$      scaling vector $S = (s_u, s_v)$    primitive normal : $t_w = t_u \\times t_v$      3x3 Rotation matrix $R = [t_u, t_v, t_w]$    2D Gaussian들은 local tangent plane에 정의됨      $P(u,v) = p_k + s_ut_uu + s_vt_vv =H(u,v,1,1)^T$ ⇒  world space에서 정의된 local tangent plane에서의 2D Gaussian  point    where $H$   is the homogeneous transformation matrix, representing the geometry of the 2D Gaussian        $G($u$) = exp(-\\frac{u^2+v^2}{2})$    $point - \\mathrm{u} = (u, v)$  2. Splatting  image space로 2D Gaussian들을 project하는 과정  affine approximation of Perspective Transformation 이용          Perspective transformation은 평행성이 지켜지지 않음, parallel한 line들이 소실점(vanishing point)에서 만남              한계점(선행 조건필요) :                              center of the Gaussian이 정확해야하고,                                center로부터의 거리가 멀어질수록 approximation error가 증가해야함                                homogeneous coordinate을 이용해서 완화된 해결방법 제안됨      2D Splat을 2D image plane에 projection한느 건 일반적인 2D-to-2D mapping in homogeneous coordinate이라고 할 수 있다    $W$ :  transformation matrix from World space to Screen space      Screen space에서의 projected points $\\mathrm{x}$    $\\mathrm{x} = (xz, yz, z, 1)^T = WP(u,v) = WH(u,v,1,1)^T$ 로 구해짐    2D Gaussian들을 rasterize하려면, inverse transfomation 인 $M= (WH)^{-1}$을 이용하는 방법이 있으나, 이 방법은  numerical instability함 특히 splat이 line segment(view가 옆면에서 보이는 상황인 경우, )에서 특히 결함이 많음      이 문제 해결위해 previous surface splatting rendering method들은 미리 정의된 predefined threshold을 이용해서  ill-conditioned transformation을 이용해왔음 (related work)      ⇒ 모두 unstable함, 따라서 본 논문에서는 “Ray-Splat Intersection” 기반 방법을 제안2-1. Ray-Splat Intersection      평행하지 않은 3가지 평면 (three non-parallel planes) 에서의 교점을 찾음으로써 ray-splat intersection을 위치시킴  (??)        image space coordinate $\\mathrm{x} = (x, y)$가 주어졌을 때,    두개의 수직 plane (x-plane과 y-plane)의 교차 ray를 파라미터화한다            x - plane : normal vector인 (-1, 0, 0)과 offset x로 정의됨    ⇒ 4D homogeneous Plane (homogeneous coordinate을 이용한 4D 좌표계 표현한 개념)    $h_x = (-1, 0, 0, x)^T$        y - plane : normal vector인 (0, -1, 0)과 offset y로 정의됨    ⇒ $h_y = (0, -1,  0, y)^T$        ray $\\mathrm{x} = (x,y),$  ( Image Coordinate ) 는 x-plane과 y-plane사이의 교차되는 line에 대한 좌표로 결정    각각의 plane을 2D Gaussian의 local coordinate( $uv-$coordinate system)으로 변환해야 함  point로부터 plane으로의 transformation matrix $M$ 의 역행렬을 곱해주는 Inverse Transpose $M^{-1}$을 수행해야 intersection of the x&amp;y-plane에 대한 좌표를 아래처럼 구할 수 있음,                  이 때 $M = (WH)^{-1}$, $M^{-T} =  (WH)^T$\\[\\begin{align} h_u = (WH)^Th_x   \\\\ h_v = (WH)^Th_y \\end{align}\\]        where, W : world space에서 screen space로 가는 변환 행렬, H : homogeneous 좌표계로 변환하는 행렬              두 평면이 만나는 intersection point $\\mathrm{u}(\\mathrm{x})$  구하기                  2D Gaussian의 point  : $(u, v, 1, 1)$        $h_u^i , h_v^i$ 는 4D homogeneous plane 파라미터들 중에서 i번쨰 파라미터를 의미함          \\[\\begin{align}h_u \\cdot (u,v,1,1)^T = h_v \\cdot (u,v,1,1)^T = 0  \\\\ u(\\mathrm{x}) = \\frac{h_u^2h_v^4-h_u^4h_v^2}{h_u^1h_v^2-h_u^2h_v^1} \\\\ v(\\mathrm{x}) = \\frac{h_u^4h_v^1-h_u^1h_v^4}{h_u^1h_v^2-h_u^2h_v^1} \\end{align}\\]        Screen space에서의 projected points    $\\mathrm{x} = (xz, yz, z, 1)^T = WP(u,v) = WH(u,v,1,1)^T$    이거 식을 Recall해서 $x, y, u, v$ 다 아니까 depth $z$를 구할 수 있다.  3. Improvements3-1. Degenerate Case  object-space low-pass filter 제안\\[\\begin{align}    \\widehat{G}(\\mathrm{x}) = \\max \\left\\{ G(\\mathrm{u}(\\mathrm{x})), G\\left(\\frac{\\mathrm{x}-c}{\\sigma}\\right) \\right\\}\\end{align}\\]⇒ 직관적인 수식 의미 = fixed screen-space(오른쪽 항)에서의 Gaussian이랑 위에서 구한 intersection point u(x)에서의 가우시안중에 더 큰 값을 선택하여 degenerate하는 가우시안들 없앰where $c$  : projection of center $p_k$본 실험에서는 $\\sigma = \\sqrt{2}/2$로 설정함      Volumetric alpha blending (Rasterization)\\[c(\\mathrm{x}) = \\sum_{i = 1}c_i\\alpha_i\\widehat{G}_i(\\mathrm{u}(\\mathrm{x}))\\prod_{j =1}^{i-1}(1-\\alpha_i \\widehat{G}_i(\\mathrm{u}(\\mathrm{x})))\\]  4. Training  3DGS의 photometric loss에다가 2가지 정규화 텀을 추가시켜서 더 smooth한 surface align을 촉진하여 geometry reconstruction의 성능 향상4.1. Depth Distortion      Depth distortion loss : ray-splat intersection결과상 구한 depth 간의 거리 차를 최소화함으로써 ray에 존재하는 weight을 더 concentrate해줌,  Mip-Nerf360에서 영감 받음\\[\\begin{align} L_d = \\sum_{i,j}w_iw_j|z_i-z_j| \\end{align}\\]    where, $w_i :$i번째 intersection하는 지점에서의 blending weight    $z_i :$ i번째 교차점에서의 depth  4.2. Normal Consistency  교차하는 median point(중간 지점) $p_s$에서 actual surface 고려      축적된 opacity가 0.5가 되면, splat된 가우시안들의 normal과 depth map에서의 gradient를 고려해서 align해주는 과정\\[\\begin{align}L_n = \\sum_iw_i(1-n_i^T\\mathrm{N}) \\end{align}\\]    where $i :$ ray상에 존재하는 intersected 된 splat들    $w :$ blending weight of the intersection point    $n_i :$ normal of the splat that is oriented towards the camera    $\\mathrm{N} :$ normal estimate by the gradient of the depth map  Final Loss: $L = L_c + \\alpha L_d + \\beta L_n$$L_c :$ RGB reconstruction loss combining L1 with the D-SSIM term𝛼 = 1000 for bounded scenes, 𝛼 = 100 for unbounded scenes, and 𝛽 = 0.05 for all scenes.5. Experiments  single RTX 3090  Datasets          DTU                  15개 씬으로 구성됨 (49장 또는 69장 이미지, resolution = 1600 x 1200)          Colmap으로 sparse point cloud얻어진 상태, 해상도를 800 x 600 으로 downsample한 이후에 사용함 효율성을 위해서                    TnT      MipNerf360        Evaluation Metrics          PSNR      SSIM      LIPPS      Limitations  assume surfaces with full opacity and extract meshes from multi-view depth maps          semi-transparent한 표면에서는 잘 복원이 안됨, 유리같은 부분        fine geometric structure에 대해서는 덜 정확함  regularization term사용할 때 이미지 퀄리티와 geometry간의 trade-off가 발생함 ⇒ 특정 영역에서 over-smooth결과 초래할 수 있음"
  },
  
  {
    "title": "Graphics-Ch3. Raster Images",
    "url": "/posts/ch3-Raster/",
    "categories": "Graphics, Study",
    "tags": "Graphics",
    "date": "2025-01-29 18:00:00 +0900",
    





    
    "snippet": "Chapter 3. Raster Images서론  Raster image : 2D 행렬 형태로 픽셀에 대한 RGB color값을 표현한 이미지          RGB 색상은 3차원 벡터 형태        디지털 카메라는 image sensor를 포함하고 있는데, 이것은 빛이 들어오는 정도(intensity)와 color를 구성하고 있음  하지만 우리가...",
    "content": "Chapter 3. Raster Images서론  Raster image : 2D 행렬 형태로 픽셀에 대한 RGB color값을 표현한 이미지          RGB 색상은 3차원 벡터 형태        디지털 카메라는 image sensor를 포함하고 있는데, 이것은 빛이 들어오는 정도(intensity)와 color를 구성하고 있음  하지만 우리가 이러한 2D array자체로 이미지를 보지는 않음,          이미지 픽셀과 디스플레이 픽셀간의 매칭(direct link)이 필요하고 이를 Rasterizer가 처리함        Vector Image : 픽셀에 대한 행렬형태로 나타내는 것이 아니라, shape이랑 line, curves로 경계지어진 color 영역에 대한 이미지 표현          resolution independent하다는 점과, 고해상도로 displayed된다는 점이 장점이지만 display되기 이전에 rasterized되어야 한다는 점이 단점이다.        이번 챕터에서는 Raster Image를 다루며, 빛의 세기(light iintensity)와 연관된 pixel value를 어떻게 결정하는지에 대한 자세한 내용은 나중 챕터에서 다룰 예정이니 일단 기억해두기.1. Raster Devices  장치를 Raster한다는 것은 무슨 의미일까  input output을 간단한 조직도로 나타낸 사진은 아래와 같다.1.1. Displays  pixel value에 대한 고정된 2D array 기반으로 디스플레이되어지는데, 방식이 두 가지로 분류 가능함 1) Emissive(방출하는) Display : 픽셀들이 direct하게 조절가능한 light를 방출하는 방식   픽셀 자체가 광원이 됨  LED(Light-Emitting Diode)가 대표적인 예시2) Transmissive(투과하는) Display : 빛을 방출하진 않고, 대신에 투과할 수 있는 만큼의 빛을 픽셀이 갖고 있는 방식  transmissive display는 픽셀 행렬 뒤에 backlight가 필요함, 어느정도 illuminate(밝게 하다)될 만큼의 광원이 필요하기 때문  LCD(Light-Crystal Display)가 대표적인 예시    on-state(위, 전압 들어온 상태)가 되면 liquid crystal cell이 회전하면서 polarized light이 front plarizer를 투과할 수 있게 해주는 원리2. Images, Pixels, and Geometry  Raster Image는 이미지 픽셀별로 RGB 색상에 대한 값을 2차원 행렬 형태로 나타내는 것을 알고 있다.  이미지를 측정하거나 reproduce할 때, light energy에 대해 다음의 2차원 분포를 알아야 한다 ;                  light emitted from the monitor as a function of a position on the face of the display  = 디스플레이 표면에서 방출되는 빛                    light falling on a camera’s image sensor as a function of a position across the sensor’s plane  = 카메라의 이미지 센서로 들어가는 빛                    Reflectance(반사율) 또는 흡수되는 빛 대비 반사되는 빛의 비율(fraction)  = function of position on a piece of paper            We can abstract an “image” as a function $I(x,y)$: \\(I(x,y) : R -&gt; V\\)          grayscale 이미지인 경우에는 $V$가 양수구역이고, ideal한 color image인 경우에 $V$는 3차원 실수 공간영역이다.        continuous(연속적인)값으로 어떻게 Raster image가 표현되는 것인가.          Point sampling과 관련있다, 자세한 것은 chapter 9의 신호처리단원에서 다룬다고 한다      이미지의 색상에 대한 Local average를 pixel value정함      pixel value인 x를 구한다는 말은 즉, “the value of the image in the vincinity(주변) of the grid point is x”를 구한다는 것과 같은 말이다. (당연한거아님?)        이미지 width가 $n_x$, 높이가 $n_y$일 떄, image rectangular domain은 다음과 같이 평행이동을 0.5씩 했다면 top-right pixel도 똑같이 평행이동시킨 만큼으로 설정할 수 있다;\\(R = [-0.5, n_x - 0.5] \\times [-0.5, n_y - 0.5]\\)2.1. Pixel Values  Pixel formats :          비트 수가 줄어들면, artifact나 flaws(결함)부분이 이미지상에 생길 수도 있다      2.2. Monitor Intensities and Gamma  모니터는 pixel “value”를 digital input으로 받아서, “intensity” level(빛의 강도)로 변환함  인간의 강도에 대한 인지는 비선형적이라서 이 단원에서 논할 부분은 아님(chapter 20에 나옴)  모니터도 input에 대해서 non-linear하게 처리하는데, 예를 들어 모니터에게 0, 0.5, 1.0의 세 픽셀 value를 주었을 때, 디스플레이되는 intensity는 0, 0.025, 1.0으로 처리됨      이러한 근사 비선형성은 $ \\gamma $에 의해 결정됨\\(displayed-intensity = (maximum-intensity) a^\\gamma\\)    이 때, a는 아래 그림의 체커보드 이미지를 통한 standard technique을 통해 찾을 수 있는 값임 (calibration과 연관된 건가?)  “Gamma Correct”          gamma값을 알고 있으면, input을 감마 변형해서 $a = 0.5$ 즉, 흑백의 중간 강도에 대해 디스플레이되게 할 수 있음, 아래의 변형과정을 거친다      $a’ = a^{\\frac{1}{\\gamma}} $      \\[: displayed-intensity = (a')^{\\gamma} = (a^{\\frac{1}{\\gamma}})^{\\gamma}(maximum-intensity) = a(maximum-intensity)\\]            느낀점  pixel 정의나 이미지에 대한 matching function같은 기초적인 개념을 훑고 가는 시작 단원이다,, 별 내용이 없음  설날인데 여유롭게 카공도 하고 기분이 좋다.ㅋㅋ"
  },
  
  {
    "title": "Graphics STUDY",
    "url": "/posts/Graphics_study/",
    "categories": "Graphics, Study",
    "tags": "Graphics",
    "date": "2025-01-25 11:00:00 +0900",
    





    
    "snippet": "Fundamentals of Computer Graphics by Steve Marschner and Peter Shirley  교수님 추천 책, (“그래픽스에서 가장 유명한, 정석 같은 책이에요. 그래픽스의 거의 모든 분야 다룸”)  아직 차례만 보긴 했는데 깊게 알고싶었던 내용 총집합인 것 같아서 아주 마음에 든다Contents1. Introduc...",
    "content": "Fundamentals of Computer Graphics by Steve Marschner and Peter Shirley  교수님 추천 책, (“그래픽스에서 가장 유명한, 정석 같은 책이에요. 그래픽스의 거의 모든 분야 다룸”)  아직 차례만 보긴 했는데 깊게 알고싶었던 내용 총집합인 것 같아서 아주 마음에 든다Contents1. Introduction  1.1 Graphics Areas  1.2 Major Applications  1.3 Graphics APIs  1.5 Numerical Issues  1.6 Efficiency  1.7 Designing and Coding Graphics Programs 2. Miscellaneous Math  2.1 Sets and Mappings  2.2 Solving Quadratic Equations  2.3 Trigonometry  2.4 Vectors  2.5 Curves and Surfaces  2.6 Linear Interpolation  2.7 Triangles 3. Raster Images  3.1 Raster Devices  3.2 Images, Pixels, and Geometry  3.3 RGB Color  3.4 Alpha Compositing 4. Ray Tracing  4.1 The Basic Ray-Tracing Algorithm  4.2 Perspective  4.3 Computing Viewing Rays  4.4 Ray-Object Intersection  4.5 Shading  4.6 A Ray-Tracing Program  4.7 Shadows  4.8 Ideal Specular Reflection  4.9 Historical Notes5. Linear Algebra  5.1 Determinants  5.2 Matrices  5.3 Computing with Matrices and Determinants  5.4 Eigenvalues and Matrix Diagonalization 6 Transformation Matrices  6.1 2D Linear Transformations  6.2 3D Linear Transformations  6.3 Translation and Affine Transformations  6.4 Inverses of Transformation Matrices  6.5 Coordinate Transformations7 Viewing  7.1 Viewing Transformations  7.2 Projective Transformations  7.3 Perspective Projection  7.4 Some Properties of the Perspective Transform  7.5 Field-of-View8. The Graphics Pipeline  8.1 Rasterization  8.2 Operations Before and After Rasterization  8.3 Simple Antialiasing  8.4 Culling Primitives for Efficiency9. Signal Processing  9.1 Digital Audio: Sampling in 1D  9.2 Convolution  9.3 Convolution Filters  9.4 Signal Processing for Images  9.5 Sampling Theory10. Surface Shading  10.1 Diffuse Shading  10.2 Phong Shading  10.3 Artistic Shading11. Texture Mapping  11.1 Looking Up Texture Values  11.2 Texture Coordinate Functions  11.3 Antialiasing Texture Lookups  11.4 Applications of Texture Mapping  11.5 Procedural 3D Textures12 Data Structures for Graphics  12.1 Triangle Meshes  12.2 Scene Graphs  12.3 Spatial Data Structures  12.4 BSP Trees for Visibility  12.5 Tiling Multidimensional Arrays20단원까지 있는데 일단 리버털 끝나고나서 2월 말까지 여기까지 1회독 목표 학부 때 배운 패턴인식이랑 컴비때 배운 내용이 포함된게 많아서 복습할겸 모르는 내용 위주 공부해야겠다."
  },
  
  {
    "title": "MarchingCubes, SIGGRAPH 1987",
    "url": "/posts/MarchingCubes/",
    "categories": "Paper Review",
    "tags": "Mesh extraction, Graphics",
    "date": "2025-01-21 11:00:00 +0900",
    





    
    "snippet": "Abstract  output = 연속적인 density값을 갖는 surface의 triangle model  알고리즘 큰 흐름 : 3D medical data를 scan-line order에 처리한 후, 선형 보간(linear interpolation)을 이용해서 삼각형의 vertices(꼭짓점)을 구하는 것Introduction  mesh extr...",
    "content": "Abstract  output = 연속적인 density값을 갖는 surface의 triangle model  알고리즘 큰 흐름 : 3D medical data를 scan-line order에 처리한 후, 선형 보간(linear interpolation)을 이용해서 삼각형의 vertices(꼭짓점)을 구하는 것Introduction  mesh extraction하는 것은 medical image에서 굉장히 유용하게 많이 쓰임  새로운 3D surface construction 알고리즘인 “Marching Cube”는 연속적인 density surface를 가지는 물체에 대한 vertices를 추출함으로써 고해상도의 메쉬 만듦  3D Medical 알고리즘 흐름 :                            Data Acquistion          MR, CT, SPECT같은 기기로 환자 데이터 얻음                                      Image Processing          3D data의 전체적인 구조를 파악할 수 있는 기법 사용                                      Surface Construction          본 논문의 주제, 적절한 알고리즘 사용                    Display      Related Work      기존의 연구 (1) Connected Contour 알고리즘 : surface의 윤곽(contour)에서 시작하여 그것들이 서로 연속적인 삼각형이 되게 연결하는 접근법   -&gt; slice에 하나 이상의 contour가 존재해야하는데 이때 ambiguity(모호성)이 발생하여 정확도 떨어짐   -&gt; 원데이터의 inter-slice의 연결성을 무시함    (2) Cuberille 접근법 : cuberilles(작은 큐브인 voxel형태로 표현하는 구조)로부터 surface 구축하는 접근법   -&gt; 이 구조에서 gradient를 계산했을 때 그 값이 shading(그림자)영역의 지점을 찾는데에 이용되는데, 이게 정확하기가 쉽지 않음   -&gt; thresholding해서 3D space를 블록 단위 voxel처럼 표현하고 surface을 표현    (3) Ray casting   -&gt; 3차원 sensation을 생산하기 위해 motion에 의존함(?)   Marching Cubbe Algorithm (Method)  크게 2가지의 주요한 단계로 구성됨  divide-and-conquer 접근법  3D space상의 큐브 하나에서 다음 큐브로 넘어갈 때, surface가 어떻게 교차하는지를 찾는 것이 목표      이웃한 8개 픽셀(두 Slice 면의 4개 꼭짓점)로부터 logical한 cube가 Figure-2처럼 위치하게 세팅,        큐브의 꼭짓점(vertex) 데이터 값이 우리가 구성하는 surface의 vertex value값을 초과하면 1을 부여  마찬가지로 큐브 vertex value가 surface vertex value값보다 아래이면 0을 부여=&gt; surface의 외부에 존재하는 점 대략 이러한 과정으로 교차점들을 통해 surface를 복원하게 된다. 큐브마다 8개의 vertices &amp; 2개의 state(inside, outside)가 존재하기 때문에, 표면이 꼭짓점 1개 당 $2^8$가지 경우의 수로 교차될 수 있다.  해당 256가지 경우의 수에 대한 table을 만들 수 있으나 매우 따분하고 에러가 발생하기 쉽기 때문에 , 256가지 경우의 수를 14가지 패턴으로 줄일 수 있는 다음의 두 가지 대칭 속성을 이용함          큐브가 reverse로 뒤집혀도, surface value들은 그대로 동일하게 바뀌지 않음              Rotational 대칭                        ‘’‘(ex) 0번 패턴 : 모든 vertices가 0으로 선택된 케이스(혹은 모두 1) =&gt; 삼각형을 생산하지 않게 됨          1번 패턴 : surface가 1개의 vertex를 나머지 7개의 vertices에 대해 분리시킨 상태 =&gt; 작은 삼각형 1개           ….  이 때, 윗 그림처럼 8개 vertices(v1~v8)와 각 vertex 사이의 bit index 12개(e1~e12)를 numbering하여 edge intersection을 고려한다.  그 후 surface와 맞닿는 edge가 어떤 것인지 알았으면, 이 edge들 사이에서 linear interpolation(선형 보간)을 수행한다.  마지막 단계로 각 triangle vertex에 대한 unit normal(단위 법선벡터)를 계산한다.          이렇게 구한 normal로 Gouraud-shaded image를 렌더링할 때 사용됨, 즉 명암 넣기 단계임                  고러드 쉐이딩(Gouraud Shading)                    Details :                  surface 의 normal은, surface의 접선 방향에 대한 gradient vector이다.                      direction of gradient vector를 $\\vec{g}$로 표기\\[\\vec{g}(x,y,z) = \\Delta{\\vec{f}(x, y,z)}\\]                                    $\\Delta{\\vec{f}(x,y,z)}$ 를 구하기 위해 3방향에서의 gradient vector를 아래처럼 계산한 후에 선형보간을 하여 surface 복원    \\(G_x(i,j,k) = \\frac{D(i+1, j, k) - D(i-1, j, k)}{\\Delta{x}}\\)     \\(G_x(i,j,k) = \\frac{D(i, j+1, k) - D(i, j-1, k)}{\\Delta{x}}\\)    \\(G_x(i,j,k) = \\frac{D(i, j, k+1) - D(i, j, k-1)}{\\Delta{x}}\\)        In summary, marching cubes creates a surface from a three-dimensional set of data as follows: (논문 표현)          Read four slices into memory.      Scan two slices and create a cube from four neighbors on one slice and four neighbors on the next slice.      Calculate an index for the cube by comparing the eight density values at the cube vertices with the surface con- stant.      Using the index, look up the list of edges from a precal- culated table.      Using the densities at each edge vertex, find the surface- edge intersection via linear interpolation.      Calculate a unit normal at each cube vertex using central differences. Interpolate the normal to each triangle ver- tex.      Output the triangle vertices and vertex normals      Enhancements to the Basic Algorithm  "
  },
  
  {
    "title": "SuGaR, CVPR 2024",
    "url": "/posts/SuGaR/",
    "categories": "Paper Review",
    "tags": "Mesh reconstruction, 3D Gaussian Splatting",
    "date": "2025-01-20 11:00:00 +0900",
    





    
    "snippet": "Abstract  precise and extremely fast mesh extraction from 3DGS representation  state-of-the-art method on SDFs, while providing a better rendering qualityContributions  Regularization term         ...",
    "content": "Abstract  precise and extremely fast mesh extraction from 3DGS representation  state-of-the-art method on SDFs, while providing a better rendering qualityContributions  Regularization term          Gaussian Splatting 최적화 중의 loss term에 추가시켜서 3D Gaussian들이 표면에 잘 정렬되게 하는 역할      3D Gaussian의 surface geometry(density &amp; SDF)를 이용함        Refinement strategy          mesh에 있는 gaussian들 삼각형으로 묶어주면서 refinement함      Introduction  mesh extraction task는 3DGS representation의 explicit한 특성 때문에 더 어려움  3DGS가 잘 최적화되었으면, 가우시안들이 평평하고 표면에 잘 분포되어있다는 가정을 얻을 수 있음          이때 gaussian density와 관련된 geometry를 이용해서 Loss term에 추가하면서 gaussian으로부터 mesh추출이 더 쉬워지게 만듦      volumne density 사용?        Marching Cube 알고리즘 대신 Poisson Reconstruction 알고리즘 사용해서 point cloud로부터 mesh extraction을 했다Method1. Aligning the Gaussians with the Surface (=Regularization)  목표 : 3DGS가 잘 최적화되어있다는 가정 하에, Gaussian의 SDF(Signed Distance Function)를 이끌어내는 것      optimized된 가우시안으로부터 예측된 SDF와 실제의 SDF간의 차이를 최소화함으로써 가우시안들이 평평하게 surface align된 특성을 갖을 수 있도록 encourage        최적화된 Gaussian Splatting Scene이 주어져있는 상황에서 시작    Gaussian의 density function $d(p)$ :\\[d(p) = \\sum_g \\alpha_g exp(-\\frac{1}{2}(p-\\mu_g)^T\\sigma_g^{-1}(p-\\mu_g))\\]  &lt; Property 1 &gt;1️⃣surface에 가까운 point $p$에 가까이 위치한 Gaussian $g^*$이 density function $d(p)$에 기여하는 정도가 크다\\[g^* = \\arg\\min_g(p-\\mu_g)^T\\sigma_g^{-1}(p-\\mu_g)\\]해석 = g*말고 나머지 가우시안들, g들에 대한 밀도가 최소가 되게 하면 된다.⇒ 이 성질을 만족하면, 가우시안들이 scene에서 잘 spread되어있다, 즉 scene에 가우시안들이 잘 퍼져있다는 가정을 만족&lt; Property 2 &gt;2️⃣잘 최적화된 3DGS scene에서는 Gaussian들이 평평하다 = scaling factor 3방향 벡터 중에서 하나는 0에 가까워야 함, (길이 짧아야 함)\\[(p-\\mu_g)^T\\Sigma_g^{-1}(p-\\mu_g) \\approx \\frac{1}{s_g^2}&lt;p-\\mu_g, n_g&gt;\\][notation]  $s_g$: 가장 짧은 scaling factor  $n_g :$ scaling factor에 대응되는 축에 대한 방향, normal(법선 벡터)처럼 생각해도 됨⇒ 결과적으로 surface-align한 density function $\\overline{d}(p)$\\[\\overline{d}(p) = exp(-\\frac{1}{2s_{g^*}^2} &lt;p-\\mu_{g^*}, n_{g^*}&gt;^2)\\]  A : d(p) 밀도함수를 따르는 가우시안,  B : $\\overline{d}(p)$ 밀도함수를 따르는 가우시안&lt; Optimize Term &gt;  $|d(p) - \\overline{d}(p)|$: 위의 density volume을 이용한 optimization term 을 3DGS loss에 추가          밀도를 이용하는 지금 optimize term 일단 좋긴 좋은데,,,density말고 SDF(Signed Distance Function) 활용하는 것도 추가시키면 surface-align Gaussian을 얻는 것에 더 좋다고 함    - 평평한 가우시안이 주어졌을 때, 즉 Gaussian $g$의 scaling factor들이 $s_g$ = 0 인 상황에서, point $p$와 true surface와의 거리 : $|&lt;p-\\mu_{g’}, n_{g’}&gt;|$      \\[SDF : \\overline{f}(p) = \\pm s_{g^*}\\sqrt{-2log(\\overline{d}(p))}\\]\\[ideal-SDF : {f}(p) = \\pm s_{g^*}\\sqrt{-2log({d}(p))}\\]   $|\\overline{f}(p)-f(p)|$ : 위의 SDF를 이용한 optimization term을 통해 표면에 더 잘 정렬된 가우시안들을 얻을 수 있었다.   Regularization term $R$ :\\[R =  \\frac{1}{|P|} \\sum_{p \\in P} |\\overline{f}(p) - f(p)|\\]      SDF의 normal(법선 벡터)에 대한 regularization term도 있음\\[R_{Norm} = \\frac{1}{|P|}  \\sum_{p\\in P} || \\frac{\\nabla f(p)}{|| \\nabla f(p) ||^2} - n_{g^*} ||_2^2\\]  2. Efficient Mesh Extraction (=Poisson Reconstruction)  최적화된 3DGS scene에서 계산된 가우시안들의 density로부터 3D points를 일정 level set에 대하여 샘플링함 =&gt; point clouds 구함                  이 때, level set은 level parameter인 $\\lambda$에 의해 결정됨                          샘플링된 Points 기반으로 Poisson reconstruction 수행하여 mesh 추출    참고. Poisson Reconstruction 정리 글  3. Binding New Gaussians to the Mesh (=Refinement)  Barycentric 좌표계 :  삼각형 또는 다면체 내부의 점을 해당 도형의 꼭짓점에 대한 가중치로 표현하는 좌표계          (ex) 삼각형의 경우:                              삼각형의 꼭짓점을 A, B, C라 할 때, 내부의 임의의 점 P는 다음과 같이 표현됩니다:            $P = \\alpha A + \\beta B + \\gamma C$                                여기서 α,β,γ는 가중치로, 아래 조건을 만족합니다:            $\\alpha + \\beta + \\gamma = 1$                                    논문 표현 :    Also, the Gaussians have only 2 learnable scaling factors instead  of 3 and only 1 learnable 2D rotation encoded with a complex number rather than a quaternion, to keep the Gaussians flat and aligned with the mesh triangles.        quaternion이란: 3D 회전을 표현하기 위해 사용되는 수학적 구조로, 복소수의 확장된 형태    $q=w+xi+yj+zk$  Experiment  single GPU Nvidia Tesla V100 SXM2 32 Go느낀점  3DGS에서 거의 최초로 mesh reconstruction태스크를 수행해서 성능이 좋게 나왔다는 것이 의의  요즘 모델들의 거의 baseline 시초급(?)으로 봐도 무방하다  포아송 재건방법이 아직 뭔지 잘 모르겠다. marching cube공부할 때 같이 공부  어렵긴 한데 재밌다."
  },
  
  {
    "title": "GS2Mesh, ECCV 2024",
    "url": "/posts/GS2Mesh/",
    "categories": "Paper Review",
    "tags": "Mesh reconstruction, Surface reconstruction, 3D Gaussian Splatting",
    "date": "2025-01-19 15:00:00 +0900",
    





    
    "snippet": "GS2Mesh :Surface Reconstruction from Gaussian Splatting via Novel Stereo Views&lt; 선정 이유 &gt;  SuGaR (CVPR 2024)논문 이후로 유의미하게 3dgs의 mesh recon 태스크에서 성능이 sota달성하였다는 점,  baseline에 SuGaR가 존재한다는 점Abstra...",
    "content": "GS2Mesh :Surface Reconstruction from Gaussian Splatting via Novel Stereo Views&lt; 선정 이유 &gt;  SuGaR (CVPR 2024)논문 이후로 유의미하게 3dgs의 mesh recon 태스크에서 성능이 sota달성하였다는 점,  baseline에 SuGaR가 존재한다는 점Abstract  noisy한 3DGS representation으로부터 smooth한 3D mesh representation을 얻는 것 어려움  pre-trained stereo-matching model사용해서 scene에 대한 geometry 활용함          stereo-aligned 되어있는 image pair를 얻고, 이를 앞선 stereo matching모델에 넣어서 depth추출하여 geometry 이용함        more smoother, more accurate mesh extraction 가능          Mesh Extraction 알고리즘 : TSDF(Truncated Signed Distance Function) 이용한 Marching cube      Contributions  pre-trained stereo-matching 모델을 통해 Image pair의 geometry 활용해서 mesh reconstruction 태스크의 sota성능을 달성하였다.          사용한 스테레오 매칭 모델 : DLNR                  stereo calibrated image pair를 모델 input으로 사용하여 이미지 쌍에 대한  correspondence 문제를 해결 + depth 추출                      TSDF(Truncated Signed Distance Function)와 depth 정보를 활용해서 mesh recon하는 알고리즘 (marching cube기반) 사용하였다. Introduction  Gaussian의 explicit한 요소만으로 geometrically consistent한 surface를 추출하는 것은 어려움          image plane(2D)에 back projected 되었을때 best matching되게끔 최적화된 가우시안이기 때문에, mesh reconstruction 에서는 오히려 가우시안 representation이 단점이 됨            stereo-aligned된 이미지 쌍에서 stereo-matching 모델(DLNR)을 사용해서 정확한 depth 측정한 후, TSDF 활용한 Depth-fusion기반 mesh extraction 알고리즘 (Marching cube) 사용하는 파이프라인    데이터셋 : TnT(Tanks and Temples) &amp; DTU(작은 object mesh 데이터셋임) 에서 sota성능을 보여주었다.MethodStep 1. Scene Capture &amp; Pose Estimation  COLMAP의 SFM(Structure From Motion)을 통해 카메라 파라미터들을 얻어내고, sparse한 3D point cloud를 재건한다.Step 2. Stereo-aligned Image Pairs 생성  3DGS에서 photometric loss를 통해 최적화되어서 image plane에 back-projected된 가우시안들 기반으로 stereo-aligned(같은 베이스라인b 선상에 존재하는, 카메라 포즈(rotation, translation)은 그대로에 baseline b길이만큼만 떨어진) 한 쌍으로 만드는 과정을 수행한다.          \\[R_R = R_L\\]\\[T_R = T_L + (R_L \\times [b, 0, 0])\\]   수식 Notation 설명   - $R_R$: 오른쪽 카메라의 rotation matrix    - $R_L$: 왼쪽 카메라의 rotation matrix    - $T_R$: 오른쪽 카메라의 translation matrix    - $T_L$: 왼쪽 카메라의 translation matrix  Step 3. Stereo Depth Estimation  input : a pair of stereo-calibrated cameras      DLNR(High Frequency Stereo matching Network) 모델 이용        DLNR pipeline          multiscale decouple LSTM 구조를 따름 + Disparity Normalization 수행하는 것이 핵심      논문까지 자세힌 아직 안읽어봄, 걍 stereo matching model 성능 중에 sota라고 함            이 모델 output에다가 아래의 mask 2개 정도 추가시켜 reconstruction성능 향상              occlusion mask                  left-to-right disparity랑 right-to-left disparity 차이 이 두개 사이에서 thresholding해서 구해짐                    depth-shading                  stereo-matching error $\\epsilon(Z)$        \\[\\epsilon(Z) = \\frac{\\epsilon(d)}{f_x \\cdot B} Z^2\\]                  $Z$ :  ground-truth depth          $d$ : disparity          $f_x$ : 수평축 카메라 focal length       - error가 baseline B 길이 값이 커질수록, 즉 stereo-paired image 사이의 간격이 클수록 에러가 작아지는 반면, occlusion이 심해질 수 있음          4B ≤ Z ≤ 20B 에 속하는 깊이만을 고려함                    Step 4. Depth Fusion into Triangulated Surface (mesh)  추출된 depth 정보들을 TSDF(Truncated Signed Distance Function) 기반 mesh reconstrucction 방법에 통합시킨다                  TSDF Cube Model이란?        : 깊이 영상 으로부터 3차원 공간 표면을 효과적으로 표현하기 위해,        전체 공간을 일정한 크기의 정육면체 복셀(voxel)들로 구성된 커다란 하나의 큐브(cube)로 표현하고, 각 복셀에는 물체 표면과의 거리를 나타내는 TSDF값과 그 값의 신뢰도를 나타 내는 가중치(weight)를 함께 저장하는 방식                    etc에 TSDF 개념 간단하게 추가              Marching cube : mesh 만들어주는 알고리즘          sdf나 tsdf같은 함수 활용되며, surface representation 방식 기반으로 mesh 뽑아냄      Experiment  ground truth point cloud랑 reconstructed point cloud간의 Chamfer Distance(CD)계산을 통해 evaluation  Baseline : SuGaR(CVPR 2024), BakedSDF, Neuralangelo, VolSDF, NeuS, MVSformer,  데이터별로 실험 진행 얘기  평가 지표 : Chamfer-Distance(CD): 두 point cloud 집합간의 거리 측정, F1, AccuracyLimitation  오른쪽 스테레오 매칭 모델은 투명한 표면에서 어려움을 겪는다.  (왼쪽) 원래 학습 이미지에서 충분히 다루어지지 않은 영역에서 floater를 생성한다.  TSDF 퓨전은 큰 장면(넓은 baseline B)에 맞게 확장되지 않는다.etc., (Preliminaries)기초개념 정리기초 개념      Mesh란?    : 3D 공간상에 존재하는 점들(Vertex/ Point) 과 그 점 3 개의 집합인 면(Polygon/face)들로 이루어진 3D 공간 표현방법        Voxel이란?    :이미지의 pixel 처럼 3D 공간을 표현하기 위해서 3D공간을 작은 단위 공간으로 쪼갠 것 –&gt; 3차원 공간을 grid로 쪼갰다고 보면 편함  TSDF (Truncated Signed Distance Function)  3D scene reconstruction의 목적은 Surface 를 찾아 recon 하는 것인데 이때 surface 를 표현하는 함수를 SDF(Signed Distance Function) 라고 함      Voxel 형태로 단위공간을 나누어 surface 라고 판단되는 곳은 0, Surface 안쪽은 음수 , Surface 바깥쪽은 양수로 표현하는 방식      Marching Cube (SIGGRAPH 1987)  과정 다 생략하고 한 줄 요약 : 3D point cloud로부터 Mesh 생성 알고리즘2차원 image plane에서 물체가 빨간색 선처럼 생겼다고 생각해보자, (3차원에서는 voxel임)이것을 2차원 도트로 표현하면 아래와 같음⇒ 원형의 물체랑 너무 달라짐, 해상도 차이 발생, 모양 이상해지는 결과따라서, 마칭 큐브 알고리즘으로 surface reconstruction을 진행한다. (…TLDR)  About Marching Cube Algorithm, tistory=&gt; 마칭 큐브 알고리즘 다음 게시물에 정리 이어서,, 느낀점  Marching Cube 알고리즘부터 완벽하게 이해를 해보자          항상 간단한 정리글로만 읽고 넘어가니까 그냥 point cloud넣고 mesh뽑아주는 그래픽스 알고리즘이다정도라고만 알고 넘어가서 모호하다, 알짜배기를 모르는 느낌      SuGaR에서는 Poisson reconstruction기반의 mesh recon알고리즘을 썼다고 되어있었는데, 이게 나는 마칭 큐브랑 완전 다른 건 줄 알았는데 또 읽다보니 포아송재건도 마칭큐브기반이라는 소리도 있고 출처가 정확하지 않으니까 혼동된다, 그래서 Poisson Reconstruction 논문도 읽어야겠다        eccv논문인데 생각보다 노벨티가 뭐가 없다          그냥 stereo-matching 모델 써서 depth추출하고 이거 기반으로 point cloud를 더 정확하고 밀도있게 뽑아내고 그 후로는 그냥 알고리즘 사용해서 메쉬추출한건데,, 성능이 좋았다는게 신기하다.      대신 단점이 명확하다, stereo 기법이다보니 위의 triangulation사진을 보다시피 베이스라인 길이에 한정된 씬만 사용될 것이므로 넓은 즉 큰 반경의 scene에 대한 mesh recon은 잘 안될 것이다 (실제로 사용한 데이터셋들도 다 작은 object based 벤치마크들이다)      3DGS도 그렇고, mesh recon도 그렇고 고질적인 문제가 투명한 transparent한 물체가 잘 복원이 어렵다는 점인데, 이부분의 개선은 왜 안되고 있는지 렌더링 측면에서 공부를 좀 더 해봐야겠다.      "
  },
  
  {
    "title": "블로그 오픈",
    "url": "/posts/First_Blog/",
    "categories": "Blogging, Tutorial",
    "tags": "personal",
    "date": "2025-01-18 02:34:00 +0900",
    





    
    "snippet": "First Blog안녕~! 데스크탑 운영체제가 리눅스여서 도커 연동이 생각만큼 잘 되지 않아 깃헙 블로그를 만드는 걸 계속 미뤄왔었다. 오늘 맥북이 새로 와서 심심해가지고 블로그 미뤄뒀던 걸 다시 도전해봤다. 아직 favicon은 반영은 뭐땜시 아직 안되고 있고,, 그대로 테마 갖다 쓰는건데도 생각보다 뭐가 잘 안되서..생각보다 오래걸렸다. 글 쓰고 ...",
    "content": "First Blog안녕~! 데스크탑 운영체제가 리눅스여서 도커 연동이 생각만큼 잘 되지 않아 깃헙 블로그를 만드는 걸 계속 미뤄왔었다. 오늘 맥북이 새로 와서 심심해가지고 블로그 미뤄뒀던 걸 다시 도전해봤다. 아직 favicon은 반영은 뭐땜시 아직 안되고 있고,, 그대로 테마 갖다 쓰는건데도 생각보다 뭐가 잘 안되서..생각보다 오래걸렸다. 글 쓰고 올리는 건 자동화가 잘돼있어서 괜찮을 것 같아 꾸준히 스터디 겸 근황을 이곳에 올리도록 하겠다.맥도 아직 익숙하지 않아서 다소 헤맸지만 좀 더 익숙해지면 활용도가 매우 높을 것 같아 만족한다!  WELCOME MY GITHUB BLOG, I am a Master student in Computer Vision Lab, Korea UniversityNext time you visit our site, this place will be much more developed and awesome."
  }
  
]

