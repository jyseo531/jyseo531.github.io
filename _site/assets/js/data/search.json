[
  
  {
    "title": "Pixie, arxiv preprint 2025",
    "url": "/posts/Pixie/",
    "categories": "Study, Paper-review",
    "tags": "3D Physics prediction",
    "date": "2025-09-27 13:00:00 +0900",
    





    
    "snippet": "paper linkAbstract  태스크: 3D scene의 물리적 특성 복원  Novel method that trains a generalizable neural network to predict physical properties across multiple scenes from 3D visual features purely using supe...",
    "content": "paper linkAbstract  태스크: 3D scene의 물리적 특성 복원  Novel method that trains a generalizable neural network to predict physical properties across multiple scenes from 3D visual features purely using supervised losses  학습시키고 나면, 우리의 feed-forward network는 빠르고 정확하게 “material fields”를 추론할 수 있다  가우시안 스플래팅으로 표현된 정적인 static scene을 현실적으로 물리기반 시뮬레이션하는 scene으로 변형 가능  PixieVerse 데이터셋 ⇒ 3D assets + physical material annotation포함  CLIP같은 pretrained visual features 활용해서 zero-shot일반화          real-world scene + synthetic data(이게 더 학습되나봄)      Introduction  NeRF + 3DGS ⇒ learning-based scene reconstruction 네트워크          sparse camera 뷰로부터 3D geometry와 장면을 사실적으로 복원할 수 있음      visual appearance에만 집중해서 표현함                  geoemtry와 scene의 color만          physical properties 부재함                      기존 3D scnee의 physical properties복원의 메소드를 크게 두가지로 분류됨          사용자가 도메인 지식을 바탕으로 장면 전체에 대한 재질 파라미터를 직접 지정하도록 요구(?)      테스트 시점 최적화(test-time optimization)를 통해 재질 추출 과정을 자동화        PIXIE : geometry + appearance + physical learning 통합해서 예측  PixieVerse 데이터 :  1,624개의 paired 3D objects &amp; annotated materials          spanning 10 semantic classes      Contributions  3D Physics Prediction에서의 새로운 프레임워크 제안          이산적인 물질의 타입을 예측 &amp;      연속적인 physical parameter (Young’s modulus, Poisson’s ratio, density)를 예측        PIXIEVERSE 데이터셋 제안          3D objects (from Objaverse) + physical material annotations 제공      1624종류의 물체 + 10개의 semantic한 클래스        Inference가 빠르고 generalizable함          CLIP의 pretrained visual feature와 3D U-Net의 feed-forward활용하여 빠른 테스트타임 최적화        Real scene에서의 zero-shot generalization 성능          합성데이터로만 학습함에도 ㅂ루구하고, real-world scene에 잘 작동함      sim-to-real gap완화        MPM(material point-method) Solver와 함께 Seamless integration 성능          물리 시뮬레이션을 위해, 예측된 material field는 Gaussian Splatting모델이랑 같이 합쳐져서 렌더링될 수 있음 ⇒ wind, gravity같은 애니메이션      Method  기본 논지: 3D 시각적 외형(기존 NeRF와 3DGS같은 뉴럴렌더링(i.e., volumetric reconstruction model)으로 얻어지는 texture, shading, shape featrue 포함) 만으로도 객체의 물리적 파라미터를 복원하기에 충분한 정보를 제공한다는 것  Young’s modulus(영률) &amp; Poisson’s ratio(포아송비)같은 물성 예측          영률: 물체를 잡아당겼을 때, 얼마나 단단하게 버티는지를 나타내는 값(뻣뻣함의 정도)      포아송비: 물체를 잡아당겼을 때, 옆으로 얼마나 퍼지는지를 나타내는 값 (길게 늘릴 때 옆으로 줄어드는 비율)        3DGS같은 reconstruction model에 point-wise한 물질 추정을 같이 augment함  CLIP visual prior 활용하고 physics solver에게 물체가 외부 요소(wind, gravity)에 의해 반응할 수 있도록 애니메이션 주입시키는 데에 사용함  pixieverse 데이터를 제안해서 우리 모델 학습하는데 사용했음\\[\\begin{align} f_{\\theta} : (\\mathcal{I}, \\Pi) \\longrightarrow \\hat{\\mathcal{M}}\\end{align}\\]  여기서 $\\mathcal{I}={I_k}_{k=1}^K $는 정적인 3D scene을 의미함      통합된 카메라 파라미터인 $\\Pi$ 를 연속적인 3차원의 “material field” $\\hat{\\mathcal{M}}$로 매핑하는 것이 목표인 것        각 3차원 포인트 p는 아래의 재질 필드(material field)을 반환한다\\(\\begin{align}\\hat{\\mathcal{M}} = \\left( \\hat{\\mathcal{l}}(p),\\; \\hat{E}(p),\\; \\hat{\\mathcal{v}}(p),\\; \\hat{\\mathcal{d}}(p) \\right)\\end{align}\\)    여기서 l은 이산적인 material의 종류를 나타내는 클래스가 무엇인지에 대한 거      E는 연속적인 영률(Young’s modulus), v는 포아송비, d는 밀도값을 각각 나타냄    l: 이산적인 material의 종류를 나타내는 클래스 =&gt; “constitutive law(구성법칙)”로도 알려짐  &lt;Material Point Method&gt;에 의하면, 이산적인 재료 클래스(즉, 구성 법칙)는 전문가가 정의한 초탄성 에너지 함수 $\\mathcal{E}$와 리턴 매핑(return mapping, $\\mathcal{P}$)의 조합으로 게산됨  이러한 point-mapping기반 방법은 fine-grained한 각 공간적 위치마다 물질의 재료 segmentation을 제공해줌  우리는 그래서 그 위치에 “semantic material label”과 함께 “physical parameter”를 같이 할당함  2D image 로부터 물성 바로 알기 쉽지않음, 따라서 우리는 “distilled feature field”를 활용해서 더 많은 visual prior를 표현할 수 있게 했음  또한 3D visual feature와 physical material사이의 매핑을 계산하기 위해 U-Net 아키텍처를 분리했음3D Visual Feature Distilaltion  CLIP같은 모델으로 dense 2D visual feature embedding이 뽑히는데, 이 피쳐를 3D로 lifting하여 volumetric representation으로 넘어가질 수 있음  전통적인 NeRF 표현을 사용하여 color와 density에 추가적으로 뷰에 상관없이 독립적인 feture vector를 에측한다 아래 식 참고.\\[\\begin{align} F_{\\theta} : (\\mathcal{x}, \\mathcal{d}) \\longrightarrow \\left(\\mathcal{f}(\\mathcal{x}), c(\\mathcal{x}, \\mathcal{d}), \\sigma({\\mathcal{x}}) \\right)\\end{align}\\]  c랑 $\\sigma$는 NeRF에서 나오는 아웃풋이고  f는 high-dimensional descriptor, visual semantic을 포착함(물체의 정체성이나 또는 다른 특성들) =&gt; view-independent함  RGB이미지의 color와 CLIP으로 뽑힌 2D visual feature를 pixel-wise supervised하여 학습  학습 이후, “scene bound”로 알려진 feature field를 그리드 $\\mathcal{F}_G$로 복셀화시킨다 (차원=$N \\times N \\times N \\times D$), which N=64 and D =768(Clip feature dim)  이 그리드가 “Material Network”의 인풋이 됨!Material Grid Learning  material learning network : $f_\\mathcal{M}$는 feature projector $f_P$와 U-Net $f_U$로 구성되어 있음(1) feature projector $f_P$: CLIP feature가 768차원으로 굉장히 high-dimensional하기 때문에, 이 고차원을 저차원으로 매핑해주는 역할          3 layers of 3D convolution 네트워크(2) U-Net architecture $f_U$: 투영된 feature Grid $\\mathcal{F}_G$를 material grid $\\hat{\\mathcal{M}}(\\mathcal{p})$로 매핑하는 것을 학습함      material field인  $\\hat{\\mathcal{M}}$의 복셀화된 버전.      &lt;/br&gt;위의 두가지 네트워크는 같이 end-to-end로 학습되고 cross-entropy와 mean-squared error loss를 이용해서 “이산적인 material classification”과 “연속적인 물성들(e.g., Young’s modulus &amp; Poission value)”을 예측함  우리의 voxel griddml 98%는 배경부분으로 매우 sparse함          따라서 material netowrk $f_M$은 거의 대부분 배경만 예측하려고 할것이다        이러한 문제를 해결하기 위해서 “occupancy mask grid $\\mathbb{M}$”을 계산함          NeRF 표현의 “밀도 값”기준으로 thresholding($\\alpha = 0.01$)해서 그 밑의 복셀들을 필터링하는 것        이렇게 필터링하고 남은 복셀들(occupied voxels)에서만 discrete cross-entropy와 continuous mean-sqaured error loss로 아래수식처럼 supervised learning시킴\\[\\begin{align}\\mathcal{L}_{\\text{sup}} &amp;= \\frac{1}{N_{\\text{occ}}}    \\sum_{p \\in \\mathcal{G}} \\mathbb{M}(p)    \\Big[       \\lambda \\cdot CE\\big(\\hat{\\ell}(p), \\ell^{GT}(p)\\big)       + \\big(\\hat{E}(p) - E^{GT}(p)\\big)^{2} \\notag \\\\&amp;\\qquad\\qquad      + \\big(\\hat{\\nu}(p) - \\nu^{GT}(p)\\big)^{2}       + \\big(\\hat{d}(p) - d^{GT}(p)\\big)^{2}    \\Big] ,\\end{align}\\]  $N_{\\text{occ}} = \\sum_{p \\in \\mathcal{G}} \\mathbb{M}(p)$ 는 그리드 내에서 \\textit{점유된 복셀(occupied voxels)의 총 개수}를 의미하며, $\\hat{\\ell}(p)$ 와 $\\ell^{GT}(p)$ 는 각각 예측된 재질 클래스 로짓(predicted material class logits)과 정답값(ground-truth)을 나타냄  $CE$ : 크로스 엔트로피 손실(cross entropy loss), $\\lambda$ 는 손실 가중치 계수(loss balancing factor)  $E, \\nu, d$ 는 각각 영률(Young’s modulus), 포아송비(Poisson’s ratio), 밀도(density) 값을 나타냄Physics Simualtion  MPM(Material Point Method)사용해서 물리 시뮬레이션 수행  MPM solver는 point cloud를 인풋받아서 예측된 material field 속성을 보여줌 + 외부의 특수상황(e.g., wind &amp; gravity) 시뮬레이션으로 변형과 이동  NeRF 모델에서 입자(particle=&gt; point cloud)를 샘플링하는 것이 가능하긴 하지만(예: 포아송 디스크 샘플링), 우리는 각 가우시안을 자연스럽게 MPM 입자로 간주할 수 있기 때문에 가우시안 스플래팅(Gaussian Splatting) 모델을 사용하는 것이 더 쉽다는 것을 발견함  따라서 자세(poses)가 주어진 다중 뷰 RGB 영상으로부터 별도로 가우시안 스플래팅 모델을 학습시킴  이후, 예측된 재질 그리드에서 얻은 재질 속성을 최근접 이웃 보간법(nearest neighbor interpolation)을 통해 가우시안 스플래팅 모델로 전이          그니까 재질 필드는 NeRF이용해서 학습하되, 마지막 시뮬레이션을 위해서만 가우시안 스픞래팅 모델 렌더링된거에서 재질 속성들만 “전이”하는 거임 (via nearest neighbor interpolation)      쥰나 비효율적인데?      PixieVerse Dataset"
  },
  
  {
    "title": "3D-LLM, NIPS 2023",
    "url": "/posts/3DLLM/",
    "categories": "Study, Paper-review",
    "tags": "Multimodal Language models, 3D LLM",
    "date": "2025-09-14 13:00:00 +0900",
    





    
    "snippet": "paper linkAbstract  3D 공간의 정보 즉,  spatial relationships, physics, layout같은 더 많은 정보를 언어모델에게 주입해서 확장해보자  3D point cloud와 그것에 대한 feature + LLM query(prompt)를 인풋으로 줌  가능한 3D-related tasks : 3D QA, navi...",
    "content": "paper linkAbstract  3D 공간의 정보 즉,  spatial relationships, physics, layout같은 더 많은 정보를 언어모델에게 주입해서 확장해보자  3D point cloud와 그것에 대한 feature + LLM query(prompt)를 인풋으로 줌  가능한 3D-related tasks : 3D QA, navigation, 3D grounding, captioning, ..  크게 세가지의 프롬프트 메커니즘을 통해 3D-language Data를 만들었다  학습 방법 : 멀티뷰 이미지들로부터 3D Feature extractor를 통해 3D feature(point cloud)를 얻은 이후 이거를 2D VLM 백본에 넣어주어 3D LLM학습하게 됨Introduction  기존 LLM : chatbot으로써의 역할, communication, commonsense reasoning에 유능함 ⇒ 텍스트 중심  요즘의 Multimodal-LLM : image, video를 text와 align하여 2D 공간 비주얼 정보로부터 reasoning가능 (e.g., Flamingo, BLIP-2)⇒ 3D 공간적 특성을 추론할 수 있는 LLM은 왜 없는가?Challenges(1) 데이터의 부족  3D Data-Language가 align되어있는 데이터가 없다  그래서 본 논문에서 만들었다,  how? 데이터 만드는 파이프라인에서 ChatGPT와 3가지의 효율적인 프롬프트를 활용해서 약 300K의 3D-language data만들었고 이거를 토대로 다양한 관련 태스크들(3D captioning, dense captioning, 3D QA, 3D task decomposition, 3D grounding, 3D-assisted dialog, navigation)가능(2) 3D LLM모델에게 언어 텍스트와 align시킬 3D feature를 어떻게 줄 것인지에 대한 모호성  3D encoder활용하는 방법          CLIP에서 2D image끼리 텍스트간 학습했던 것처럼, 3D feature와 language간의 contrastive learning 패러다임 활용하는 방법      시간소요, GPU자원, 데이터 양이 많이 필요하다는 점에서 별로임        3D feature extractor 활용하는 방법(2D multi-view images로부터) ⇒ 채택          BLIP-2나 Flamingo같은 2D VLM모델에서는 2D pretrained시킨 CLIP feautre를 활용하기 때문에 이 2D-VLM모델을 백본으로 삼아서 3D feature를 우리모델의 인풋으로 주는거다 (??)      Contribution 정리  3D-LLM모델 : 3D Point cloud &amp;  language prompt인풋으로 받음 ⇒ 3D-related 다양한 태스크 수행 가능  새로운 데이터 수집 파이프라인 제안 : 대규모 3D-language data 300k개 이상으로 구성  3D feature extractor 활용해서 렌더링된 2D 멀티뷰 이미지들로부터 3D feature 추출하여 활용 + 2D pretrained VLM을 백본으로 사용 + 3D localization 메커니즘 소개 (3D 공간적 정보를 더 잘 이해하게 함)  실험적 얘기 ⇒ ScanNet데이터셋에서 더 BLEU-1 score 소타성능보다 9퍼센트 더 높아짐3D-language Data Generation  최신의 multi-modal data(2D 이미지들과 text pair로 구성된)는 접근성이 좋지만 3D asset에 대해서는 그것들의 “scarcity(희소성)”와 “3D asset에 대한 language data의 부재”로 인해 3D관련 multi-modal data가 많지 않음      기존 존재하는 3D-language 데이터들 : ScanQA, ScanRefer ⇒ 퀄리티와 다양성 측면에서 한계가 있고, 데이터당 하나의 태스크만 가능하다는 점에서 제한적임    본 논문에서 새로 만든 데이터 : 다양한 3D관련 데이터 가능한 벤치마크임          아래 3개의 효율적인 프롬프트 단계들을 거쳐서 데이터 만듦                  Boxes-Demonstration-instruction prompting                          3D 공간에서의 방과 물체들에 대한 AABB(Axis-Aligned Bounding Boxes)를 LLM(ChatGPT) 인풋으로 줘서 장면에 대한 semantic, spatial 위치 정보를 제공해줌                                ChatCaptioner based prompting                          ChatGPT한테 해당 이미지에 대한 일련의 question을 만들게끔 프롬프팅하고, BLIP-2 VLM모델에게 이 질문에 대한 응답을 생성시킴.              3D-related data를 수집하기 위해, BLIP-2모델에게 multi-view 2D rendered image들을 인풋으로 주고, ChatGPT가 전체 장면에 대한 “glboal 3D 묘사를 형성 및 수집하기 위해 질문 던짐                                Revision based prompting                          해당 3D data중 한종류를 다른 것으로 바꾸는데 사용 (?)                                                  3D-LLM PipelineOverview  2D image-text multimodal data: CLIP처럼 2D image들과 텍스트간의 contrastive learning을 통해 feature embedding을 학습하는 인코더 사용          얘네는 2D image-text pair데이터가 billion-scale이고 자원 소스를 그렇게 많이 먹지 않기때문에 scratch부터 학습시킬 수 있음        그러나 3D data는 그들의 “희소성”과 “language의 연결부재”같은 문제점들때문에 billion-scale 데이터가 존재하지 않을뿐더러, 리소스를 너무 많이먹어서 이런 3D encoder방식으로 바로 contrastive learning을 scratch부터 시키는 것은 말 안됨  따라서 그 대안으로, 2D multi view이미지들로부터 3D feature를 추출하는 방법을 사용한다          how? : pretrained 이미지 인코더로 멀티뷰에 대한 2D feeature 추출하고 이거를 2D VLM모델에게 넘겨서 3D data로 매핑시킴      2D 이미지 피쳐에 대응되는 3D feature가 2D VLM에게 인풋으로 들어가지는 것이기 때문에 우리는 이 2D VLM을 백본으로 삼아 활용시켜서 scratch부터 학습시킬 필요 없음        3D localization mechanosim 제안 : 3D spatial 정보를 더 잘 포착하게 해줌1. 3D Feature Extractor(1) Direct Reconstruction  카메라 파라미터 GT값이랑 RGB-D의 2D 이미지들로부터 point cloud를 복원할 수 있음  정확한 카메라 intrinsic parameter주어졌을 때 적합한 방법이라 채택함(2) Feature Fusion  2D feature로부터 3D feature로 매핑하는 과정 ⇒ gradSLAM 모델 사용  기존 dense mapping 방법들과 다르게, 추가적인 “depth”와 “color”가 함께 fusion된다 ⇒ 3D data에 적합함(3) Neural Field  neural voxel field 이용해서 3D field의 복셀단위로 밀도와 색깔을 포함한 feature를 저장함  그 이후 MSE loss를 이용해서 ray를 따라 3D feature와 pixel단위의 2D feature가 align되도록 학습함⇒ 결과적으로 3D 장면에 대한 3D feature인 point cloud 얻음2. 2D VLMs as backbones  앞선 feature extractor와 더불어 3D-LLM을 scratch부터 학습시키는 것은 빡세기 때문에 2D VLM(e.g., BLIP-2)모델을 백본으로 활용함  3D 특징을 2D 이미지와 같은 특징 공간(feature space) 으로 매핑할 수 있기 때문에 2D VLM을 백본(backbone)으로 활용하는 것은 합리적이라고 주장하고있음⇒ CLIP 같은 2D 이미지 인코더는 이미 잘 학습되어 있으니, 얘네는 그대로 쓰고 파라미터는 고정(freeze)시킨다.  대신, 3D feature extractor를 만들어서 3D 데이터를 2D 이미지 특징 공간으로 매핑한다.  이렇게 하면 3D와 2D 데이터를 같은 backbone에 넣을 수 있음.Perceiver 구조      perceiver 구조는 [DeepMind(2021)]에서 제안한 범용(multimodal) 신경망 아키텍처=&gt; 쉽게 말하면, 입력 크기나 모달리티(이미지, 텍스트, 오디오, 3D 포인트 클라우드 등)에 상관없이 처리할 수 있는 범용 Transformer 변형    Perceiver(또는 QFormer)는 입력 크기와 모달리티에 제약이 적음.  즉, 이미지처럼 고정된 크기뿐만 아니라 포인트 클라우드(3D 점 집합)처럼 크기가 가변적인 입력도 처리 가능.  그래서 3D 특징을 그대로 넣어도 문제없이 VLM에 활용할 수 있음.최종 전략  3D feature extractor → 3D 특징 (2D와 같은 차원으로 매핑)  frozen image encoder → 2D 특징  둘 다 perceiver에 입력 → pretrained 2D VLM backbone으로 학습  결과: 2D-기반 모델을 이용해 3D-LLM을 효과적으로 학습3. 3D Localizaton mechanism: 3D LLM이 단순히 “무엇(what)”만 아는 게 아니라, “어디(where)”에 있는지도 잘 이해하게 하려는 기법(1) 3D 특징 + 위치 정보 결합  단순히 3D feature vector만 쓰면 “내용”은 알 수 있지만 “위치”는 놓칠 수 있음  그래서 (x, y, z) 좌표에 대한 sin/cos position embedding을 만들어서 feature에 추가 → 위치감각 강화(2) 위치를 토큰으로 텍스트에 연결  LLM은 보통 단어 단위 토큰만 이해하는데, 여기선 공간 위치도 토큰처럼 다룸  3D 바운딩 박스를 ⟨xmin, ymin, zmin, xmax, ymax, zmax⟩ 식으로 토큰화  LLM의 단어 사전에 이 “위치 토큰”을 추가해서, 언어와 공간 위치를 같은 입력 공간에서 학습할 수 있도록 함  즉, “the red cube in the top-left” 같은 표현을 모델이 좌표 기반으로 grounding할 수 있게 됨=&gt; 3D LLM이 위치 정보를 잘 이해하도록, (i) 3D feature에 sin/cos 기반 좌표 임베딩을 더하고, (ii) 바운딩 박스를 위치 토큰으로 사전에 추가해 LLM과 3D 공간을 직접 연결실험내용 생략"
  },
  
  {
    "title": "PERSONA, ICCV 2025",
    "url": "/posts/PERSONA/",
    "categories": "Study, Paper-review",
    "tags": "Avatar reconstruction, 3D reconstruction",
    "date": "2025-08-26 13:00:00 +0900",
    





    
    "snippet": "기존 선행 연구들 문제(1) 3D 기반:  3DGS + SMPL-X같은 뉴럴렌더링 활용하는방법  ⇒  주로 강체(rigid) 변형만을 지원하는 3D 파라메트릭 모델을 활용하기 떄문에, 변형가능한(non-rigid)한 옷의 주름을 포착하기 위해서는 다양한 Pose정보가 포함된 대규모 데이터셋으로 학습이 필요함(2) 2D 기반:  diffusion활용  ...",
    "content": "기존 선행 연구들 문제(1) 3D 기반:  3DGS + SMPL-X같은 뉴럴렌더링 활용하는방법  ⇒  주로 강체(rigid) 변형만을 지원하는 3D 파라메트릭 모델을 활용하기 떄문에, 변형가능한(non-rigid)한 옷의 주름을 포착하기 위해서는 다양한 Pose정보가 포함된 대규모 데이터셋으로 학습이 필요함(2) 2D 기반:  diffusion활용  gemotry정보 잡는거 취약하고 multi-view에 의해 복원 inconsistency함Persona’s Contributions  앞선 3D neural rendering기반 메소드랑 2D diffusion기반 메소드의 장점만 모아서  단일 이미지 한 장으로 pose-driven deformation(변형) 잘 포착하는 3D 아바타 복원하는 메소드(1) Balanced sampling  아바타 최적화하는동안 input image oversampling하는거 +  pose에 의존적인 geometry와 그림자같은 내재된 artifact 예방(2) geometry-weighted optimization  sharp rendering + texture 유지하기 위해 제안Overview  SMPL-X 모델(whole-body animation)이랑 3DGS(texture복원 및 geometry 이용)  3DGS(Mip-splatting 이용)기법에서 mean-offset 소개 ⇒ pose-driven(non-rigid) deformation의 성능을 잘되게 함                  MLP의 input으로 가우시안의 triplane feature를 넣음        : triplane feature란 = 세 개의 직교하는 2D 평면을 의미함        ⇒ 3D gaussian splat 포인트가 단순히 색, 크기같은 물리적인 속성만 특성으로 갖고있는것이 아니라, 세개의 2D 평면(XY, YZ, XZ)에서 뽑은 feature embedding으로 보강된 표현을 뜻함 ⇒ 메모리 효율성이 좋고 가우시안의 삼평면 기반 보강된 neural representation이 되면서 “텍스처”, “세밀한 표면”속성같은 정보를 포함할 수 있게 됨              SMPL-X의 shape 파라미터 가져와서 “geometric identity” 보존 ⇒ 3D pose랑 facial expression 둘다 SMPL-X에서 가져옴 ⇒ MLP에 같이 넣음= 그 이후, canonical space에서 가장먼저 아바타 constructed되고그 다음에 LBS(Linear-blend skinning)로 아바타 자연스럽게 animated된 이후, 최종 Mip-Splatting으로 3D 공간에서 rendering하는 흐름MethodPose-driven(non-rigid) deformation(1) Balanced Sampling  디퓨전 기반 메소드들은 “주어의 identity(특히 얼굴표정)”를 잃는 것이 태반임  Baked-in-Artifacts :  그림자 지는 곳, 옷 쭈글거리는 부분 ⇒ canonical 공간에서 렌더링된 아바타에서도 보이는 문제임  해결 방법 :          positional map에서 sobel filter(edge detection해주는 2D image process filter)먹여가지고 seam boundary(옷 경계선) 따서 이 전경과 배경을 분리함으로써 신체부위간의 내부 경계도 탐지할 수 있음      albedo 이미지 활용 : 음영지는곳이 많은 이미지 즉, 그림자 지는 영역 많아지면 artifact심해지니까 이를 최소한으로 포함하는 , 특히 “texture”표현할 때 그림자 artifact를 최소화하는데 도움되는 이미지인 알비도 이미지를 추가적인 supervison 정보로 사용      (2) Geometry-weighted optimization for Sharp rendering  low-loss weight on low images &amp; high loss weights on geometry  Geometry : binary mask, depth, normal, part segmentation          texture 퀄리티에서 불안정함 일반 렌더링(rigid)한 부분에서는 ㄱㅊ음                  binary mask : color value이용          depth map : 렌더링된 가우시안의 depth는 각 가우시안의 “색깔 속성”으로 취급됨          normal maps : 렌더링된 가우시안 각각의 normal vector이용해서 계산됨 + ExAvatar의 hybrid representation이용          part segmentation : RGB이미지로 표현되고 각 색깔은 Sapiens palette(모델 이름)에 기반되어 몸 부위마다 세그멘트되는거 활용                    (3) Mean offsets  sharp rendering 유지하기 위해서 제안  isotropic(등방성⇒ 모든 방향에서 분산(scale)이 동일한 것) 가우시안에다가 mean-offset만 적용          등방성 가우시안이 되기 때문에 가우시안 각각의 shape이 고정되어서 이게 변형되더라도 blurry해지지 않게됨      position-only 변형에 해당됨      반면, scale offset은 가우시안이 단순히 커지거나 작아지면서 이동하지 않기 때문에 결과가 흐릿하게 렌더링될 수 있는 문제있음 + RGB 오프셋은 생성된 프레임으로부터 신뢰할 수 없는 색상을 복사할 위험      Optimization  최적화한 파라미터들 : 3D Gaussian features(means, scales, RGB colors) + triplane, MLP weights, per-frame SMPL-X parameters  고전적인 image reconsgtruction loss들 ($L_1$,SSIM, LPIPS)과 geoemtry regularizer(라플라시안 정규화같은)도 같이 이용해서 학습시킴  (2) Geometry-weighted optimization: 최종 rendered된 output과 target geometry map(e.g., normal map, depth map, ..)사이의 $L_1$ loss도 최소화되게 학습  geoemtric plausibility를 위해, “손”영역에서는 3DGS와 SMPL-X hand model로 렌더링된 meshes들 사이의 각각 $L_1$ loss를 사용하여 최적화함Experiments  evaluation metrics: PSNR, SSIM, LPIPS  evluation(test) datas: X-Humans, NeuMan  comparison methods: NeuMan, ExAvator, LHM, AniGS, Champ, GaussianAvatar, ….Results  qualitative &amp; quantiatitve 느낀점  이미지 피킹한 페이지가 많아서 본문 설명 글이 별로 없었다  메소드자체는 엄청 어려워보이진 않는데 SMPL-X랑 mip-splatting 코드 합치고 짜는게 어려웠을 것 같다  핵심 컨트리뷰션에서 mean-offset을 위해서 mlp인풋으로 주는 triplane gaussian관련 처음 나온 논문 : Triplane Meets Gaussian Splatting: Fast and Generalizable Single-View 3D Reconstruction with Transformers » 이 논문도 같이 읽어봐야겠다  개인적으로 전체 overview pipeline을 너무 대충 그려놓으셔서 아쉬웠다 일부러 quantiative 결과들 많이 보여줘야돼서 분량상 그런것같긴한데"
  },
  
  {
    "title": "LiDAR SLAM tutorial",
    "url": "/posts/LiDAR_SLAM/",
    "categories": "Study, SLAM",
    "tags": "SLAM, 3D Perception, Autonomous Driving",
    "date": "2025-07-13 08:00:00 +0900",
    





    
    "snippet": "Paper List (TODO)      87 PAMI Least-squares fitting of two 3D point sets    tldr: SVD-based closed form of registration; a basic of basic of the scan matching        92 PAMI A method for registrat...",
    "content": "Paper List (TODO)      87 PAMI Least-squares fitting of two 3D point sets    tldr: SVD-based closed form of registration; a basic of basic of the scan matching        92 PAMI A method for registration of 3-D shapes    tldr: a.k.a ICP; a basic of the scan matching        97 AR Globally Consistent Range Scan Alignment for Environment Mapping    tldr: mostly called as “Lu and Milios”; considered as the first work of a scan matching and pose graph optimization-based SLAM.        03 IROS The Normal Distributions Transform: A New Approach to Laser Scan Matching    tldr: NDT registration        06 IJRR Square Root SAM: Simultaneous Localization and Mapping via Square Root Information Smoothing    tldr: from probabilistic nature to least square formulation of SLAM for smoothing (i.e., modification of past poses)        07 JFR Scan registration for autonomous mining vehicles using 3D-NDT    tldr: 3D version of NDT registration        08 TRO iSAM: Incremental Smoothing and Mapping    tldr: incremental SAM and an open source library        09 ICRA Real-Time Correlative Scan Matching    tldr: prof. Olson; later affects  to Cartographer, etc.        09 ICRA Fast Point Feature Histograms (FPFH) for 3D Registration    tldr: FPFH (the most famous 3D local descriptor) registration        09 RSS Generalized-ICP    tldr: uncertainty-embedded ICP (probabilistic perspective)        10 ITSM A Tutorial on Graph-Based SLAM    tldr: Grisetti’s must-read tutorial        11 IV Velodyne SLAM    tldr: an early work of modern 3D scanning LiDAR-based motion estimation        12 TRO Zebedee: Design of a Spring-Mounted 3-D Range Sensor with Application to Mobile Mapping    tldr: mobile mapping system and IMU fusion        12 RAM Tutorial: Point Cloud Library: Three-Dimensional Object Recognition and 6 DOF Pose Estimation    tldr: PCL tutorial, but not much delved into the SLAM perspective.        12 IJRR iSAM2: Incremental smoothing and mapping using the Bayes tree    tldr: in GTSAM 4.0, iSAM2 (not iSAM1) is currently a de-facto default factor graph optimizer.        13 ICRA Robust Odometry Estimation for RGB-D Cameras    tldr: a.k.a DVO; this is not an actually LiDAR thing, but to understand the effectiveness of direct alignment rather ICP        13 IROS Dense Visual SLAM for RGB-D Cameras    tldr: a SLAM version (i.e., including loop closures) of the DVO; studying RGB-D SLAMs is also worthy for LiDAR guys because they frequently considers the both a photometric error and a geometric error.        13 AR Challenging data sets for point cloud registration algorithms    tldr: a.k.a the open library: Libpointmatcher        14 RSS LOAM: Lidar Odometry and Mapping in Real-time    tldr: THE LOAM; (surface and corner) feature matching for frame-to-frame registration and frame-to-map refinement        15 ICRA Visual-lidar Odometry and Mapping: Low-drift, Robust, and Fast    tldr: Visual + LOAM        15 ICRA Initialization Techniques for 3D SLAM: a Survey on Rotation Estimation and its Use in Pose Graph Optimization    tldr: LiDAR SLAM usually have more opportunity to think of the better pose graph optimization because it directly measures the depth (rather easier front-end than visual domain).        15 RAM Registration with the Point Cloud Library: A Modular Framework for Aligning in 3-D    tldr: PCL tutorial for registration        15 IROS NICP: Dense normal based point cloud registration    tldr: as already in the title, dense normal        16 ICRA Real-time loop closure in 2D LIDAR SLAM    tldr: the Google Cartographer’s paper        16 SSRR ICP-based pose-graph SLAM    tldr: an almost standard framework of scan matching- and pose-graph-based LiDAR SLAM        16 IROS M2DP: A novel 3D point cloud descriptor and its application in loop closure detection    tldr: place descriptor using a single LiDAR scan        16 BookChapter World modeling    tldr: various representations of a surrounding environment; selecting a proper representation of the environment is important and it determines the state estimation ways.        18 RSS Efficient Surfel-Based SLAM using 3D Laser Range Data in Urban Environments    tldr: a.k.a SuMa; projective view rendering, and open source        18 ICRA IMLS-SLAM: scan-to-model matching based on 3D data    tldr: sophisticated feature selections, not real-time but for the accuracy, this is considered as a SOTA        18 ICRA Elastic LiDAR Fusion: Dense Map-Centric Continuous-Time SLAM    tldr: (continuous-time) non-rigid map deformation (see 15 RSS ElasticFusion also)        18 ICRA Efficient Continuous-time SLAM for 3D Lidar-based Online Mapping    tldr: a hierarchical continuous-time LiDAR SLAM        18 IROS LeGO-LOAM: Lightweight and Ground-Optimized Lidar Odometry and Mapping on Variable Terrain    tldr: range image-based fast feature selection for LOAM; and open source        18 IROS LIPS: LiDAR-Inertial 3D Plane SLAM    tldr: leveraging plane for LINS system        18 IROS Scan Context: Egocentric Spatial Descriptor for Place Recognition Within 3D Point Cloud Map    tldr: A visibility-based place descriptor for fast and robust place recognition, and open source        19 A-LOAM (code only)    tldr: a well-implemented LOAM algorithm (the original LOAM author closed the official code), and open source        19 ICRA Tightly Coupled 3D Lidar Inertial Odometry and Mapping    tldr: a.k.a lio-mapping; imu tight fusion but practically slow, and open source        19 IV DeLiO: Decoupled LiDAR Odometry    tldr: rotation and translation are decoupled        19 IJRR SegMap: Segment-based mapping and localization using data-driven descriptors    tldr: deep segment feature learning for LiDAR place recognition        19 IROS SuMa++: Efficient LiDAR-based Semantic SLAM    tldr: merging semantic information into SuMa        20 AR DVL-SLAM: sparse depth enhanced direct visual-LiDAR SLAM    tldr: enhanced visual SLAM by LiDAR data        20 RSS OverlapNet: Loop Closing for LiDAR-based SLAM    tldr: learning two scan’s overlap and integrated it into the modern probabilistic SLAM system.        20 IROS LIO-SAM: Tightly-coupled Lidar Inertial Odometry via Smoothing and Mapping    tldr: IMU fusion (tightly) of LOAM, and open source.        20 IROS SpoxelNet: Spherical Voxel-based Deep Place Recognition for 3D Point Clouds of Crowded Indoor Spaces    tldr: Deep LiDAR feature learning for place recognition and robust to occlusions        20 IROS Semantic Graph Based Place Recognition for 3D Point Clouds    tldr: Summarizing a place with a single semantic graph. The matching part is also deep (SegMap didn’t).        20 IROS A Fast and Robust Place Recognition Approach for Stereo Visual Odometry Using LiDAR Descriptors    tldr: LiDAR descriptors are also good for stereo-camera-based place recognition  "
  },
  
  {
    "title": "Graphics-Ch10. Surface shading",
    "url": "/posts/ch10-Surface_Shading/",
    "categories": "Graphics, Study",
    "tags": "Graphics",
    "date": "2025-02-19 20:04:00 +0900",
    





    
    "snippet": "1. Diffuse Shading  Lambertian objects          paper, wood, dry, unpolished stone같은 모든 면에서 빛이 난반사(diffuse)되는 불투명한 표면의 물체        perspective transformation이후의 warped coordinate에서 생각하는 것이 아닌, world ...",
    "content": "1. Diffuse Shading  Lambertian objects          paper, wood, dry, unpolished stone같은 모든 면에서 빛이 난반사(diffuse)되는 불투명한 표면의 물체        perspective transformation이후의 warped coordinate에서 생각하는 것이 아닌, world coordinate에서의 shading 적용을 고려함1.1. Lambertian Shading Model      “Lambert’s cosine law” : \\[\\begin{align} c &amp;\\propto cos \\theta . \\\\                c &amp;\\propto \\mathrm{n} \\cdot \\mathrm{l}, \\end{align}\\]          $c$ : surface color      $\\mathrm{n}$ : surface normal      vector $\\mathrm{l}$ : light direction (물체가 놓여진 표면의 위치에 영향을 받지 않는다는 중요한 성질 있음)        광원의 세기(intensity)와 표면의 반사도(reflectance)에 따라 표면은 더 밝아지거나 어두워질 수 있음          $c_r$ : diffuse reflectance (diffuse object 표면에서 흡수대비 반사되는 빛의 비율)    \\[\\begin{align} c \\propto c_r \\mathrm{n} \\cdot \\mathrm{l}. \\end{align}\\]          $c_l$ : RGB intensity term (in the range [0,1])    \\[\\begin{align} c = c_r  c_l \\mathrm{n} \\cdot \\mathrm{l}. \\end{align}\\]        이 때, $n \\cdot l$은 만약 surface normal이 light direction과 정반대 즉, 두 벡터 사이 각도 $\\theta$가 180도 이상인 경우는 dot product값이 음수가 나오는 상황이므로, 내적연산을 “max” function으로 대체할 수 있음 :\\[\\begin{align} c = c_r  c_l max(0, \\mathrm{n} \\cdot \\mathrm{l}). \\end{align}\\]          1.2. Ambient Shading  실제 환경에서 모든 방향의 광원을 고려하다보면 diffuse shading만으로 완벽하게 실제 조명을 구현하기에 충분하지 않다는 문제      상수항(ambient term)을 더해줌\\[\\begin{align} c = c_r (c_a + c_l max(0, \\mathrm{n} \\cdot \\mathrm{l})). \\end{align}\\]  2. Phong Shading  모든 표면이 lambertian suface일 수는 없다, highlight되는 부분이 포함된 matte surface인 경우의 light shading model2.1. Phong Lighting model  $\\mathrm{r}$ : natural direction of reflection(입사되는 빛과 같은 각도 $\\theta로 반사되는 방향의 벡터)      unit vector $\\mathrm{e}$ (viewing direction)를 추가시킴\\[\\begin{align} c &amp;= c_l (\\mathrm{e} \\cdot \\mathrm{r}) . \\\\c &amp;= c_l max(0, \\mathrm{e} \\cdot \\mathrm{r}) ^p \\end{align}\\]          $p$ : Phong exponent                  양의 실수임          이 값에 따라 highlight되는 영역의 크기와 세기가 아래 그림의 figure 10.6처럼 변화됨                                  아래 그림처럼 벡터 $\\mathrm{r}$은 입사 방향 $\\mathrm{l}$과 n을 기준으로 각도 $\\theta$만큼 대칭되게 존재한다는 성질로 아래 수식으로 r 계산 가능 :\\[\\begin{align} \\mathrm{r} = -\\mathrm{l} + 2(\\mathrm{l} \\cdot \\mathrm{n})\\mathrm{n}, \\end{align}\\]    $\\mathrm{r}$대신 휴리스틱하게 $\\mathrm{l}$과 $\\mathrm{e}$의 가운데 존재하는 halfway vector $\\mathrm{h}$를 이용하여 highlight 영역 light modeling 가능함          n과 h의 내적은 항상 양수라는 장점 있음      단점은 h 계산을 위해서는 루트 연산과 나눗셈이 필요함        $\\mathrm{h}$가 $\\mathrm{n}$ 과 가까울수록 highlight가 커진다고 해석 가능      이때, h와 n 사이의 각도는 $\\theta$보다 작아질 수밖에 없어서 디테일이 살짝 달라질수는 있다\\[\\begin{align} &amp;c = c_l (\\mathrm{h} \\cdot \\mathrm{n})^p , \\\\  &amp;where, \\quad \\mathrm{h} = \\frac{\\mathrm{e} + \\mathrm{l}}{|| \\mathrm{e} + \\mathrm{l} ||} \\end{align}\\]            최종 Phong illumination model :          ambient light term, diffse(lambertian) term, highlight term이 순서대로 더해진 전체 shading model 수식    \\[\\begin{align} c = c_r(c_a + c_l max(0, \\mathrm{n} \\cdot \\mathrm{l})) + c_l c_p (\\mathrm{h} \\cdot \\mathrm{n})^p \\end{align}\\]          $c_p$ : RGB color, which allows us to change highlight colors      "
  },
  
  {
    "title": "Graphics-Ch9. Signal Processing",
    "url": "/posts/ch9-Signal_Processing/",
    "categories": "Graphics, Study",
    "tags": "Graphics",
    "date": "2025-02-18 21:04:00 +0900",
    





    
    "snippet": "Intro  신호 처리 단원  Sampling(연속적인 아날로그 신호에서 이산적인 디지털 신호로 샘플링하는 과정)과 Reconstruction(이산적인 신호 -&gt; 연속적인 아날로그 신호)의 일련의 과정에서 수행되는 filtering, anti-aliasing, Fourier transform,등의 내용을 다룸  학부 패턴인식 수업 중간범위에서 배...",
    "content": "Intro  신호 처리 단원  Sampling(연속적인 아날로그 신호에서 이산적인 디지털 신호로 샘플링하는 과정)과 Reconstruction(이산적인 신호 -&gt; 연속적인 아날로그 신호)의 일련의 과정에서 수행되는 filtering, anti-aliasing, Fourier transform,등의 내용을 다룸  학부 패턴인식 수업 중간범위에서 배웠던 내용이랑 많이 겹침, 나중에 참고1. Digital Audio : Sampling in 1D  Continuous Analog signal $\\rightarrow$ Lowpass filter $\\rightarrow$ ADC(A/D converter : Sampling) $\\rightarrow$ Discrete digital signal  Discrete digital signal $\\rightarrow$ DAC(D/A converter : Reconstruction) $\\rightarrow$ Lowpass filter $\\rightarrow$ Continuous analog signal          Undersampling : 너무 적은 sample rate으로 샘플링하면(듬성듬성) ambiguous result를 초래하기 때문에, artifact가 생길 수 있음      1.1. Sampling Artifacts and Aliasing            undersampling으로 생긴 artifact를 완화하는 방법은?       sampling이전에 filtering을 해주고, reconstruction이후에 filtering을 수행하기        Aliasing : sampling 이후 결과만 보면 원본 아날로그 신호가 빠른 sine wave였는지, 느린 sine wave였는지 원본을 알 수 없다          이런 두 경우에, reconstruction을 적절하게 수행할 간단한 방법을 찾기가 쉽지않음      이때 high-frequency signal이 low-frequency signal “처럼 되려고 하는” 현상을 aliasing이라고 함        sampling과 reconstruction과정에서 다루어지는 challenges :          Sample rate을 얼마나 키워야 적절할까      어떤 종류의 Filter를 사용해야하는가      smoothing의 정도($\\alpha$)를 어느정도로 해야하는가      2. Convolution  합성곱 연산 내용  $f(x), g(x)$ : continuous function  $a[i]$ : discrete function  $f \\star g$ : $f$와 $g$의 convolution 연산, $f$를 $g$로 convolved했다고도 표현함2.1. Moving Averages (이동평균)  각 포인트에 대해 일정 범위(radius $=r$)안에서 평균치를 계산하는 것은 얼마나 smoothing시킬것인지에 대한 연산으로 생각할 수 있음      continuous function $g(x)$에 대한 이동평균 값 :\\[\\begin{align} h(x) = \\frac{1}{2r} \\int_{x-r}^{x+r} g(t) \\,dt \\end{align}\\]        discrete function $a[i]$에 대한 이동평균 값 :\\[\\begin{align} c[i] = \\frac{1}{2r+1} \\sum_{j=i-r}^{j=i+r} a[j] \\end{align}\\]  2.2. Discrete Convolution      sequence $a$를 filter $b$로 convolution :\\[\\begin{align} (a \\star b)[i] &amp;= \\sum_{j}a[j]b[i-j]  \\\\                                &amp;= \\sum_{j=i-r}^{i+r} a[j]b[i-j] \\end{align}\\]          $b[i-j]$ : position $j$에서의 샘플에 대한 가중치 역할        convolution연산에는 교환(commutative)법칙, 결합(associative)법칙, 분배(distributive)법칙 모두 성립  $d$ : discrete impulse $\\rightarrow$ 한 point에서만 1 신호를 갖고 나머지는 모두 02.3. Convolution as a Sum of Shifted Filters\\[\\begin{align} b_{\\rightarrow j}[i] = b[i-j] \\end{align}\\]\\[\\begin{align} (a \\star b)[i] &amp;= \\sum_{j}a[j]b_{\\rightarrow j}[i] \\end{align}\\]2.4. Convolusion with Continuous Functions  두 연속함수에 대한 convolution 연산을 아래와 같이 표현할 수 있음 :\\(\\begin{align} (f \\star g)(x) = \\int_{-\\infty}^{+\\infty} f(t) g_{\\rightarrow t} \\,dt. \\end{align}\\)Dirac Delta Function $\\delta(x)$  delta function $\\delta(x)$ : x = 0일때만 impulse 1값을 갖는 continuous identity function\\[\\begin{align} \\int_{-\\infty}^{\\infty} \\delta(x)f(x) \\,dx &amp;= f(0), \\\\                (\\delta \\star f)(x) &amp;=  \\int_{-\\infty}^{\\infty} \\delta(t)f(x-t) \\,dt = f(x). \\end{align}\\]  이므로, discrete impulse $d \\star a = a$이런 성질처럼 delta function에서도 $\\delta \\star f = f$ 수식이 성립함2.5. 2D discrete convolution  1차원보다 많아지는 컨볼루션 연산식은 어떻게 표현하는가, 똑같다. 차원만 늘리면 된다\\[\\begin{align} (a \\star b)[i,j] &amp;= \\sum_{i' = i-r}^{i+r} \\sum_{j' = j-r}^{j+r} a[i',j']b[i-i', j-j'] \\\\ (f \\star g)(x,y) &amp;= \\int \\int f(x', y')g(x - x', y - y') \\,dx' \\,dy'  \\end{align}\\]3. Convolution Filters3.1. Box filter\\[\\begin{align} a_{\\text{box},r}[i] &amp;= \\begin{cases} \\frac{1}{2r+1}, &amp; \\text{if } |i| \\leq r \\\\ 0, &amp; \\text{otherwise} \\end{cases} \\\\  f_{\\text{box},r}(x) &amp;= \\begin{cases} \\frac{1}{2r}, &amp; \\text{if } -r \\leq x &lt; r \\\\ 0, &amp; \\text{otherwise} \\end{cases} \\end{align}\\]3.2. Tent filter\\[\\begin{align} f_{\\text{tent}}(x) = \\begin{cases} 1 - |x|, &amp; |x| &lt; 1, \\\\  0, &amp; \\text{otherwise}.\\end{cases} \\end{align}\\]3.3. Gaussian filter\\[\\begin{align} f_{g, \\sigma}(x) = \\frac{1}{2\\pi\\sigma^2} \\exp\\left(-\\frac{x^2 + y^2}{2\\sigma^2}\\right) \\end{align}\\]  표준편차 $\\sigma$는 필터에서 얼마나 스무딩할것인지 강도를 조정할 수 있음  참고로 가우시안 분포에서 일정 band만큼 trimming(잘라내기)할수도 있는데, 이것을 trimmed Gaussian이라고 하며, 이때 trimmed Gaussian에서 스케일 $s$배만큼 늘리면 기존 가우시안분포로 복원시킬 수 있다는 성질 존재함3.4. B-Spline Cubic filter  부드러운 보간을 제공하며, 부드러운 블러 효과를 줄 때 사용됨\\[\\begin{align}    B(x) =    \\begin{cases}        \\frac{1}{6} (3 |x|^3 - 6 |x|^2 + 4), &amp; 0 \\leq |x| &lt; 1, \\\\        \\frac{1}{6} (2 - |x|)^3, &amp; 1 \\leq |x| &lt; 2, \\\\        0, &amp; \\text{otherwise}.    \\end{cases}\\end{align}\\]3.5. Catmull-Rom Cubic filter  날카로운 디테일을 유지하면서 부드러운 전환을 제공하는 특징\\[\\begin{align} C(x) = \\begin{cases}  \\frac{1}{2} (3 |x|^3 - 5 |x|^2 + 2), &amp; 0 \\leq |x| &lt; 1, \\\\        \\frac{1}{2} (-(|x| - 2)^3), &amp; 1 \\leq |x| &lt; 2, \\\\        0, &amp; \\text{otherwise}. \\end{cases} \\end{align}\\]3.6. Mitchell-Netravali Cubic filter  두 개의 파라미터 B와 C를 조절하여 샤프니스와 스무딩을 균형 잡을 수 있음\\[\\begin{align}    M(x) &amp;= \\frac {1}{3} B(x) + \\frac{2}{3} C(x) \\\\    &amp;= \\begin{cases}        \\frac{1}{6} ((12 - 9B - 6C) |x|^3 + (-18 + 12B + 6C) |x|^2 + (6 - 2B)), &amp; 0 \\leq |x| &lt; 1, \\\\        \\frac{1}{6} ((-B - 6C) |x|^3 + (6B + 30C) |x|^2 + (-12B - 48C) |x| + (8B + 24C)), &amp; 1 \\leq |x| &lt; 2, \\\\        0, &amp; \\text{otherwise}.    \\end{cases}\\end{align}\\]            Overshooting(Ringing)  catmull-rom filter라던지, mitchell-netravali filter는 1과 2사이 그리고 -2와 -1사이에 필터 value가 0보다 작아지는 지점들이 존재함      이런 범위 부근에서 overshooting이라는 아래 그림처럼 extra oscillations(진동)이 초래되는 결과를 overshoot되었다고 함      4. Signal Processing for Images  블러링은 그냥 위 그림처럼 box filter, gaussian filter처럼 low pass filter를 기존 이미지에 convolution 연산해주면 구현됨      “Sharpening”            식 유도과정 :\\[\\begin{align}  I_{\\text{blur}} &amp;= I \\ast f_{g, \\sigma} \\\\  I_{\\text{sharp}} &amp;= I + \\alpha (I - I_{\\text{blur}}) \\\\                    &amp;= I + \\alpha (I - I \\ast f_{g, \\sigma}) \\\\                    &amp;= (1 + \\alpha) I - \\alpha (I \\ast f_{g, \\sigma}) \\\\                    &amp;= I \\ast \\left( (1+\\alpha) \\delta - \\alpha f_{g, \\sigma} \\right) \\\\                    &amp;= I \\ast f_{\\text{sharp}} \\end{align}\\]  5. Sampling Theory  푸리에 신호와 관련된 샘플링 이론 관련한 내용  나이퀘스트 샘플링 이론이라던지 anti-aliasing 부분을 주파수 도메인의 푸리에 변환, 역푸리에변환과 관련되어 깊은 수학적인 내용을 다룸5.1. Fourier Transform  푸리에 신호 내용  Fourier Series          주기 $T$를 갖는 주기 신호 $x(t) = x(t+T)$가 존재할 때, fundamental frequency $w_0 = 2\\pi / T$ 이다      이를 오일러 공식을 이용해서 cosine함수로 아래처럼 표현가능 :        \\(\\begin{align} x(t) = cos w_0 t \\quad x(t) = e^{jw_0t} \\end{align}\\)\\                  Harmonically related complex exponentials 세트란?  \\(\\begin{align} \\Theta_k(t) = e^{jkw_0t} = e^jk(2\\pi /T)t \\quad k = 0, +-1. +- 2, .. \\end{align}\\)            이 때, 위의  harmonically related complex exponential들의 선형결합을 “Fourier Series Representation“이라고 함      그럼 주기가 없는(aperiodic)한 신호들에 대해서는 푸리에 변환으로 표현할 수 있다, sum말고 integral을 사용함        \\(\\begin{align} \\hat{f(u)} &amp;= \\int_{-\\infty}^{+\\infty} f(x) e^{-2\\pi i u x} \\,dx \\\\                  f(x) &amp; = \\int_{-\\infty}^{+\\infty} \\hat{f(u)}e^{2\\pi i u x} \\,du \\\\                  \\int (f(x))^2 \\,dx &amp;= \\int (\\hat{f(u)})^2 \\,du \\end{align}\\)          차례대로 Fourier transform을 이용해서 푸리에 계수(Fourier Coefficient)구하는 공식, Inverse Fourier Transform으로 원본 aperiodic signal을 표현하는 공식          …5.4. Sampling and Aliasing  앨리아싱 현상을 완화하려면, 적절한 sampling filter와 reconstruction filter를 설정하는 것이 중요하다  이때 low pass filter로 filtering을 하는 목적은, 일정 주파수 도메인 범위를 잘라내서 제한하는 것임 $\\rightarrow$ alias spectra 가 원본 신호를 방해하지 않게 도와줌  “Nyquist Criterion“(나이퀘스트 샘플링 이론):          연속 신호를 디지털 신호로 변환(샘플링)할 때, 신호의 최대 주파수(f_{max}​)의 최소 두 배 이상으로 샘플링해야 정보 손실 없이 복원 가능      샘플링 실행하는 min frequency =  나이퀘스트 속도(Nyquist Rate) = 2f_{max​}      "
  },
  
  {
    "title": "Graphics-Ch8. Graphics Pipeline",
    "url": "/posts/ch8-Graphics_pipeline/",
    "categories": "Graphics, Study",
    "tags": "Graphics",
    "date": "2025-02-15 12:04:00 +0900",
    





    
    "snippet": "Intro  Graphics Pipeline : object에서 시작해서 pixel이 업데이트되기까지의 일련의 operations sequence  object-order rendering = rendering by “rasterization”  그래픽스 파이프라인 과정들 중에서 “vertex processing”, “Rasterization”, “F...",
    "content": "Intro  Graphics Pipeline : object에서 시작해서 pixel이 업데이트되기까지의 일련의 operations sequence  object-order rendering = rendering by “rasterization”  그래픽스 파이프라인 과정들 중에서 “vertex processing”, “Rasterization”, “Fragment Processing”, “Blending” 이렇게 핵심 네 스텝에 대해서 이번 단원에 다루도록 하겠다          Vertex processing : vertices(꼭짓점)들이 연산되는 과정      Rasterziation stage : 그 꼭짓점들을 이용한 primitives가 보내져서 연산되고, 각각의 Primitive를 fragment로 쪼갬      Fragment processing : 쪼개진 fragment들이 연산되는 과정      Blending : 다양한 Fragment들이 대응되는 픽셀에 따라서 혼합됨      1. Rasterization  그래픽스 파이프라인 중 가장 핵심이 되는 과정  rasterizer는 다음의 두 가지 일을 수행함          픽셀을 순회함, 각 픽셀은 primitive로 덮여있음      그 primitive들을 보간(Interpolate)함      1.1. Line Drawing  오든 그래픽스 패키지들은 line drawing관련된 명령을 포함하고 있음  시작점 $(x_0, y_0)$과 끝점 $(x_1, y_1)$을 연결하는 직선 그리기  크게 implicit line equation을 이용하는 방법과 parametric한 line equation을 이용하는 방법으로 2가지 방법이 있음1.1.1. Implicit Line Equations  Midpoint alogrithm 이용: 직선의 방정식 $f(x,y)$가 이 midpoint보다 위에 있는지 아래에 있는지에 따라서 픽셀 선택하는 알고리즘  시작점 끝점을 잇는 직선의 방정식 $f(x,y)$일 떄, 기울기 $m = \\frac{y_1 - y_o}{x_1 - x_0}$  직선 위에 있는 점들에 대해서는 $f(x,y)=0$을 만족한다는 성질 이용  $(x, y)$에서 다음으로 이동할 픽셀의 후보군(아래 그림에서 주황색 픽셀들)을 다음의 두 가지로 설정함 : $(x+1, y)$ 또는 $(x+1, y+1)$          평행하게 오른쪽 한 칸으로 움직이거나, 대각선 방향으로 이동하는 경우의 수        Midpoint = $(x+1, y+0.5)$  $f$가 midpoint보다 above한지, below한지 분류하는 방법       : $d = f(x+1, y+0.5)$라고 할 때,        1. if $d &lt; 0$이라면          대각선 방향으로 즉, $(x+1, y+1)$ 으로 픽셀 결정       2. else $d &gt; 0$이라면,           평행한 방향으로 즉, $(x+1)$ 으로 픽셀 결정1.2. Triangle Rasterization  2D points로 2D 삼각형 그리는 방법          Given points $p_0 = (x_0, y_0)$ , $p_1 = (x_1, y_1)$, $p_2 = (x_2, y_2)$ in screen space      각 꼭짓점이 color $c_0, c_1, c_2$를 저장하고 있다는 가정하에, 삼각형 위의 한 점을 $(\\alpha, \\beta, \\gamma)$계수르 이용하여 표현 가능 ( barycentric 좌표계를 이용):     \\[\\begin{align} c = \\alpha c_0 + \\beta c_1 + \\gamma c_2. \\end{align}\\]          위 방법의 interpolation을 “Gouraud interpolation“(고러드 보간, 고러드 쉐딩이랑 차이점 공부하기)이라고도 부름        line drawing이랑 삼각형 rasterization의 미묘한 차이점중에 하나는 vertices and edges에 있다.          “Hole problem”: 인접한 삼각형을 그릴 떄, hole이 생기면 안됨      “Order problem” : 만약 인접한 삼각형들이 다른 색상을 갖고 있다면 그려지는 순서에 따라서 이미지 색상이 달라짐      이 두가지 문제를 완화하기 위해, 삼각형의 center가 삼각형 내부에 존재할때만 픽셀을 그리는 성질을 이용함 (당연한거 아님?)  같은 말로, barycentric coordinate상에서 삼각형의 중심점(center)이 (0,1)간격 사이에 존재해야 한다는 의미      만약, center가 삼각형의 edge에 정확하게 존재하면 어떻게 하는가?? 이 질문에 대한 내용은 후에 다룬다    Barycentric Coordinate을 통해 삼각형같은 다면체의 “꼭짓점”이 주어지는 상황에서 어떤 위치의 픽셀에 그릴지와 그 픽셀의 색상또한 결정하는 key가 된다.  모든 픽셀에 대해서 고러드 보간을 이용하면 계산 복잡성이 크기 때문에, 아래의 간단한 방법으로 효율성 증가시킬 수 있음          삼각형의 세 꼭짓점을 기준으로 한 bounding rectangle을 찾아서 이 직사각형 내부에 존재하는 후보 픽셀들에 대해서만 계산 Looping을 진행한다      알고리즘x_min = floor(x_i)x_max = ceiling(x_i)y_min = floor(y_i)y_max = ceiling(y_i)for y in range(y_min, y_max + 1):    for x in range(x_min, x_max + 1):        α = f_12(x, y) / f_12(x_0, y_0)        β = f_20(x, y) / f_20(x_1, y_1)        γ = f_01(x, y) / f_01(x_2, y_2)        if α &gt; 0 and β &gt; 0 and γ &gt; 0:s            c = α * c_0 + β * c_1 + γ * c_2            drawpixel(x, y, color=c)      여기서 $f_{ij}$는 꼭짓점을 잇는 직선의 방정식(line drawing 섹션)이고 아래처럼 표현함\\(\\begin{align} f_{01}(x,y) &amp;= (y_0 - y_1)x + (x_1 - x_0)y + x_0y_1 - x_1y_0 , \\\\                f_{12}(x,y) &amp;= (y_1 - y_2)x + (x_2 - x_1)y + x_1y_2 - x_2y_1 , \\\\                f_{20}(x,y) &amp;= (y_2 - y_0)x + (x_0 - x_2)y + x_2y_0 - x_0y_2 . \\end{align}\\)      Dealing with Pixels on Triangle Edges  삼각형의 중심(center)이 Edge에 정확하게 존재하는 경우의 픽셀은 어떻게 결정할 것인지에 대한 내용  삼각형 중심이 삼각형 변에 위치하는 대표적인 예시가 직각-삼각형임 \"off-screen point\"는 두 삼각형의 공유된 변의 정확히 한쪽에 위치하며,  우리가 그릴 edge는 바로 그 변임,  공유 변 위에 있지 않은 정점들은 해당 변 기준으로 서로 반대쪽에 위치한다.   따라서, 그림 상 a 또는 b 정점 중 하나는 off-screen point와 shared edge기준 같은 부호면을 가진다는 성질이용  $pq &gt; 0$알고리즘y_max = ceiling(y_i)f_α = f_12(x_0, y_0)f_β = f_20(x_1, y_1)f_γ = f_01(x_2, y_2)for y in range(y_min, y_max + 1):    for x in range(x_min, x_max + 1):        α = f_12(x, y) / f_α        β = f_20(x, y) / f_β        γ = f_01(x, y) / f_γ        if α &gt;= 0 and β &gt;= 0 and γ &gt;= 0:            if (α &gt; 0 or f_α * f_12(-1, -1) &gt; 0) and \\  # 부호               (β &gt; 0 or f_β * f_20(-1, -1) &gt; 0) and \\  # 부호               (γ &gt; 0 or f_γ * f_01(-1, -1) &gt; 0):       # 부호                c = α * c_0 + β * c_1 + γ * c_2                drawpixel(x, y, color=c)1.3. CLipping  눈의 뒤(behind)에 존재하는 공간 혹은 view volume의 바깥에 존재하는 primitive를 clipping해주는 과정들이 있어야 정확한 Rasterizing을 수행할 수 있다.  삼각형에서 두 정점은 view volume 내부에 존재하지만, 하나의 정점이 behind the eye에 있다는 상황을 생각해보자.          perspective transformation을 하게 되면, depth $z$가 $z’$로 변환되고 부호도 반대로 바뀜      이떄 모든 정점이 view volume에 있지 않는다면 제대로 transformation이 이루어지지 않아서 삼각형 모양도 변형되는 incorrect result가 초래됨        Clipping : removes part of primitives that could extend behind the eye (시야에 보이지 않는 primitives들을 제거하는 역할)  transform되기 이전 단계의 6개의 평면을 이용하여 표현된 world coordinate들에서 수행되는 clipping module  homogeneous coordinate을 이용하여 4D transform된 space에서 수행되는 clipping module          위의 두 가지 옵션을 통해 clipping 작업이 수행된다.      1.4. Clipping against a Plane      평면의 방정식 : $f(\\mathrm{p}) = \\mathrm{n} \\cdot (\\mathrm{p}-\\mathrm{q}) = 0$ 이 implicit equation을 다시 쓰면 아래처럼 재표현 가능:\\[\\begin{align} f(\\mathrm{p}) = \\mathrm{n} \\cdot \\mathrm{p} + D = 0, \\end{align}\\]    points $a$와 $b$를 잇는 line segment가 있는 상황에서, 이 선분은 평면에 의해 clipping되어짐  평면 $f(\\mathrm{p}) &gt; 0$ 이면 평면의 내부(inside)에 존재하는 것이고, $f(\\mathrm{p}) &lt; 0$이면 평면의 외부(outside)에 존재하는 것임          clipping된 선분의 양끝 지점에 대한 부호가 다르다는 성질        선분과 평면이 만나는 intersection point $\\mathrm{p}$ ($\\mathrm{p} = a + t(b-a)$)에서의 $f(\\mathrm{p}) = 0$이라는 성질 이용해서 아래 그림처럼 교차점에서의 $t$값을 도출할 수 있다.2. Operations Before and After Rasterization  Graphics pipeline을 recall          Rasterization 이전에 “Vertex processing” 단계를 통해 geometry들이 적절하게 transformed되어 primitives로 도출되어야 함                  이 때, viewing transformations을 이용해서 world coordinate에서 screen space로 변환되는 정보들 + colors, surface normals, texture coordinate 같은 정보들도 변환되어야함          전자는 chapter 7에서 다루었기 때문에 후자의 내용을 이번 챕터에서 어떻게 변환시키는지 다루도록 한다.                    Rasterization 이후에는 “Fragment processing” 단계를 통해 rasterization으로 나온 interpolated color와 depth를 그대로 통과시키던가,아니면 복잡한 shading operation으로 각각의 fragment에 대한 color와 depth를 계산한다.      그 후, “Blending” 단계를 통해 각 픽셀마다 겹쳐있는 primitive에서 final color를 혼합시켜 결정한다.                  가장 쉽고 흔한 방법으로는 depth가 가장 작은, 즉 eye와 가장 가까운 fragment의 color로 선택하는 것                    2.1. Simple 2D Drawing  가장 간단한 파이프라인은, vertex와 fragments stage, 그리고 blending stage 에서 아무것도 수행되지 않고 오로지 rasterization에서 pixel coordinate에 직접적으로 연산들이 모두 수행되는 것2.2. A Minimal 3D Pipeline  3D 공간에 물체를 그리기 위해서는 2D Drawing pipeline에서 matrix transformation이 포함되어 vertex processing이 이루어진다.          vertex-processing에서 인풋으로 들어오는 vertex position에다가 camera, projection, 그리고 viewport transformation같은 행렬을 곱함으로써 행렬 변환이 수행됨            이 결과로 삼각형이 screen space에 띄워질 수 있고, 이것은 2D상에 직접적으로 그려지는 것과 같은 결과임    Occlusion problem : back-to-front 순서로 물체들을 그리지 않으면, 더 가까이 있는 물체가 멀리 있는 물체에 가려지는 incorrect result가 초래됨          이 back-to-front 순서로 물체 그리는 걸 Painter’s algorithm이라고도 부름      hidden surface를 제거하는 가장 타당한 방법임      하지만, Occlusion cycle같은 어떤 물체가 더 앞에 있고 뒤에있는지 이 상대적인 깊이를 평가할 수 없는 경우가 존재함                  이럴때는 painter’s algorithm으로 back-to-front order를 설정할 수가 없다.          또한, depth 기준으로 primitives를 정렬하는 것은 시간소요적이라서 큰 씬에 대해서는 애초에 painter’s algorithm을 사용하기가 거의 어려움                     2.3. Using a Z-Buffer for Hidden Surfaces  앞선 occlusion problem에서 필요한 depth sorting이 시간소요적이라는 측면에서, painter’s algorithm이 거의 사용되지 않기 떄문에 나온 방법  아이디어 = 각 픽셀에서 지금까지 렌더링된 가장 가까운 표면까지의 거리(depth)를 기록하고, 그 거리보다 더 멀리 있는 fragment는 폐기한다.          이 거리를 RGB color 세 값에 추가적으로 buffer를 두어 z값을 저장하고 z차원에서 grid를 생성함        buffer algorithm은 Fragment blending phase에서 구현됨  현재 z-buffer에 저장된 depth value랑 각 fragment별로 depth를 비교하면서, 만약 해당 fragment value값이 더 작다면 color와 depth value를 덮어씌운다.2.4. Per-vertex Shading  rasterization으로 interpolated된 color와 계산된 depth로만 3D 공간에서 object를 나타내는 것으로 충분할 수도 있지만, 대부분은 objects with shading을 구현하고 싶어함  light direction, eye direction, surface normal 같은 조명과 관련한 변수들 필요로 함  vertex stage에서 shading computation을 수행하는 방법 소개          “고러드 쉐이딩”      카메라 poistion, lights, vertex에 의해서 빛의 방향과 viewer의 gaze direction이 계산되는 방법      이 shading equation을 통해 계산된 vertex color는 rasterizer에게 넘어감 (즉, rasterizer 이전 단계에서 수행되는 shading equation이다)        각각의 vertex에서 shading을 다루고 vertex끼리는 다루지 않아서 아무래도 디테일이 좀 떨어진다는 단점이 있다.2.5. Per-fragment Shading  Rasterization 이후에 나온 interpolated color를 이용해서 fragment stage에서 수행되는 shading  “Phong shading” (Phong illumination model이랑 다른 개념임)  shading equation자체는 똑같지만, 정점 각각에 행해지는 것이 아니라 rasterziation으로 나온 보간된 색상을 이용한 fragment각각에 수행된다는 점이 다름          이것을 위해서 vertex stage 좌표계가 fragment stage와 일련되게 존재해야 데이터가 적절하게 사용될 수 있음      2.6. Texture Mapping  Texture에 대한 디테일은 chapter-11에서 더 자세하게 다룰 예정  Textures : 표면의 음영(shading)에 추가적인 디테일을 더하기 위해 사용되는 이미지로, 추가되지 않으면 지나치게 균일하고 인공적으로 보이게 될 수 있음2.7. Shading Frequency  shading computation들을 어떤 스텝에 위치시킬지를 color change가 얼마나 빠른지에 따라 결정할 수 있다.  이 변하는 정도를 “scale”로 확인할 수 있음          large-scale features(e.g., diffuse shading(난반사된 빛) on curved surfaces)                  low shading frequency로 계산되어야 함          vertex stage                    small-scale features(e.g., sharp highlightsor detailed textures)                  high shading frequency로 계산됨          vertex stage  &amp; fragment stage 모두 가능                    simple anti-aliasing, culling primitives 내용 교재 참고"
  },
  
  {
    "title": "Graphics-Ch7. Viewing",
    "url": "/posts/ch7-Viewing/",
    "categories": "Graphics, Study",
    "tags": "Graphics",
    "date": "2025-02-13 12:04:00 +0900",
    





    
    "snippet": "Intro  3D to 2D mapping을 통해 object를 3d location과 2D view 사이에서 이동시키는 것을 “viewing Transformation“이라고 함  object-order rendering에서 중요함  ch4(Ray-Tracing)단원에서 orthographic(paralled) view와 perspective vie...",
    "content": "Intro  3D to 2D mapping을 통해 object를 3d location과 2D view 사이에서 이동시키는 것을 “viewing Transformation“이라고 함  object-order rendering에서 중요함  ch4(Ray-Tracing)단원에서 orthographic(paralled) view와 perspective view에서 vieweing ray가 어떻게 생성되는지에 대해 간단하게 다루었음          ray-tracer는 ray와 만나는 가장 가까운 교차점에 대한 surface를 찾는 것이고, object-order renderer는 solid-looking object에서 어떤 표면이 screen space(pixcel space)의 어떤 점들에 매칭되어 그려지는지에 대해 다루는 것      1. Viewing Transformations  Viewing Transformation을 통해 Canonical Coordinate System에서의 $(x, y, z)$로 표기되는 3D location을 이미지 픽셀 단위의 screen space 매핑하는 것이 필요함          고려되어야 할 것 : projection의 종류, FOV(Field of View, 시야각), 이미지 해상도 등        아래 세 가지의 transformation 과정으로 World Space에 있는 물체가 Screen Space로 View Transformed된다.          Camera Transformation (Eye Transformation) :                  camera의 pose(position, oriendtation)에만 의존          World Coordinate -&gt; Camera Coordinate                    Projection Tranformation :                  projection 종류에 의해서만 의존          Camera space -&gt; Canonical View Volume          -1에서 +1 범위의 points                    Viewpoint Transformation (Windowing Transformation) :                  output 이미지의 size와 position에 의존          Canonical view volume -&gt; Screen space                    1.1. The Viewport Transformation  canonical view volume에 있는 view를 가정해아 함, input처럼 생각      $(x, y, z) \\in [-1, +1]^3$\\[\\begin{bmatrix} x_{\\text{screen}} \\\\ y_{\\text{screen}} \\\\ 1 \\end{bmatrix} =  \\begin{bmatrix} \\frac{n_x}{2} &amp; 0 &amp; \\frac{n_x - 1}{2} \\\\ 0 &amp; \\frac{n_y}{2} &amp; \\frac{n_y - 1}{2} \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix}  \\begin{bmatrix} x_{\\text{canonical}} \\\\ y_{\\text{canonical}} \\\\ 1 \\end{bmatrix} .\\]    Viewport Transformation Matrix :          앞선 식의 변환행렬에다가 $z$ coordinate의 행과 열에 $ [0 \\quad 0 \\quad 0 \\quad 1]$ 를 추가시킴      \\[\\begin{align}M_{\\text{vp}} = \\begin{bmatrix}  \\frac{n_x}{2} &amp; 0 &amp; 0 &amp; \\frac{n_x - 1}{2} \\\\  0 &amp; \\frac{n_y}{2} &amp; 0 &amp; \\frac{n_y - 1}{2} \\\\  0 &amp; 0 &amp; 1 &amp; 0 \\\\  0 &amp; 0 &amp; 0 &amp; 1 \\end{bmatrix}.\\end{align}\\]1.2. Orthographic Projection Transformation  canonical view volume 좌표계로 옮기는 변환 과정  input : camera coordinate (orthographic view)  orthographic view volume을 아래의 3D space로 정의할 수 있음   = $[l,r]$ x $[b,t]$ x $[f,n]$          좌우, 상하, 앞뒤 절단면을 정의하는 클리핑 파라미터        아래 그림처럼 orthorgraphic view volume에서 $-z$방향으로 바라보고 있다고 가정하기 때문에(상황마다 다르게 설정가능하긴 하다) $f$(far)가 $n$(near)보다 작은 값임  \\[\\begin{align}M_{\\text{orth}} =  \\begin{bmatrix}  \\frac{2}{r - l} &amp; 0 &amp; 0 &amp; -\\frac{r + l}{r - l} \\\\  0 &amp; \\frac{2}{t - b} &amp; 0 &amp; -\\frac{t + b}{t - b} \\\\  0 &amp; 0 &amp; \\frac{2}{n - f} &amp; -\\frac{n + f}{n - f} \\\\  0 &amp; 0 &amp; 0 &amp; 1  \\end{bmatrix}.\\end{align}\\]      camera space(orthographic view volume) =&gt; screen space(image pixel)로의 최종 변환 수식은 아래와 같음:\\[\\begin{align}  \\begin{bmatrix}  x_{\\text{pixel}} \\\\ y_{\\text{pixel}} \\\\ z_{\\text{canonical}} \\\\ 1 \\end{bmatrix} = (M_{\\text{vp}}M_{\\text{orth}}) \\begin{bmatrix}x \\\\ y \\\\ z \\\\ 1\\end{bmatrix}.\\end{align}\\]  1.3. The Camera Transformation  World(object) space 에서 Camera Space($uvw$-space)로의 변환 과정새롭게 정의되는 변수 3가지 = {eye position $e$, gaze direction $g$, view-up vector $t$} + 기저 벡터 ${uvw}$  이 정보들로 왼쪽처럼 coordinate system 세팅하는데 충분한 정보를 제공함    arbitrary view(world space coordinate $xyz$)으로부터 origin $e$와 기저벡터 $u,v,w$로 표현된 카메라 좌표 시스템으로 저장하는 것이 목표          origin $e$로의 translation &amp; u,v,z로의 scaling matrix 처럼 생각하면 됨 (왼에서 오로 곱해지는 그거)            아래의 변환 수식을 통해 수행됨:\\[\\begin{align} M_{\\text{cam}} =\\begin{bmatrix}\\mathbf{u} &amp; \\mathbf{v} &amp; \\mathbf{w} &amp; \\mathbf{e} \\\\0 &amp; 0 &amp; 0 &amp; 1\\end{bmatrix}^{-1}=\\begin{bmatrix}x_u &amp; y_u &amp; z_u &amp; 0 \\\\x_v &amp; y_v &amp; z_v &amp; 0 \\\\x_w &amp; y_w &amp; z_w &amp; 0 \\\\0 &amp; 0 &amp; 0 &amp; 1\\end{bmatrix}\\begin{bmatrix}1 &amp; 0 &amp; 0 &amp; -x_e \\\\0 &amp; 1 &amp; 0 &amp; -y_e \\\\0 &amp; 0 &amp; 1 &amp; -z_e \\\\0 &amp; 0 &amp; 0 &amp; 1\\end{bmatrix}. \\end{align}\\]    최종적인 Viewing Transformation 알고리즘 :            Matrix Name      Description                  M_vp      Viewport Matrix              M_orth      Orthographic Matrix              M_cam      Camera Matrix              M      $( M = M_{\\text{vp}} M_{\\text{orth}} M_{\\text{cam}} )$      Algorithm  Construct ( $M_{\\text{vp}}$ )  Construct ( $M_{\\text{orth}}$ )  Construct ( $M_{\\text{cam}}$ )  Compute ( $M = M_{\\text{vp}} M_{\\text{orth}} M_{\\text{cam}}$ )  For each line segment ( (a_i, b_i) ):          Compute ( $p = M_{a_i}$ )      Compute ( $q = M_{b_i}$ )      Draw line from ( $(x_p, y_p)$ ) to ( $(x_q, y_q)$ )      2. Projective Transformations  앞선 1.2.단계의 orthographic veiw volume(camera space)에서 canonical view volume으로 변환 하는 단계에서 사용하는 transformation 종류들에 대해서 알아보자.  주요 특성(key property) :          screen에 보이는 object의 size는 $-z$축 방향으로 보여지는 camera space와의 거리(depth) $z$의 역수에 비례함            Affine Transformation(어파인 변환)으로는 분모로 z(input vector의 한 요소)를 나누는 이러한 연산은 구현할 수가 없다    Homogeneous Coordinate 메커니즘을 활용함          마지막 원소를 1로 두고 차원을 하나 확장시키는 좌표계 $(x,y,z) -&gt; (x,y,z,1)$      여기서 1을 $(x,y,z)$좌표의 분포로 생각할 수 있다.        Affine 변환을 통해 변환된 point $x’ = ax + by + cz +d$3. Perspective Projection      matrix transformation : \\(\\begin{bmatrix} y_s \\\\ 1 \\end{bmatrix}  \\sim \\begin{bmatrix} d &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\end{bmatrix}  \\begin{bmatrix} y \\\\ z \\\\ 1 \\end{bmatrix}.\\)        Perspective Matrix $P$ : \\(\\begin{align} \\mathbf{P} =\\begin{bmatrix}n &amp; 0 &amp; 0 &amp; 0 \\\\0 &amp; n &amp; 0 &amp; 0 \\\\0 &amp; 0 &amp; n + f &amp; -f n \\\\0 &amp; 0 &amp; 1 &amp; 0\\end{bmatrix}.\\end{align}\\)    perspective matrix를 orthograpic 시스템에 병합하기 위해서, $M_{per} = M_{orth} P.$를 이용하여 아래와 같은 transforamtion matrix 로 분해할 수 있다:  \\[\\begin{align} M = M_{vp} (M_{orth} P) M_{cam} \\end{align}\\]          \\[\\begin{align} M_{\\text{per}} =  M_{orth} P =  \\begin{bmatrix}  \\frac{2n}{r - l} &amp; 0 &amp; \\frac{l + r}{l - r} &amp; 0 \\\\  0 &amp; \\frac{2n}{t - b} &amp; \\frac{b + t}{b - t} &amp; 0 \\\\  0 &amp; 0 &amp; \\frac{f + n}{n - f} &amp; \\frac{2fn}{f - n} \\\\  0 &amp; 0 &amp; 1 &amp; 0  \\end{bmatrix}. \\end{align}\\]            5. Field-of-View(FOV)  한국말로 시야각이라는 의미  $\\theta$로 표현 = the angle from the bottom of the screen to the top of the screen as measured from the eye\\[\\begin{align} tan\\frac{2}{\\theta} = \\frac{t}{|n|}  \\end{align}\\]"
  },
  
  {
    "title": "Mip-NeRF 360, CVPR 2022",
    "url": "/posts/MipNerf360/",
    "categories": "Paper Review",
    "tags": "Surface reconstruction, Graphics",
    "date": "2025-02-10 12:30:00 +0900",
    





    
    "snippet": "Mip-NeRF 360: Unbounded Anti-Aliased Neural Radiance Fields  depth distortion loss의 시초          현재 GS기반 Surface Reconstruction 정규화 텀들에 많이 사용함      Abstract  “unbounded scene” : 학습된 데이터의 방향(directio...",
    "content": "Mip-NeRF 360: Unbounded Anti-Aliased Neural Radiance Fields  depth distortion loss의 시초          현재 GS기반 Surface Reconstruction 정규화 텀들에 많이 사용함      Abstract  “unbounded scene” : 학습된 데이터의 방향(direction)과 거리(distance)와 무관하게 모든 ray상에 존재하는 point에 대해 잘 합성된 scene          주로 전경 물체 뿐만아니라 배경까지 있는 scene을 의미함        NeRF(Neural Radiance Fields)          volumentric density 랑 color를 MLP를 통해 합성함      MLP : ray상의 작은 3D point들을 이용        Critical Issues          Parameterization      Efficiency      Ambiguity      Preliminaries : mip-NeRF  ray : $\\mathrm{r}(t) = \\mathrm{o} + t\\mathrm{d}$ 가 있을 때,  distance $t$에 의해서 정렬된 벡터가 정의되고 ray가 t에 대한 구간의 집합으로 쪼개진다 $T_i = [t_i, t_{i+1}]$  각 구간마다 원뿔대(conical frustum)의 평균과 공분산$(\\mu, \\Sigma)=\\mathrm{r}(T_i)$을 계산함          ray의 focal length와 image plane의 픽셀사이즈에 의해서 결정됨              featureize 단계(IPE, Integratged positional Encoding)\\[\\gamma(\\mu, \\Sigma) =\\left\\{\\begin{bmatrix}    \\sin(2\\ell \\mu) \\exp\\left(-2^ {2\\ell-1} \\operatorname{diag}(\\Sigma)\\right) \\\\    \\cos(2\\ell \\mu) \\exp\\left(-2^ {2\\ell-1} \\operatorname{diag}(\\Sigma)\\right)    \\end{bmatrix}    \\right\\}_{\\ell=0}^{L-1}\\]              위 값들이 MLP의 input으로 들어가고, MLP weight인 $\\Theta_{NeRF}$와 같이 들어가서 density $\\tau$와 color $\\mathrm{c}$를 출력함 :          \\[\\forall T_i \\in t, \\quad (\\tau_i, c_i) = \\text{MLP}(\\gamma(r(T_i)); \\Theta_{\\text{NeRF}})\\]              view direction $\\mathrm{d}$도 MLP에 같이 들어감      Volume rendering을 통해 렌더링되는 최종 pixel color $\\mathrm{C}(\\mathrm{r}, t)$ :\\[\\begin{align} \\mathrm{C}(\\mathrm{r}, t) = \\sum_limits_i w_i c_i \\\\     w_i = (1-e^{-\\tau_i(t_{i+_1} - t_i)})e^{-\\sum_{i' &lt; i}\\tau_{i'}(t_{i'+1}-t_{i'})} \\end{align}\\]    NeRF는 두 개의 서로 다른 MLP (“coarse” MLP와 “fine” MLP)를 사용하는 계층적 샘플링 절차를 사용 :          $t^c \\sim U[t_n, t_f], \\quad t^c = \\operatorname{sort}({t^c}) $      $t^f \\sim hist[t^c, w^c], \\quad t^f = \\operatorname{sort}({t^f}) $            final loss (combination of coarse and fine reconstruction loss) :\\[\\sum_{\\mathbf{r} \\in \\mathcal{R}} \\frac{1}{10}   \\mathcal{L}_{\\text{recon}} \\left( \\mathbf{C}(\\mathbf{r}, t^c), \\mathbf{C}^*(\\mathbf{r}) \\right)   + \\mathcal{L}_{\\text{recon}} \\left( \\mathbf{C}(\\mathbf{r}, t^f), \\mathbf{C}^*(\\mathbf{r}) \\right)\\]  1. Scene and Ray Parameterization1.1. 3D coordinate $x$에 대한 Parameterization  기존의 연구들은 unbounded scene에 대해 point를 파라미터화했지만, 본 논문에서는 Gaussian을 재파라미터화하였다  $f(x)$ : smooth coordinate transformation function 또는 state transition model이라고 정의  $f$ 에 대한 선형 근사 = $f(x)≈f(μ)+J_f(μ)(x−μ)$          where, $J$: $\\mu$에서의 f함수에 대한 쟈코비안 행렬    \\[\\begin{align} f(μ,Σ)=(f(μ),J_f(μ)ΣJ_f(μ)⊤) \\end{align}\\]    Extended Kalman Filter이랑 비슷하게 작동함                  Contract 함수 : point x를 입력받아 특정 반지름(1,2)를 기준으로 변형하는 연산\\[\\text{contract}(x) =\\begin{cases} x, &amp; \\text{if } \\|x\\| \\leq 1 \\\\\\left(2 - \\frac{1}{\\|x\\|} \\right) \\left(\\frac{x}{\\|x\\|} \\right), &amp; \\text{if } \\|x\\| &gt; 1\\end{cases}\\]              의미          x가 1보다 큰 경우(그림에서 주황색 범위인 경우)에는 벡터를 방향은 유지하면서 특정 크기로 축소(수축, contract)      변형된 크기는 $2−1∥x∥2−∥x∥1​$ 로 조정됨 , 벡터를 단위 벡터로 변환한 후 크기를 다시 스케일링하는 방식      =&gt; 거리가 멀리 떨어져 있을수록 disparity(시점 차) 즉, distance의 역수만큼 비율적으로 분포하게 해줌      NDC(normalzed device coordinate)와 같은 motivation        본 논문에서는 아래의 식에 따라 유클리드 공간에서 mip-NeRF의 IPE feature encoding($\\gamma$) input으로 축소된 공간에서의 contract함수 반환 값($\\gamma(\\text{contract}(x))$)을 감마 함수에 집어넣는다2. 구간을 나눌 광선 거리 $t$에 대한 Parameterization :  일반적인 NeRF에서는 uniform distribution에서 $t_n$(근거리)과 $t_f$(원거리)사이 구간에서 샘플링한 광선거리 $t^c$를 정렬하여 사용          하지만, Scene parameterization에서 NDC처럼 역할하는 축소된 공간에서의 state transition 과정으로 인해서 실제로는 깊이의 역수(disparity)간격으로 균일하게 샘플들이 배치된다.      이 상황은 카메라 방향이 하나만 존재하는 unbounded scene에는 적합하겠지만, 방향이 다른 모든 뷰에 대한 unbounded scene을 복원하는 데에는 적합하지 않다            유클리디안 공간에 있는 ray distance $t$를 “normalized” ray distance $s$로 역매핑해주는 것이 필요 :\\[s \\triangleq \\frac{g(t) - g(t_n)}{g(t_f) - g(t_n)}, \\quad t \\triangleq g^{-1} \\left( s \\cdot g(t_f) + (1 - s) \\cdot g(t_n) \\right), \\quad (11)\\]          여기서 g는 가역(정규화된 값을 출력해주는)함수      2. Coarse-to-Fine Online Distillation  Mip-NeRF :          MLP에서 “coarse” ray interval $t^c$을 사용하여 한 번 평가하고, 다시 “fine” ray interval $t^f$를 사용하여 평가되며, 두 수준 모두에서 image reconsruction loss를 사용        Mip-NeRF 360 :          두 가지의 MLP학습(NeRF MLP $\\Theta_{NeRF}$ &amp; Proposal MLP $\\Theta_{prop}$)                  NeRF MLP $\\Theta_{NeRF}$ : 기존 Mip-NeRF에서 사용하던 MLP          Proposal MLP $\\Theta_{prop}$ : volumetric density만 에측함, color는 예측하지 않음                          output의 volumetric density가 다시 weight vector $\\hat{w}$로 반환됨              proposal 가중치 $\\hat{w}$는 자체 가중치 벡터 $w$를 예측하는 NeRF MLP에 제공되는 $s$-간격을 샘플링하는 데 사용              입력 이미지를 재현하도록 학습되지 않고 대신 NeRF MLP에 의해 생성된 가중치 $w$를 제한하도록 학습              두 MLP 모두 랜덤하게 초기화되고 공동으로 학습되므로 이 supervision은 NeRF MLP 지식을 proposal MLP에 대한 일종의 “online distillation”로 생각함                                          proposal MLP $(\\hat{\\mathrm{t}}, \\hat{\\mathrm{w}})$와 NeRF MLP $(\\mathrm{t},\\mathrm{w})$의 히스토그램이 일관되도록 장려하는 loss function이 필요                  이 때 두 히스토그램 x축 bin길이가 동일할 필요가 없음, 아니 오히려 달라져야 proposal MLP가 잘 학습되었다는 근거임          따라서 bin이 다를 때의 히스토그램 유사성을 평가하는 통계적 방법론 연구들이 상대적으로 부족하기 때문에 꽤 어려운 문제임                          $\\text{bound}(\\hat{\\mathrm{t}}, \\hat{\\mathrm{w}}, T) = \\sum\\limits_{j : T \\cap \\hat{T}_j neq \\emptyset}\\hat{w_j} .$              만약 두 히스토그램이 서로 일관(일치)하면, $(\\mathrm{t},\\mathrm{w})$의 모든 간격 $(T_i, w_i)$에 대해 $w_i \\leq \\text{bound}((\\hat{\\mathrm{t}}, \\hat{\\mathrm{w}}), T_i)$가 모든 구간에 대해 성립해야 함              이 성질 이용해서 $L_{prop}$ 설계 :            \\[\\begin{align} \\mathcal{L}_{\\text{prop}} (\\mathbf{t}, \\mathbf{w}, \\hat{\\mathbf{t}}, \\hat{\\mathbf{w}}) =\\sum_i \\frac{1}{w_i} \\max \\left( 0, w_i - \\text{bound} (\\hat{\\mathbf{t}}, \\hat{\\mathbf{w}}, T_i) \\right)^2,\\end{align}\\]                                  3. Regularization for Interval-Based Models (Depth distortion loss)  “floater”가 발생하거나 “back-ground collapse”가 발생하는 결함 상황에서 잘 해결할 수 있음      정규화된 ray distance $s$와 blending weight $w$간의 step function으로 구성됨\\[\\begin{align}  \\mathcal{L}_{\\text{dist}} (\\mathbf{s}, \\mathbf{w}) =\\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} w_{\\mathbf{s}}(u) w_{\\mathbf{s}}(v) |u - v| \\, d u \\, d v, \\end{align}\\]          where, w_s(u)는 $u$에서 $(s,w)$에 의해 정의된 step function에 대한 보간이다.      $ w_s(u)=\\sum_i w_i 𝟙[s_i,s_{i+1})(u) $            위 적분 식은 정의는 직관적이지만 계산하는 것이 어렵기 때문에, 아래 식으로 rewritten할 수 있음\\[\\begin{align} \\mathcal{L}_{\\text{dist}} (\\mathbf{s}, \\mathbf{w}) =  \\sum_{i,j} w_i w_j \\left| \\frac{s_i + s_{i+1}}{2} - \\frac{s_j + s_{j+1}}{2} \\right|  + \\frac{1}{3} \\sum_i w_i^2 (s_{i+1} - s_i)  \\end{align}\\]    첫 번째 항은 모든 interval에 대한 midpoint사이의 weighted distances 최소화 term, 두 번째 항은 각각의 interval에 대한 weighted size를 최소화하는 텀이다.느낀점  이전에 나온 모델로 Mip-NeRF(ICLR 2021)가 있는데 ray tracing based volume rendering이 아니라, “Cone” tracing한다는 점이 가장 큰 특징인 것만 알고 읽은 상태라 모든 수식이 매끄럽게 이해되진 않았다  NeRF자체를 제대로 공부해본적이 없었어서 잘 이해가 안되는 게 많긴 하는데 언젠가 공부 급한건 아님"
  },
  
  {
    "title": "Gaussian Opacity Fields, SIGGRAPH Asia 2024",
    "url": "/posts/GOF/",
    "categories": "Paper Review",
    "tags": "Surface reconstruction, Graphics",
    "date": "2025-02-08 11:30:00 +0900",
    





    
    "snippet": "Gaussian Opacity Fields: Efficient Adaptive Surface Reconstruction in Unbounded ScenesAbstract  본 논문의 GOF는 ray-tracing 기반 3D Gaussian의 volume rendering을 통해 직접적으로 geometry를 추출하여 level-set을 확인하는 과정을 ...",
    "content": "Gaussian Opacity Fields: Efficient Adaptive Surface Reconstruction in Unbounded ScenesAbstract  본 논문의 GOF는 ray-tracing 기반 3D Gaussian의 volume rendering을 통해 직접적으로 geometry를 추출하여 level-set을 확인하는 과정을 수행          SuGaR처럼 포아송 재건(Poisson reconstruction)을 사용하지 않으며, 2DGS와 GS2Mesh처럼 TSDF fusion을 사용하지 않음      surface normal을 ray-Gaussian intersection plane(ray와 교차하는 가우시안의 평면)을 이용하여 근사함      geometry extraction method로는 “Marching Tetrahedra“(마칭 큐브아니고 마칭-사면체)방법 사용함      Introduction  NeRF 기반          SDF(Signed Distance Function)와 occupancy 네트워크를 사용하여 surface reconstruction을 수행      foreground object 복원에만 제한적임      e.g., Neuralangelo      NeRF’s opacity Field로부터 real-time rendering 및 surface 추출하는 연구도 진행됨 (e.g., Binary Opacity Grids-BOG)        Marching Cube Algorithm          mesh extraction Algorithm      surface recon자체보다 NVS(novel view synthesis)에 초점이 맞춰져서 고안된 방법이라, 정규화 텀이 부족하고 다소 noisy함        3DGS 기반          SuGaR : surface-align Gaussian들을 얻기 위해 정규화 텀 추가 &amp; depth map으로부터 Poisson reconstruction기반 메쉬추출      2DGS : image plane에 projected된 2D Gaussian의 property이용 + TSDF fusion이용      - 이 방법들은 surface reconstruction 성능 향상이 되었지만, fine-grained geometry를 잡는 것과 background region을 복원하는 것에는 아직 부족하다.        Poisson-Reconstruction 과 TSDF Fusion의 결함          포아송 재건은 Gaussian primitive에 대한 opacity(투명도), scale, rendered depth같은 정보들을 고려하지 않음      TSDF fusion은 얇은 구조물이나 Unbounded scene에 대한 복원 성능이 정확하지 않음      Contributions  Volume Rendering 측면 :          projection-based 방법이랑 다르게, explicit한 ray-Gaussian intersection을 이용해서 volume rendering할 때의 Gaussian의 “기여도”를 결정함      이러한 ray-tracing 으로부터 기반된 formula는 ray상에 존재하는 모든 Point에 존재하는 Gaussian의 opacity를 결정짓게 할 수 있음 //  (여기까진 RaDe-GS에서도 ray-tracing intersection을 이용해서 rasterized method로 가우시안의 primitive들을 풀어냈다는 점이 유사하다.)      “view independence” = 모든 뷰에 대해 opacity가 최소인 값을 취하면 view에 대해 독립적이다(??) : opacity field가 Poisition에 대한 함수를 전적으로 책임짐        Surface normal 측면 :          ray와 Gausssian사이의 intersection plane(교차 평면)에 대한 normal        Surface extraction technique(alogirthm) 측면 :          poisson recon, marching cube와는 다른 메소드      tetrahedra-grids(다면체 그리드)에 기반한 ‘Marching tetrahedra’ 사용함                  3D Gaussian primitive중에서 3D bounding box의 코너값과 중앙값을 이용하여 다면체 메쉬의 꼭짓점 집합으로 구성하는 방법임                    Methods1. Modeling  Given multiple posed + calibrated images      3D scene은 3D Gaussian의 집합 $\\mathcal{G}_k$은 중심점 $\\mathrm{p}_k$, scaling matrix $\\mathrm{S}_k$, 그리고 rotation matrix $\\mathrm{R}_k$으로 파라미터화되고, 이는 쿼터니언(quaternion, 복소수의 확장형태)로 아래처럼 나타내짐 :\\[\\begin{align} \\mathcal{G}_k(\\mathrm{x}) = e^{-\\frac{1}{2}(\\mathrm{x}-\\mathrm{p}_k)\\Sigma_k^{-1}(\\mathrm{x}-\\mathrm{p}_k)} \\end{align}\\]  Ray Gaussian Intersection  ray와 Gaussian이 만나는 intersection의 정도를 이용한다.  RaDe-GS에 있던 내용이랑 겹침, 그래서 어떤 부분이 어디서부터 노벨티인지 분간이 안된다(ray tracing chapter advanced ver공부 더 해야겠다)  “ray intersection” : 1-D Gaussian Function이 최대화되는 점          point $\\mathrm{x} = \\mathrm{o} + t\\mathrm{r}$ , r은 ray direction, t는 ray의 depth            local coordinate system으로 point $\\mathrm{x}$ 을 변환 &amp; scale로 normalize한다\\[\\begin{align} \\mathrm{o}_g &amp;= \\mathrm{S}_k^{-1}\\mathrm{R}_k(\\mathrm{o}-\\mathrm{p}_k) \\\\               \\mathrm{r}_g &amp;= \\mathrm{S}_k^{-1}\\mathrm{R}_k\\mathrm{r} \\\\               \\mathrm{x}_g &amp;= \\mathrm{o}_g + t\\mathrm{r}_g \\end{align}\\]        ray 선 상에 존재하는 depth t에서의 1-D Gaussian value :\\[\\begin{align} \\mathcal{G}_k(\\mathrm{t}) = e^{-\\frac{1}{2}\\mathrm{x}_g^T\\mathrm{x}_g} = e^{-\\frac{1}{2}(\\mathrm{r}_g^T\\mathrm{r}_gt^2 + 2\\mathrm{o}_g^T\\mathrm{r}_gt + \\mathrm{o}_g^T\\mathrm{o}_g)} \\end{align}\\]        위의 가우시안 value가 t에 대한 quadratic term(이차형식)을 포함하고 있으므로 미분 이용해서 최대화되는 지점의 $t^*$을 유도 가능(RaDe-GS review에서 했음) :\\[\\begin{align} t^* = -\\frac{B}{A} \\end{align}\\]    where, $A =\\mathrm{r}_g^T\\mathrm{r}_g ,$  $B = \\mathrm{o}_g^T\\mathrm{r}_g$ 이렇게 구한 ray-Gaussian intersection은 world space에서 바로(directly) 구해질 수 있다는 점에서 surface normal구할 때도 유용한 특성임        “Gaussian $\\mathcal{G}_k$에 대한 기여도(Contribution) $\\mathcal{E}$ 라고 정의함 :\\[\\begin{align} \\mathcal{E}(\\mathcal{G}_k, \\mathrm{o}, \\mathrm{r}) = \\mathcal{G}_k^{1D}(t^*) \\end{align}\\]      Volume Rendering      camera ray상의 pixel color는 Gaussian primitive의 depth로 정렬된 순서에 따라서 alpha blending을 통해 렌더링됨\\[\\begin{align} \\mathrm{c}(\\mathrm{o}, \\mathrm{r}) = \\sum\\limits_{k=1}^{K}\\mathrm{c}_k\\alpha_k\\mathcal{E}(\\mathcal{G}_k, \\mathrm{o}, \\mathrm{r})\\prod\\limits_{i=1}^{k-1}(1 - \\alpha_j \\mathcal{E}(\\mathcal{G}_j, \\mathrm{o}, \\mathrm{r})) \\end{align}\\]  where, $c_k$ : view-dependent color modeled with spherical harmonics and $\\alpha_k$ : additional parameter that influences the opacity of Gaussian $k$.      (tile-based rendering process) 즉 standard 3DGS와 같이 depth 기반 alpha blending방식으로 pixel color rendering하는 방식 사용함      2. Gaussian Opacity Fields      projected 2D Gaussian대신 ray-Gaussian Intersection을 이용하기 때문에, ray에 존재하는 어떠한 점이라도 opacity value($\\mathrm{O}_k(\\mathcal{G}_k, \\mathrm{o}, \\mathrm{r}, t)$)를 구할 수 있다는 장점\\[\\begin{align} \\mathrm{O}_k(\\mathcal{G}_k, \\mathrm{o}, \\mathrm{r}, t) = \\begin{cases}\\mathcal{G}_k^{1D}(t) &amp; \\text{if } t \\leq t^* \\\\\\mathcal{G}_k^{1D}(t^*),  &amp; \\text{if } t  &gt; t^*\\end{cases} \\end{align}\\]        이렇게 구해진 ray상의 opacity value를 이용하여 volume rendering process는 아래 수식처럼 이루어짐 :\\[\\begin{align} \\mathrm{O}(\\mathrm{o}, \\mathrm{r}, t) = \\sum\\limits_{k=1}^{K}\\alpha_k \\mathrm{O}_k(\\mathcal{G}_k, \\mathrm{o}, \\mathrm{r}, t) \\prod\\limits_{i=1}^{k-1}(1 - \\alpha_j \\mathrm{O}_k(\\mathcal{G}_k, \\mathrm{o}, \\mathrm{r}, t)) \\end{align}\\]        이 때 3D point는 모든 뷰에서 보여지는데, 3D point $\\mathrm{x}$의 opacity는 이 모든 학습 뷰에 대한 opacity value 중 최솟값으로 정의한다.\\[\\begin{align} \\mathrm{O(\\mathrm{x})} = \\min\\limits_{(o, r)}\\mathrm{O}(\\mathrm{o}, \\mathrm{r}, t) \\end{align}\\]        위의 $\\mathrm{O(\\mathrm{x})}$를 \"Gaussian Opacity Fields(GOF)\"라고 언급한다.          이 GOF를 이용하면 poisson reconstruction이나 TSDF Fusion없이도 surface를 바로 추출할 수 있다.      by identifying their level sets      본 논문에서 만든 Tetrahedral 기반 메쉬 추출 방법과 연결되어 사용함, 4.섹션에 더 자세히 수록      3. Optimization  기본적으로 2DGS에서 언급된 loss들(depth distortion, normal consistency loss)를 이용하여 정규화함3.1. Depth Distortion loss  Mip-NeRF360 논문에서 처음으로 제안됨      ray-Gaussian intersection들이 더 밀집되고 집중되게 해주는 것 촉진함\\[\\begin{align} L_d = \\sum\\limits_{i,j}w_iw_j|t_i-t_j| \\end{align}\\]                  where $w_i$ : i번째 가우시안의 blending weight \\[w_i = \\alpha_k\\mathcal{E}(\\mathcal{G}_k, \\mathrm{o}, \\mathrm{r})\\prod_{j=1}^{k-1}(1-\\alpha_j\\mathcal{E}(\\mathcal{G}_k, \\mathrm{o}, \\mathrm{r}))\\]              하지만, depth distortion만으로는 Gaussian마다의 거리와 weight를 모두 감소시키는 정규화 텀이기 때문에, 이것은 alpha values가 증가하는 결과가 초래될 수 있다.          alpha blending에서 섞여지는 초기 가우시안의 alpha value값이 지나치게 크다면, 과장된 Gaussian이 초래되고 이것은 \"floater\"를 유발하는 원인이 됨        따라서, 가우시안끼리의 거리에 대해서만 최소화하고, blending weight $w_i$는 최소화 텀에서 뗴어냄3.2. Normal Consistency loss\\[\\begin{align} L_n = \\sum\\limits_{i}w_i(1-n_i^TN) \\end{align}\\]  2DGS의 normal consistency regularization을 바로 3D normal로 적용하는 것이 챌린지  2D Gaussian의 gradient는 항상 투영된 image plane에서의 가우시안 중심점(center, mu)에서 바깥쪽으로 위치하는 특성이 있음          투영된 2D 가우시안 중심에서 픽셀 좌표까지의 방향이 다르면, 두 개의 다른 픽셀에서 렌더링된 노멀은 서로 다르게 나타난다 =&gt; 정확성이 떨어지는 모호함이 발생함            이 문제 완화를 위해서 3D Gaussian의 normal을 ray direction r이 주어졌을 때, intersection plane의 normal로 정의함.      Final Loss\\[\\begin{align} L = L_c + \\alpha L_d + \\beta L_n \\end{align}\\]  where,  $L_c$ : RGB reconstruction loss with combining $L_1$ with the D-SSIm term4. Surface Extraction (Marching-Tetrahedral)  tetrahedral(다면체) 기반으로 그리드를 생성하여 메쉬 추출하는 알고리즘을 이용  학습 이후, surface 또는 triangle mesh extraction 단계  전통적인 메쉬 추출 방법들은 DTU dataset처럼 작은 물체단위의 베경없는 foreground object(regions of interest)영역은 비교적 잘 되지만, large-scale의 unbounded dataset에는 성능 좋게 나오는게 어려운 문제였음          dense evaluation으로 하는 기존 방법 -&gt; 그리드의 해상도에 따라 computation complexity가 증가한다는 점에서, large scale mesh에 적합하지 않고 시간이 매우 많이 걸리는 문제        본 논문에서 novel method 소개 : “Tetrahedral grid”를 이용한 “Marching Tetrahedra“4.1. Tetrahedral Grids Generation  3D Gaussian의 primitive에서 position과 scale value는 surface의 존재에 유의미한 정보를 주는 역할  각각의 가우시안을 감싸는 3D Bounding box를 정의 :          3d box의 중심점에서 가장 높은 opacity를 가지고, 가장자리 꼭짓점(corner)에서 가장 작은 opacity를 가진다.      이 opacity자체를 고려하는 것은 아님, 낮은 opacity value를 가지는 가우시안을 filter out(pruning)하긴 함        bbox의 center와 corner들로 [사면체 그리드]를 생성함          CGAL 라이브러리 이용(Tetra-NeRF에서 영감받아 사용)해서 [Delaunay triangulation] 수행      들로네 삼각법이란? : 2D 평면의 점 집합을 삼각형들로 연결하는 방법 중 하나로, 삼각형의 내접원의 원 안에 다른 점이 포함되지 않도록 삼각형을 구성하는 기법      refer : 들로네-삼각분할        생성된 사면체 그리드에서 filtering step을 통해 겹쳐지지 않은 가우시안과 연결된 edge를 포함하는 사면체 cell들을 제거함          겸쳐지지 않은 가우시안으로 판별하는 과정 : 사면체의 edge 길이가 그것에 대한 maximum scale(평균에서 3σ(표준편차)의 최대 범위까지 확장된 차원)의 합을 초과할 때      4.2. Efficient Opacity Evaluation  앞서 구해진 사면체 그리드의 vertices 즉, 꼭짓점에서의 opacity를 측정하기 위해, 3DGS의 rasterized method처럼 tile-based evaluation algorithm 설계함          꼭짓점들을 image space로 projection한 후, 타일로 쪼개져있을 때 그 대응되는 타일 ID를 확인한다.      각각의 타일에 대해서 projection으로 들어가져있는 point 리스트를 얻을 수 있다. 그 후에 점들을 다시 pixel space로 projection해서 대응되어 떨어지는 pixel을 구할 수 있다.      따라서 해당 pixel에 기여하는 가우시안들을 추적할 수 있고 이 과정은 모든 학습 이미지들에 대해서 수행된다.      그 후, pre-filtered된(pruning) Opcaity를 가지는 가우시안들중에서 minimum값을 Tetrahedral grid 꼭짓점의 opacity로 삼는다.      4.3. Binary Search of level Set  traingle mesh 추출 단계 via. Marching Tetrahedral method  “marching tetrahedral” :           선형 보간(linear interpolation)이 level set구분하는 것에 의존하기 때문에, Opacity Field라는 가우시안의 비선형적 특성에는 misalign되어서 성능이 불완전함      선형 추정(linear assumption)을 늘려가면서 non-linear한 opacity field로 level set을 정확하게 확인      Binary Search(이진 탐색)알고리즘으로 구현함                  8 iteration binary search한 것이 dense evaluation 256번 한 것이랑 같은 시뮬레이션 효과나오는 것 확인함                            Experiments  custom CUDA kernel  DTU, TnT 같은 foreground simple dataset뿐만 아니라 Mip-Nerf360 dataset처럼 unbounded된 scene에서 background영역까지 복원 잘 됨  느낀점 &amp; Future work  ray-Gaussian Intersection(GOF)과 ray-tracing based rasterized method(RaDe-GS)에 나오는 공통 방법론이 어디서부터 시작된 이론인지 레퍼런스 찾기  (mip-nerf 360) 논문에서 depth distortion loss 부분 중심으로 읽기 (완)  들로네 삼각분할 « 대충 훑기만 해선 이해안됨  포아송 재건, 마칭큐브// 마칭-사면체 같은 메쉬추출 알고리즘만 정리돼있는 내용 어디없는지 확인"
  },
  
  {
    "title": "RaDe-GS, arXiv preprint",
    "url": "/posts/RaDe-GS/",
    "categories": "Paper Review",
    "tags": "Surface reconstruction, stereo vision, Graphics",
    "date": "2025-02-07 19:34:00 +0900",
    





    
    "snippet": "RaDe-GS: Rasterizing Depth in Gaussian Splatting  GOF(Gaussian Opacity Fields, SIGGRAPH Asia 2024) 다음에 읽기Abstract  3DGS의 이산적이고 비구조적인 특성때문에 shape accurac가 떨어지는 문제 있음  최근 관련 연구 중, 2D-GS에서는 shape reco...",
    "content": "RaDe-GS: Rasterizing Depth in Gaussian Splatting  GOF(Gaussian Opacity Fields, SIGGRAPH Asia 2024) 다음에 읽기Abstract  3DGS의 이산적이고 비구조적인 특성때문에 shape accurac가 떨어지는 문제 있음  최근 관련 연구 중, 2D-GS에서는 shape reconstruction 성능을 올리기 위해, Gaussian primitives들을 이용했지만 이것은 렌더링 퀄리티랑 계산적인 효율성은 감소시키는 효과가 있음  그래서 본 논문에서는 rasterized 접근법을 통해, 3DGS의 depth map과 surface normal map을 렌더링하는 Shape reconstruction 정확성 보장  DTU dataset에서 NeuraLangelo와 비교했을 때 CD(Chamfer Distance) error가 좋게 나옴Introduction  multi-view 이미지들로부터 3D Reconstruction하는 태스크에서는 multi-view stereo 알고리즘을 통해 depth map을 얻는 것을 포함한다.  이렇게 얻어진 depth map으로 완전한 triangle mesh를 만드는 모델로 통합되는 것이 가능하다      전통적인 image-based 모델링 접근법은 꽤 정확한 결과로 depth map rendering + mesh recon이 가능하나, 고질적으로 빛나는 표면(shiny surface)이나 reflective한 유리같은 transparent surface에서 특히나 robustness가 떨어지는 한계점이 존재한다.    explicit한 3DGS에서 surface 잘 뽑아내는 것은 어려운 문제임          2DGS[Huang et al.2024] : PSNR 수치 줄어듦      GOF[Yu et al.2024] : ray-tracing 기반 방법을 통해 광선(light ray)에 따른 Gaussian opacity를 계산하는 방법으로 high-quality surface를 뽑아냄      Contributions  본 논문에서는 raasterized 기반 방법으로 general Gaussian Splatting에서 정확한 depth map을 얻는 것을 발전시켰다.  standard GS만큼의 computation efficiency를 가진다.  DTU dataset에서 shape reconstruction accuracy를 Chamfer Distance 0.69mm 달성, 5분의 학습시간          이 수치는 implicit한 representation 기반 모델인 NeuraLangelo(0.69mm)와 비슷      GS-based 방법론들(sota는 GOF)에서는 당연히 성능 더 좋았음      Methods  scene에 splatting된 Gaussian들과 광선에 의한 intersection point를 통한 솔루션을 찾음  💡Key idea💡   &gt; camera center로부터 각각의 Light ray에 의해 Gaussian value가 최대화되는 지점이 intersection point이다   &gt; intersection point들에서 Affine-projection을 시키면 co-planar (동일 평면상에 존재)하다   &gt; 그리고 이 intersection point들을 Image plane에 projection 된 depth로 정의하여 curved surface를 구할 수 있다   &gt; 따라서, planar equation으로 projected depth를 구하는데 이것은 rasterization에 의해 효율적으로 계산될 수 있다.     &gt; final depth map은 투명도(translucency)를 고려하여 투영된 Gaussian들 중에서 중간값 깊이(median depth)로 계산1. Rasterizing Depth for Gaussian Splats      Gaussian Splatting에서는 아래 사진처럼 Perspective camera projection을 Affine transformation을 통해 근사시켜서 더 효율적인 rendering가능하다는 것 사전에 알아두고 시작        standard 3DGS에서는 알파 블렌딩을 위해서 image plane에 projected된 2D Gaussian의 중심점(center)을 depth로 설정한다.          shape detail 포착 불가능        그러므로, 본 논문에서는 spatially varying depth를 rasterized method기반 방법으로 계산한다.1.1. Depth Under Perspective Projection  planar equation을 얻는 본격적인 과정 이전에, perspective projection으로 camera coordinate에 대한 기본 concepts부터 먼저 이해해보자.          Figure 4.              위 그림에서 왼쪽의 (a)에서 보여진 것 처럼, camera center $\\mathrm{o}$와 unit direction $\\mathrm{v}$가 주어졌을 때, ray 상에 $o$로부터 거리 $t$만큼 떨어져 있는 point $\\mathrm{x}$는, \\(\\begin{align} \\mathrm{x}= \\mathrm{o} + t\\mathrm{v} \\end{align}\\)으로 구할 수 있다.        이 ray에 존재하는 Gaussian value $\\mathrm{G}^1(t)$는,\\(\\begin{align} \\mathrm{G}^1(t)= e^{-(\\mathrm{o} + t\\mathrm{v}-\\mathrm{x}_c)^T\\Sigma^{-1}(\\mathrm{o} + t\\mathrm{v}-\\mathrm{x}_c)} \\end{align}\\)으로 구할 수 있다.    이 때, Gaussian value는 1-D function이고, 이 값이 최대화되는 지점이 3D Gaussian과 ray가 만나는 “intersection point” 라고 정의한다.      그 후 distance $t^*$ : intersection point와 camera center간의 거리는 위의 Gaussian value가 최대화되는 t를 찾으면 되고 구한 식은 아래처럼 됨\\[\\begin{align} t^* = \\frac{\\mathrm{v}^T\\Sigma^{-1}(\\mathrm{x}_c - \\mathrm{o})}{\\mathrm{v}^T\\Sigma^{-1}\\mathrm{v}} \\end{align}\\]    (유도 과정) :        이렇게 유도된 distance $t^*$이 의미하는 바는, 3D Gaussian들과 광선들 집합간의 intersection들이 “curved surface”(연두색 곡선) 를 나타낸다고 해석할 수 있음          different pixels have different depth values of $t^*$ and different viewing direction $\\mathrm{v}$.      1.2. Depth Under Local Affine Projection  이제 가우시안말고, 픽셀의 depth를 “local affine projection”으로 구해보자.      위의 Figure4.(b)상황 =&gt; “ray space” 좌표계    [1.2.1. Transformation from Camera to Ray space]          (a) 의 파란 점 $\\mathrm{x} = (x, y, z)^T$ 가 (b)의 파란 점 $\\mathrm{u} = (u,v,t)$ 로 변환      $(u,v,t)$에서 $(u,v)$는 image plane coordinate(이미지 평면에서의 좌표계)이고, $(t)$는 point와 $uv-plane$사이의 거리를 나타낸다      즉, $t=\\sqrt{x^2+y^2+z^2}$              ray space에서 light direction $\\mathrm{v}$ 는 단위고유벡터 $(0,0,1)^T$임                    가우시안들도 ray space로 변환되어야 하는데, 변환된 Gaussian Function은 아래와 같다\\[\\begin{align} \\mathrm{G}'(\\mathrm{u}) = e^{-(\\mathrm{u}-\\mathrm{u_c})^T\\Sigma^{'-1}(\\mathrm{u}-\\mathrm{u_c})} \\end{align}\\]        Gaussian center $\\mathrm{u_c}=(u_c, v_c, t_c)^T$로, (b)그림에서 빨간 점                  [1.2.2. Intersection in Ray Space]    기본 과정은 perpective projection때와 같음     1) ray space 상에 존재하는 point $\\mathrm{u}$  :\\[\\begin{align} \\mathrm{u} = \\mathrm{u_o} + t\\mathrm{v'} \\end{align}\\]    where , $\\mathrm{u_o} = (u,v,0)^T \\quad and \\quad \\mathrm{v’} = (0,0,1)^T $    2) 1D Gaussian function $\\mathrm{G^{‘1}}(t)$ :\\[\\begin{align} \\mathrm{G^{'1}}(t)=  e^{-(\\mathrm{u_o} + t\\mathrm{v'}-\\mathrm{u_c})^T\\Sigma^{'-1}(\\mathrm{u_o} + t\\mathrm{v'}-\\mathrm{u_c})} \\end{align}\\]    3) maximum point가 위치한 distance $t^*$ :\\[\\begin{align} t^* = \\frac{\\mathrm{v'}^T\\Sigma^{'-1}(\\mathrm{u}_c - \\mathrm{u_o})}{\\mathrm{v'}^T\\Sigma^{'-1}\\mathrm{v'}} \\end{align}\\]    간단하게 표현하기 위해서 $(\\mathrm{u}_c - \\mathrm{u_o})$ 벡터 빼고 나머지 항들을 $\\hat{q}$로 정의하여 간단하게 아래처럼 표현 :\\[\\begin{align} t^* = \\hat{q}(\\mathrm{u}_c - \\mathrm{u_o}) \\end{align}\\]        [1.2.3. Depth of Intersection]    1) $t$는 3D point $x$와 camera center $o$사이의 거리이므로 x에 대한 깊이는, 간단하게 삼각비 성질을 이용해서 (a)그림의 $z^*$을 아래처럼 구할 수 있다:\\[\\begin{align} d = cos\\theta t^* \\end{align}\\]    2) (b), affine projection인 상황에 대입해서 depth구하면,\\[\\begin{align} d = cos\\theta_c t^* = \\frac{x_c}{t_c}t^* = \\frac{x_c}{t_c}\\hat{q}(\\mathrm{u}_c - \\mathrm{u_o}) = \\hat{p}(\\mathrm{u}_c - \\mathrm{u_o})    \\end{align}\\]    여기서 $\\hat{p} = \\frac{z_c}{t_c}\\hat{q}$로 정의    3) 이어서 위의 식을 아래처럼 나타낼 수 있는데,\\[\\begin{align} d = \\hat{p}(\\mathrm{u}_c - \\mathrm{u_o}) =  \\hat{p}\\begin{pmatrix} u_c - u \\\\ v_c-v \\\\ t_c \\end{pmatrix} = \\hat{p}\\begin{pmatrix} 0 \\\\ 0 \\\\ t_c \\end{pmatrix} + \\hat{p}\\begin{pmatrix} \\Delta{u} \\\\ \\Delta{v} \\\\ 0 \\end{pmatrix}.  \\end{align}\\]    4) 위의 최종 분해된 식 중, 첫 번째 항 $\\begin{pmatrix} 0 \\ 0 \\ t_c \\end{pmatrix}$에 대한 성질 :\\[\\begin{align}       \\hat{p} \\begin{pmatrix} 0 \\\\ 0 \\\\ t_c \\end{pmatrix}       &amp;= \\frac{z_c}{t_c} \\hat{q} \\begin{pmatrix} 0 \\\\ 0 \\\\ t_c \\end{pmatrix} \\\\      &amp;= \\frac{z_c}{t_c} \\frac{\\mathrm{v^{'T}}\\Sigma^{'-1}}{\\mathrm{v^{'T}}\\Sigma^{'-1}\\mathrm{v'}} \\begin{pmatrix} 0 \\\\ 0 \\\\ t_c \\end{pmatrix} \\\\      &amp;= \\frac{z_c}{t_c} \\frac{\\mathrm{v^{'T}}\\Sigma^{'-1}}{\\mathrm{v^{'T}}\\Sigma^{'-1}\\mathrm{v'}} (t_c\\mathrm{v'}) \\\\      &amp;= \\frac{z_c}{t_c} \\frac{\\mathrm{v^{'T}}\\Sigma^{'-1}\\mathrm{v'}}{\\mathrm{v^{'T}}\\Sigma^{'-1}\\mathrm{v'}} (t_c) \\\\      &amp;= z_c .  \\end{align}\\]       5) 두 번째 항의 $\\hat{p}$ 는 $p$ 로 근사됨          따라서, 최종적으로 depth를 구하는 rasterized method수식으로 유도된다.       Recall. $d = z_c + p(\\begin{pmatrix} \\Delta{u} \\ \\Delta{v} \\end{pmatrix})^T$      2. Rasterizing Normal for Gaussian Splats  느낀점  이 분야에서는 살짝 뭐 바꿨더니 소타다 이런건 절대 안먹히겠다  완전히까진 아니여도 적당히 창의적으로 메소드를 짜야되겠구나라는 생각이 들었다. 이런 면이 재밌지만서도 다소 막막하긴 하다  Affine aprroximation projection기반으로 spatial varying depth 구하는 방법이 창의적이다  2DGS에서 쓴 loss 가져다 쓴 건 아쉬운데 이거까지 새로 짰으면 걍 씨그라프 오랄 찍었을 것 같다  이건 어셉 왜 안되지          GOF(SIGGRAPH Asia 2024)가 작년 9월에 나와서 sota인 것 같다. 이어서 이거 읽기      "
  },
  
  {
    "title": "Graphics-Ch6. Transformation Matrices",
    "url": "/posts/ch6-Transformation_Matrices/",
    "categories": "Graphics, Study",
    "tags": "Graphics",
    "date": "2025-02-07 13:04:00 +0900",
    





    
    "snippet": "Introduction  Chapter 5는 선형대수학(Linear Algebra)이라 아는 내용이 많아서 정리 생략  Transformation matrix는 왜 필요한가 : Rotation, Translation, Scaling, Projection같은 기하학적 변환에서 행렬 곱을 통해 변환되는데 이때 변환 행렬이 필요함1. 2D Linear Tr...",
    "content": "Introduction  Chapter 5는 선형대수학(Linear Algebra)이라 아는 내용이 많아서 정리 생략  Transformation matrix는 왜 필요한가 : Rotation, Translation, Scaling, Projection같은 기하학적 변환에서 행렬 곱을 통해 변환되는데 이때 변환 행렬이 필요함1. 2D Linear Transformations      2D vector를 2x2 행렬로 변환하는 과정은 아래처럼 전개됨 : \\[\\begin{bmatrix} a_{11} &amp; a_{12} \\\\ a_{21} &amp; a_{22} \\end{bmatrix}  \\begin{bmatrix} x \\\\ y \\end{bmatrix} = \\begin{bmatrix} a_{11}x &amp; a_{12}y \\\\ a_{21}x &amp; a_{22}y \\end{bmatrix}\\]        여기서, 벡터 $(x y)^T$에 곱해지는 행렬 $A$를 변환행렬이라고 한다  1.1. 2D Scaling\\[\\begin{align} scale(s_x, s_y) = \\begin{bmatrix} s_x &amp; 0 \\\\ 0 &amp; s_y \\end{bmatrix} \\end{align}\\]=&gt; Cartesian components와 결합되면 아래와 같이 전개됨 \\[\\begin{bmatrix} s_x &amp; 0 \\\\ 0 &amp; s_y \\end{bmatrix}  \\begin{bmatrix} x \\\\ y\\end{bmatrix} = \\begin{bmatrix} s_xx \\\\ s_yy \\end{bmatrix}\\]1.2. Shearing  물체에 평행한 방향으로 힘이 가해져 변형되는 현상      얼만큼($=s$만큼) x또는 y방향으로 물체를 밀 건지\\[shear-x(s) = \\begin{bmatrix} 1 &amp; s \\\\ 0 &amp; 1 \\end{bmatrix} , shear-y(s) = \\begin{bmatrix} 1 &amp; 0 \\\\ s &amp; 1 \\end{bmatrix}\\]        만약, 시계 방향으로 각도 $\\phi$만큼 회전시키는 shear transformation matrix :\\(\\begin{bmatrix} 1 &amp; tan\\phi \\\\ 0 &amp; 1 \\end{bmatrix}\\)    만약, 반시계 방향으로 각도 $\\phi$만큼 회전시키는 shear transformation matrix :\\(\\begin{bmatrix} 1 &amp; 0 \\\\ tan\\phi &amp; 1 \\end{bmatrix}\\)1.3. Rotationvector $\\mathrm{a}$가 x축으로부터 각도 $\\alpha$만큼 떨어진 길이가 r인 벡터라는 상황 가정 반시계방향(counterclockwise)으로 벡터 a를 회전시킨 벡터를 $\\mathrm{b}$라고 두자.   $$ \\begin{align} \\mathrm{a} = (x_a, y_a) , \\mathrm{b} = (x_b, y_b) \\end{align} $$  $$ \\begin{align} x_a = rcos\\alpha \\\\ y_a = rsin\\alpha \\end{align} $$    $$ \\begin{align} x_b = rcos(\\alpha+\\phi)=rcos\\alpha cos\\phi - rsin\\alpha sin\\phi \\\\ y_b = rsin(\\alpha+\\phi) = rsin\\alpha cos\\phi + rcos\\alpha sin\\phi \\end{align} $$  위 식에서, $x_a =rcos\\alpha$ 그리고 $y_a = rsin\\alpha$ 를 대입하면 아래처럼 정리된다. \\[\\begin{align} x_b = x_a cos\\phi - y_a sin\\phi \\\\ y_b = x_bcos\\phi + x_a sin\\phi \\end{align}\\]따라서, vector $\\mathrm{a}$에서 vector $\\mathrm{b}$로 가는 Rotation matrix term은 아래와 같다 :\\[\\begin{align} rotate(\\phi) = \\begin{bmatrix} cos\\phi &amp; -sin\\phi \\\\ sin\\phi &amp; cos\\phi \\end{bmatrix} \\end{align}\\]  각 행에 대한 원소 norm $(sin^2\\phi + cos^2\\phi = 1)$ 이기 때문에, 행들은 서로 직교한다(orthogonal)  따라서, Rotation matrix를 직교행렬(orthogonal matrix)로 볼 수 있다.1.4. Reflection  대칭이동(축을 기준으로 뒤집는 변환) \\[\\begin{align} reflect-y = \\begin{bmatrix}  -1 &amp; 0 \\\\ 0 &amp; 1 \\end{bmatrix}  ,\\quad reflect-x = \\begin{bmatrix}  - &amp; 0 \\\\ 0 &amp; -1 \\end{bmatrix}  \\end{align}\\]1.5. Composition and Decomposition of Transformations      처음 상태 : 2D vector $\\mathrm{v_1}$        scale transformation $S$ 적용 이후, Rotation transformation $R$을 적용한다면 : \\[\\begin{align} \\mathrm{v_2} = S\\mathrm{v_1}, \\quad then, \\mathrm{v_3} = R\\mathrm{v_2} \\end{align}\\]\\[\\begin{align}  \\mathrm{v_3} = R(S\\mathrm{v_1}) . \\\\ \\mathrm{v_3}=(RS)\\mathrm{v_1} \\end{align}\\]  처럼 나타낼 수 있다.따라서, $M = RS$행렬을 변환 행렬로 취급한다.  중요한 것은, 이러한 변환들은 오른쪽 변환부터 적용된다는 것을 혼동하면 안됨, 즉, RS에서 scaling먼저 적용하는 의미  이 순서가 바뀌면 결과도 달라짐1.6. Decomposition of Transformations  Compoisition말고 곱해진 상태의 변환 행렬을 “분해”하는 방법  크게 고유값 분해(Eigenvalue Deomposition) 기반과 특이값 분해(Singular Value Decomposition)기반 방법이 있다.1. Symmetric Eigenvalue Deomposition      행렬이 symmetric(대칭)행렬일 때, \\(\\mathrm{A} = \\mathrm{R}\\mathrm{S}\\mathrm{R}^T\\) 으로 분해됨      1) $\\mathrm{R}^T$ 변환 : 행렬 A의 eigen vector(고유벡터)인 $\\mathrm{v_1}$과 $\\mathrm{v_2}$를 x 또는 y축 기반으로 회전시킴 2) $\\mathrm{S}$ 변환 : x와 y를 $(\\lambda_1, \\lambda_2)$만큼 scaling 시킴 3) $\\mathrm{R}$ 변환 :  $\\mathrm{v_1}$과 $\\mathrm{v_2}$를 x 또는 y축 기반으로 다시 역회전시킴   2. Singular Value Deomposition  전체적으로 과정은 같음, 대신 전체 변환행렬 A가 대칭행렬이 아님  고유값 대신 특이값사용2. 3D Linear Transformations  2D 변환의 확장된 ver.3. Translation and Affine Transformations[1] 변환 행렬 M이 곱해졌을 때, \\[\\begin{align} x' = m_{11}x + m_{12}y \\\\ y' = m_{21}x + m_{22}y \\end{align}\\][2] Translation 이 있을 때, \\[\\begin{align} x' = x + x_t \\\\ y' = y + y_t \\end{align}\\]위의 두 가지 변환 연산을 합친 Transformation matrix M을 single하게 어떻게 나타낼 것인가, trick : Homogeneous coordinate을 이용함 :  $(x \\quad y) -&gt; [x \\quad y \\quad 1]^T $\\[M = \\begin{bmatrix}  m_{11} &amp; m_{12} &amp; x_t \\\\ m_{21} &amp; m_{22} &amp; y_t \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix}\\]이렇게 곱해지는 형태의 변환을 “Affine Transformation” 이라고 한다.… 이어서"
  },
  
  {
    "title": "Graphics-Ch4. Ray Tracing",
    "url": "/posts/ch4-Ray-Tracing/",
    "categories": "Graphics, Study",
    "tags": "Graphics",
    "date": "2025-02-04 17:34:00 +0900",
    





    
    "snippet": "Rendering  Object based Rendering          각각의 object단위로 고려됨      물체가 존재하는 모든 픽셀들이 찾아지고 업데이트 됨        Image based Rendering          각각의 pixel단위로 고려됨      물체에 영향을 주는 각 pixel들이 찾아지고 업데이트 됨둘의 차이점에 대해...",
    "content": "Rendering  Object based Rendering          각각의 object단위로 고려됨      물체가 존재하는 모든 픽셀들이 찾아지고 업데이트 됨        Image based Rendering          각각의 pixel단위로 고려됨      물체에 영향을 주는 각 pixel들이 찾아지고 업데이트 됨둘의 차이점에 대해서는 ch8에서 더 자세하게 다룬다      Ray Tracing이란  3D Scene을 렌더링하는데에 사용되는 image-order 알고리즘  object-order rendering에서 사용되어지는 수학적 방법1. Basic Ray-Tracing Algorithm  image상의 Pixel에서 ray tracing을 했을 때 보여지는 물체를 찾는 것이 목표  각각의 픽셀은 다른 방향을 “바라본다”          이 보여지는 물체들은 모두 viewing ray 상에 존재한다        카메라에 가장 가깝게 존재하는 viewing ray에 대해 교차하고 있는 특정 Object를 찾는 것이 목표      그 물체가 찾아지면, shading 계산을 통해 intersection point, surface normal, 그리고 다른 정보들을 구해서 pixel color를 결정할 수 있음    Basic Ray Tracer의 세 가지 흐름 :          Ray Generation : Camera geometry에 기반하여, origin과 각 픽셀마다 vieweing ray의 direction을 구하는 것      Ray Intersection : viewing ray와 교차하는 가장 가까운 물체(Object)를 찾는 것      Shading : ray-intersection 과정을 통해 구해진 결과를 기반으로 Pixel color를 계산하는 것        기본 내용을 다룬다, 더 발전된 내용은 chapter 10, 12, 13 등등에 나옴2. Perspective projection  how to mapping 3D objects into 2D image plane?                            Linear perspecrive          straight line =&gt; straight line                                      Parallel projection          projection direction에 따라서 움직여짐                          parallel projection을 통해 보이는 뷰는 orthographic view라고도 부른다          장점 : 변환되어도 size랑 shape은 변화시키지 않고 일정하게 유지한다          단점 : 물체가 view point기준으로 멀어질수록 작게 보인다 =&gt; vanishing point(소실점)과 연관 있음                          this is because eyes and cameras don’t collect light from a single viewing direction(뷰 하나 보이는 것으로 눈과 카메라에서 모든 빛을 수집할 수 없기 때문임)                                           3. Computing Viewing Rays (Ray Generation)\\(p(t) = e + t(s-e)\\)  $e$ : eye(origin point, view point)  $s$ : end point of the ray      $t$ : 시간 축    vector $(s-e)$ 를 view direction로 해석할 수 있다  만일 $t$가 0보다 작다면, view behind에 있다고 해석할 수 있다  “Ray Generation” 단계의 origin과 view direction을 구하는 과정에서 camera frame으로 알려진 orthonormal 좌표계에서 시작해야 한다.          orthonormal coordinate frame : 세 가지 기저 벡터 $u, v, w$으로 구성되어 있음      3.1. Orthographic (Parallel) Views  이미지를 표현하는 4가지 차원 = ${l, r, b, t}$[Orthographic viewing rays 만드는 방법]  ray Starting point(Origin)로 pixel의 image-plane position을 그대로 사용할 수 있다 = $e + u\\mathrm{u} + v\\mathrm{v}$  ray’s Direction으로는 view direction을 가져다가 사용할 수 있다 = $-w$3.2. Perspective Views  각 픽셀마다의 ray들은 같은 origin$(e)$를 공유함          Image Plane이 더이상 $e$ 점에 위치하는 것이 아니라, 거리 $d$만큼 떨어져서 존재함      $d$ : image plane distance 또는 focal length 라고 불린다        view direction은 모두 다름          어떻게 구해지는가? -&gt; image planedml pixel위치와 viewpoint에 의해서 정의된다 = $-dw + u\\mathrm{u} + v\\mathrm{v}$      4. Ray-Object Intersection  앞선 Ray Generation단계를 통해 ray $\\mathrm{e} + t\\mathrm{d}$를 만들었다면, $t &gt; 0$ 인 구간에서 ray와 처음으로 교차되는(만나는) 물체를 찾는 것이 필요하다.  General problem = find the first intersection between the ray &amp; a surface that occurs at a $t$ in the interval $[t_0, t_1]$          sphere과 traingles 물체(surface)를 먼저 다루고, 다른 다면체들은 다음 섹션에 다루겠음      4.1. Ray-Sphere Intersection다음의 ray와 surface가 만나는 교차점을 구한다고 생각해봅시다,  ray $p(t) = \\mathrm{e} + t\\mathrm{d}$  implicit surface $f(p) = 0$\\[\\begin{align} f(p(t)) = 0 \\\\  f(\\mathrm{e} + t\\mathrm{d}) = 0 \\end{align}\\]      그리고 sphere는 중심점 $c = (x_c, y_c, z_c)$와 반지름 $R$을 이용해서 아래처럼 implicit equation을 나타낼 수 있음\\(\\begin{align} (x - x_c)^2 + (y - y_c)^2 + (z - z_c)^2 - R^2 = 0 \\\\  (p-c) \\cdot (p-c) - R^2 = 0    \\end{align}\\)              이 vector form 구의 방정식에서 p에다가 ray $p(t)$를 집어넣어도 식이 성립해야 함, intersection point이기 때문      $(\\mathrm{e} + t\\mathrm{d} - c) \\cdot (\\mathrm{e} + t\\mathrm{d} - c) - R^2 = 0$                  식을 풀면 아래와 같이 $t$에 대한 2차방정식 term으로 풀어짐      $(d \\cdot d)t^2 + 2d \\cdot (e-c)t + (e-c) \\cdot (e-c) - R^2 = 0$            근의 공식을 이용하여 $t$를 구한다\\(t = \\frac{-d \\cdot (e-c) \\pm \\sqrt{(d \\cdot (e-c))^2 - (d \\cdot d)( (e-c) \\cdot (e-c) - R^2)}}{(d \\cdot d)}\\)        근의 공식 풀기 위해서는  판별식 ($B^2 - 4AC$) 이거를 통해 근이 몇 개 나오는지 먼저 판단한다, 이 값이 음수면 허수라서 교차점 없다고 판단하면 됨              section 2.5.4에서 다루었던 것처럼, point $p$에 있는 normal vector는 gradient를 통해 아래처럼 계산할 수 있음      \\mathrm{n} = 2(p-c)        unit normal = (p-c) / R4.2. Ray-Traingle Intersection  barycentric 좌표계 사용함 =&gt; 삼각형을 포함하는 parametric plane을 표현할 때 삼각형의 꼭짓점만 이용하면 다른 storage가 필요없는 효율적인 좌표계표현이기 때문  parameteric surface와 ray간의 intersection point구하는 방법 :          Cartesian coordinate (데카르트 좌표계)를 이용한 다음 연립방정식으로 푼다=&gt; 식이 세 개이고, 구해야 되는 미지수도 $(t, u, v)$로 세 개이기 때문에 계산 가능      왼쪽 그림과 같이, $a,b,c$ 의 vertices로 이루어진 삼각형이 존재하고 ray $p(t) = \\mathrm{e} + t\\mathrm{d}$가 존재하는 상황을 가정할 때, intersection point인 $p$는 그림과 같이 ray 연장선 위에 존재한다. $$ \\mathrm{e} + t\\mathrm{d} = \\mathrm{a} + \\beta(\\mathrm{b} - \\mathrm{a}) + \\gamma(\\mathrm{c}-\\mathrm{a}) $$ 이 식에서 $t, \\beta, \\gamma$를 구해야 함위의 식을 vector form으로 아래처럼 식을 확장할 수 있음 :\\[\\begin{align}x_e  + tx_d = x_a + \\beta(x_b - x_a) + \\gamma(x_c - x_a), \\\\y_e  + ty_d = y_a + \\beta(y_b - y_a) + \\gamma(y_c - y_a), \\\\z_e  + tz_d = z_a + \\beta(z_b - z_a) + \\gamma(z_c - z_a). \\\\\\end{align}\\]      vector form을 행렬로 바궈서 standard linear system(선형 결합 형태)로 아래처럼 바꿔 표현 가능:        그 후, 크래머 규칙(Cramer’s rule)을 이용해서 $t, \\beta, \\gamma$ 값을 도출할 수 있다 (자세한 풀이 생략 책 참고)  5. Shading  픽셀에 대한 intersection point, 즉 visible surface가 구해졌다면, 광원을 고려해서 pixel color(intensity) 결정하는 단계  Light Reflection(반사) 관련 모델링을 사용한다          중요한 변수들                  light direction $\\mathrm{l}$          view direction $\\mathrm{v}$          surface normal $\\mathrm{n}$                    5.1. Lambertian Shading  가장 간단한 shading modeling 을 위한 가정**Lambertian Shading**  : 왼쪽 그림에서 표면에 떨어지는 광원으로부터 나오는 빛의 양은  빛에 대한 입사각 $\\theta$에 의해서만 결정된다. (View-Independent) 이를 이용해서 lambertian Shading model식을 아래처럼 설계  $$ L = k_d I max(0, \\mathrm{n} \\cdot \\mathrm{l}) $$  $k_d$ : diffuse coefficient (난반사 계수) 또는 surface color = 표면이 입사된 빛을 얼마나 균일하게(난반사로) 반사하는지를 나타내는 계수  $I$ : intensity of the light source  여기서 n과 l은 크기가 1인 단위벡터이기 때문에, $\\mathrm{n} \\cdot \\mathrm{l}$을 $cos\\theta$ 로 계산 가능하다.          즉, 내적은 cosine 유사도를 의미하므로 빛이 들어오는 방향에 대해서 얼마나 반사되는지 그 강도(intensity)가 결정된다는 의미      =&gt; 위의 모델링 수식은 RGB 채널 3개에 대해서 각각 적용되어 pixel value가 구해진다  $\\mathrm{v}, \\mathrm{l}, \\mathrm{n}$이 모두 단위벡터(크기가 1)인 것을 잊지 말기!5.2. Blinn-Phong Shading  모든 광원이 난반사(diffuse)로만 구성되지는 않는다,  specular component (정반사되는 성질) 빛을 모델링하기 위한 모델  Idea : $\\mathrm{v}$랑 $\\mathrm{l}$ 이 surface normal $\\mathrm{n}$을 가로지르는 상황에서 반사가 가장 잘된다는 것          mirror reflection이 발생할때 반사율이 가장 크다.      오른쪽 그림처럼, $\\mathrm{v}$와 $\\mathrm{l}$ 각도 중간에 위치한 half vector $\\mathrm{h}$가 있다고 가정해보자, 이 h가 surface noraml n과 가까울수록, specular component가 증가한다. (밝기가 증가) $\\mathrm{h}$와 $\\mathrm{n}$사이의 유사도는 **dot product(내적)**으로 생각할 수 있다. *Phong exponent*라고 불리는 power $p$가 surface에 대한 광택의 정도를 조절하는 역할임\\[\\begin{align}h = \\frac{\\mathrm{v} + \\mathrm{l}}{||\\mathrm{v} + \\mathrm{l}||}, \\\\L = k_d  I  max(0, \\mathrm{n} \\cdot \\mathrm{l}) + k_s I max(0, \\mathrm{n} \\cdot \\mathrm{h})^p \\\\\\end{align}\\]where, $k_s$ is the specular coefficient, or the specular color of the surface5.3. Ambient Shading      조명이 도달하지 않는 표면에서는 완전하게 black으로 보이기 때문에, 이런 현상을 줄이기 위해서 constant component를 shading model에 추가시킴        Full version of a simple and useful shading model(Ambident shading components &amp; Blinn-Phong model) :\\[\\begin{align}l = k_aI_a + k_dImax(0, \\mathrm{n} \\cdot \\mathrm{l}) + k_s I Max(0, \\mathrm{n} \\cdot \\mathrm{h})^n\\\\  \\end{align}\\]  "
  },
  
  {
    "title": "2D Gaussian Splatting, SIGGRAPH 2024",
    "url": "/posts/2DGS/",
    "categories": "Paper Review",
    "tags": "Geometry Reconstruction, Mesh extraction, Novel View Synthesis, Graphics",
    "date": "2025-02-03 16:34:00 +0900",
    





    
    "snippet": "Intro  3DGS : Redering결과 Inconsistency한 depth발생, 왜냐하면 pixel ray사이의 intersection을 통해 gaussian value를 구하기 때문  2DGS : explicit ray-splat intersection 이용  surface normal 정확하게 추출하면 mesh도 잘 구해짐          ...",
    "content": "Intro  3DGS : Redering결과 Inconsistency한 depth발생, 왜냐하면 pixel ray사이의 intersection을 통해 gaussian value를 구하기 때문  2DGS : explicit ray-splat intersection 이용  surface normal 정확하게 추출하면 mesh도 잘 구해짐                  depth distortion        : 2D primitives에 집중해서 더 좁은 범위의 ray에 가우시안들이 분포하게 해줌                    normal consistency        : rendered normal map과 rendered depth의 gradient 사이의 discrepancies 를 최소화함              두 개 regularaization term을 이용해서 smoother surface 구함  Contributions  efficient differentiable 2D Gaussian Renderer  surface recon을 위한 2가지 정규화 텀 소개 (depth distortion, normal consistency)  sota Reconstruction &amp; NVS result1. Modeling  normal = the steepest change of density          better alignment with thin surfaces        input = only a sparse calibration point cloud &amp; photometric supervision2D Gaussian Explicit Properties  central point $p_k$  two principal tangential vector $t_u, t_v$      scaling vector $S = (s_u, s_v)$    primitive normal : $t_w = t_u \\times t_v$      3x3 Rotation matrix $R = [t_u, t_v, t_w]$    2D Gaussian들은 local tangent plane에 정의됨      $P(u,v) = p_k + s_ut_uu + s_vt_vv =H(u,v,1,1)^T$ ⇒  world space에서 정의된 local tangent plane에서의 2D Gaussian  point    where $H$   is the homogeneous transformation matrix, representing the geometry of the 2D Gaussian        $G($u$) = exp(-\\frac{u^2+v^2}{2})$    $point - \\mathrm{u} = (u, v)$  2. Splatting  image space로 2D Gaussian들을 project하는 과정  affine approximation of Perspective Transformation 이용          Perspective transformation은 평행성이 지켜지지 않음, parallel한 line들이 소실점(vanishing point)에서 만남              한계점(선행 조건필요) :                              center of the Gaussian이 정확해야하고,                                center로부터의 거리가 멀어질수록 approximation error가 증가해야함                                homogeneous coordinate을 이용해서 완화된 해결방법 제안됨      2D Splat을 2D image plane에 projection한느 건 일반적인 2D-to-2D mapping in homogeneous coordinate이라고 할 수 있다    $W$ :  transformation matrix from World space to Screen space      Screen space에서의 projected points $\\mathrm{x}$    $\\mathrm{x} = (xz, yz, z, 1)^T = WP(u,v) = WH(u,v,1,1)^T$ 로 구해짐    2D Gaussian들을 rasterize하려면, inverse transfomation 인 $M= (WH)^{-1}$을 이용하는 방법이 있으나, 이 방법은  numerical instability함 특히 splat이 line segment(view가 옆면에서 보이는 상황인 경우, )에서 특히 결함이 많음      이 문제 해결위해 previous surface splatting rendering method들은 미리 정의된 predefined threshold을 이용해서  ill-conditioned transformation을 이용해왔음 (related work)      ⇒ 모두 unstable함, 따라서 본 논문에서는 “Ray-Splat Intersection” 기반 방법을 제안2-1. Ray-Splat Intersection      평행하지 않은 3가지 평면 (three non-parallel planes) 에서의 교점을 찾음으로써 ray-splat intersection을 위치시킴  (??)        image space coordinate $\\mathrm{x} = (x, y)$가 주어졌을 때,    두개의 수직 plane (x-plane과 y-plane)의 교차 ray를 파라미터화한다            x - plane : normal vector인 (-1, 0, 0)과 offset x로 정의됨    ⇒ 4D homogeneous Plane (homogeneous coordinate을 이용한 4D 좌표계 표현한 개념)    $h_x = (-1, 0, 0, x)^T$        y - plane : normal vector인 (0, -1, 0)과 offset y로 정의됨    ⇒ $h_y = (0, -1,  0, y)^T$        ray $\\mathrm{x} = (x,y),$  ( Image Coordinate ) 는 x-plane과 y-plane사이의 교차되는 line에 대한 좌표로 결정    각각의 plane을 2D Gaussian의 local coordinate( $uv-$coordinate system)으로 변환해야 함  point로부터 plane으로의 transformation matrix $M$ 의 역행렬을 곱해주는 Inverse Transpose $M^{-1}$을 수행해야 intersection of the x&amp;y-plane에 대한 좌표를 아래처럼 구할 수 있음,                  이 때 $M = (WH)^{-1}$, $M^{-T} =  (WH)^T$\\[\\begin{align} h_u = (WH)^Th_x   \\\\ h_v = (WH)^Th_y \\end{align}\\]        where, W : world space에서 screen space로 가는 변환 행렬, H : homogeneous 좌표계로 변환하는 행렬              두 평면이 만나는 intersection point $\\mathrm{u}(\\mathrm{x})$  구하기                  2D Gaussian의 point  : $(u, v, 1, 1)$        $h_u^i , h_v^i$ 는 4D homogeneous plane 파라미터들 중에서 i번쨰 파라미터를 의미함          \\[\\begin{align}h_u \\cdot (u,v,1,1)^T = h_v \\cdot (u,v,1,1)^T = 0  \\\\ u(\\mathrm{x}) = \\frac{h_u^2h_v^4-h_u^4h_v^2}{h_u^1h_v^2-h_u^2h_v^1} \\\\ v(\\mathrm{x}) = \\frac{h_u^4h_v^1-h_u^1h_v^4}{h_u^1h_v^2-h_u^2h_v^1} \\end{align}\\]        Screen space에서의 projected points    $\\mathrm{x} = (xz, yz, z, 1)^T = WP(u,v) = WH(u,v,1,1)^T$    이거 식을 Recall해서 $x, y, u, v$ 다 아니까 depth $z$를 구할 수 있다.  3. Improvements3-1. Degenerate Case  object-space low-pass filter 제안\\[\\begin{align}    \\widehat{G}(\\mathrm{x}) = \\max \\left\\{ G(\\mathrm{u}(\\mathrm{x})), G\\left(\\frac{\\mathrm{x}-c}{\\sigma}\\right) \\right\\}\\end{align}\\]⇒ 직관적인 수식 의미 = fixed screen-space(오른쪽 항)에서의 Gaussian이랑 위에서 구한 intersection point u(x)에서의 가우시안중에 더 큰 값을 선택하여 degenerate하는 가우시안들 없앰where $c$  : projection of center $p_k$본 실험에서는 $\\sigma = \\sqrt{2}/2$로 설정함      Volumetric alpha blending (Rasterization)\\[c(\\mathrm{x}) = \\sum_{i = 1}c_i\\alpha_i\\widehat{G}_i(\\mathrm{u}(\\mathrm{x}))\\prod_{j =1}^{i-1}(1-\\alpha_i \\widehat{G}_i(\\mathrm{u}(\\mathrm{x})))\\]  4. Training  3DGS의 photometric loss에다가 2가지 정규화 텀을 추가시켜서 더 smooth한 surface align을 촉진하여 geometry reconstruction의 성능 향상4.1. Depth Distortion      Depth distortion loss : ray-splat intersection결과상 구한 depth 간의 거리 차를 최소화함으로써 ray에 존재하는 weight을 더 concentrate해줌,  Mip-Nerf360에서 영감 받음\\[\\begin{align} L_d = \\sum_{i,j}w_iw_j|z_i-z_j| \\end{align}\\]    where, $w_i :$i번째 intersection하는 지점에서의 blending weight    $z_i :$ i번째 교차점에서의 depth  4.2. Normal Consistency  교차하는 median point(중간 지점) $p_s$에서 actual surface 고려      축적된 opacity가 0.5가 되면, splat된 가우시안들의 normal과 depth map에서의 gradient를 고려해서 align해주는 과정\\[\\begin{align}L_n = \\sum_iw_i(1-n_i^T\\mathrm{N}) \\end{align}\\]    where $i :$ ray상에 존재하는 intersected 된 splat들    $w :$ blending weight of the intersection point    $n_i :$ normal of the splat that is oriented towards the camera    $\\mathrm{N} :$ normal estimate by the gradient of the depth map  Final Loss: $L = L_c + \\alpha L_d + \\beta L_n$$L_c :$ RGB reconstruction loss combining L1 with the D-SSIM term𝛼 = 1000 for bounded scenes, 𝛼 = 100 for unbounded scenes, and 𝛽 = 0.05 for all scenes.5. Experiments  single RTX 3090  Datasets          DTU                  15개 씬으로 구성됨 (49장 또는 69장 이미지, resolution = 1600 x 1200)          Colmap으로 sparse point cloud얻어진 상태, 해상도를 800 x 600 으로 downsample한 이후에 사용함 효율성을 위해서                    TnT      MipNerf360        Evaluation Metrics          PSNR      SSIM      LIPPS      Limitations  assume surfaces with full opacity and extract meshes from multi-view depth maps          semi-transparent한 표면에서는 잘 복원이 안됨, 유리같은 부분        fine geometric structure에 대해서는 덜 정확함  regularization term사용할 때 이미지 퀄리티와 geometry간의 trade-off가 발생함 ⇒ 특정 영역에서 over-smooth결과 초래할 수 있음"
  },
  
  {
    "title": "Graphics-Ch3. Raster Images",
    "url": "/posts/ch3-Raster/",
    "categories": "Graphics, Study",
    "tags": "Graphics",
    "date": "2025-01-29 18:00:00 +0900",
    





    
    "snippet": "Chapter 3. Raster Images서론  Raster image : 2D 행렬 형태로 픽셀에 대한 RGB color값을 표현한 이미지          RGB 색상은 3차원 벡터 형태        디지털 카메라는 image sensor를 포함하고 있는데, 이것은 빛이 들어오는 정도(intensity)와 color를 구성하고 있음  하지만 우리가...",
    "content": "Chapter 3. Raster Images서론  Raster image : 2D 행렬 형태로 픽셀에 대한 RGB color값을 표현한 이미지          RGB 색상은 3차원 벡터 형태        디지털 카메라는 image sensor를 포함하고 있는데, 이것은 빛이 들어오는 정도(intensity)와 color를 구성하고 있음  하지만 우리가 이러한 2D array자체로 이미지를 보지는 않음,          이미지 픽셀과 디스플레이 픽셀간의 매칭(direct link)이 필요하고 이를 Rasterizer가 처리함        Vector Image : 픽셀에 대한 행렬형태로 나타내는 것이 아니라, shape이랑 line, curves로 경계지어진 color 영역에 대한 이미지 표현          resolution independent하다는 점과, 고해상도로 displayed된다는 점이 장점이지만 display되기 이전에 rasterized되어야 한다는 점이 단점이다.        이번 챕터에서는 Raster Image를 다루며, 빛의 세기(light iintensity)와 연관된 pixel value를 어떻게 결정하는지에 대한 자세한 내용은 나중 챕터에서 다룰 예정이니 일단 기억해두기.1. Raster Devices  장치를 Raster한다는 것은 무슨 의미일까  input output을 간단한 조직도로 나타낸 사진은 아래와 같다.1.1. Displays  pixel value에 대한 고정된 2D array 기반으로 디스플레이되어지는데, 방식이 두 가지로 분류 가능함 1) Emissive(방출하는) Display : 픽셀들이 direct하게 조절가능한 light를 방출하는 방식   픽셀 자체가 광원이 됨  LED(Light-Emitting Diode)가 대표적인 예시2) Transmissive(투과하는) Display : 빛을 방출하진 않고, 대신에 투과할 수 있는 만큼의 빛을 픽셀이 갖고 있는 방식  transmissive display는 픽셀 행렬 뒤에 backlight가 필요함, 어느정도 illuminate(밝게 하다)될 만큼의 광원이 필요하기 때문  LCD(Light-Crystal Display)가 대표적인 예시    on-state(위, 전압 들어온 상태)가 되면 liquid crystal cell이 회전하면서 polarized light이 front plarizer를 투과할 수 있게 해주는 원리2. Images, Pixels, and Geometry  Raster Image는 이미지 픽셀별로 RGB 색상에 대한 값을 2차원 행렬 형태로 나타내는 것을 알고 있다.  이미지를 측정하거나 reproduce할 때, light energy에 대해 다음의 2차원 분포를 알아야 한다 ;                  light emitted from the monitor as a function of a position on the face of the display  = 디스플레이 표면에서 방출되는 빛                    light falling on a camera’s image sensor as a function of a position across the sensor’s plane  = 카메라의 이미지 센서로 들어가는 빛                    Reflectance(반사율) 또는 흡수되는 빛 대비 반사되는 빛의 비율(fraction)  = function of position on a piece of paper            We can abstract an “image” as a function $I(x,y)$: \\(I(x,y) : R -&gt; V\\)          grayscale 이미지인 경우에는 $V$가 양수구역이고, ideal한 color image인 경우에 $V$는 3차원 실수 공간영역이다.        continuous(연속적인)값으로 어떻게 Raster image가 표현되는 것인가.          Point sampling과 관련있다, 자세한 것은 chapter 9의 신호처리단원에서 다룬다고 한다      이미지의 색상에 대한 Local average를 pixel value정함      pixel value인 x를 구한다는 말은 즉, “the value of the image in the vincinity(주변) of the grid point is x”를 구한다는 것과 같은 말이다. (당연한거아님?)        이미지 width가 $n_x$, 높이가 $n_y$일 떄, image rectangular domain은 다음과 같이 평행이동을 0.5씩 했다면 top-right pixel도 똑같이 평행이동시킨 만큼으로 설정할 수 있다;\\(R = [-0.5, n_x - 0.5] \\times [-0.5, n_y - 0.5]\\)2.1. Pixel Values  Pixel formats :          비트 수가 줄어들면, artifact나 flaws(결함)부분이 이미지상에 생길 수도 있다      2.2. Monitor Intensities and Gamma  모니터는 pixel “value”를 digital input으로 받아서, “intensity” level(빛의 강도)로 변환함  인간의 강도에 대한 인지는 비선형적이라서 이 단원에서 논할 부분은 아님(chapter 20에 나옴)  모니터도 input에 대해서 non-linear하게 처리하는데, 예를 들어 모니터에게 0, 0.5, 1.0의 세 픽셀 value를 주었을 때, 디스플레이되는 intensity는 0, 0.025, 1.0으로 처리됨      이러한 근사 비선형성은 $ \\gamma $에 의해 결정됨\\(displayed-intensity = (maximum-intensity) a^\\gamma\\)    이 때, a는 아래 그림의 체커보드 이미지를 통한 standard technique을 통해 찾을 수 있는 값임 (calibration과 연관된 건가?)  “Gamma Correct”          gamma값을 알고 있으면, input을 감마 변형해서 $a = 0.5$ 즉, 흑백의 중간 강도에 대해 디스플레이되게 할 수 있음, 아래의 변형과정을 거친다      $a’ = a^{\\frac{1}{\\gamma}} $      \\[: displayed-intensity = (a')^{\\gamma} = (a^{\\frac{1}{\\gamma}})^{\\gamma}(maximum-intensity) = a(maximum-intensity)\\]            느낀점  pixel 정의나 이미지에 대한 matching function같은 기초적인 개념을 훑고 가는 시작 단원이다,, 별 내용이 없음  설날인데 여유롭게 카공도 하고 기분이 좋다.ㅋㅋ"
  },
  
  {
    "title": "Graphics STUDY",
    "url": "/posts/Graphics_study/",
    "categories": "Graphics, Study",
    "tags": "Graphics",
    "date": "2025-01-25 11:00:00 +0900",
    





    
    "snippet": "Fundamentals of Computer Graphics by Steve Marschner and Peter Shirley  교수님 추천 책, (“그래픽스에서 가장 유명한, 정석 같은 책이에요. 그래픽스의 거의 모든 분야 다룸”)  아직 차례만 보긴 했는데 깊게 알고싶었던 내용 총집합인 것 같아서 아주 마음에 든다Contents1. Introduc...",
    "content": "Fundamentals of Computer Graphics by Steve Marschner and Peter Shirley  교수님 추천 책, (“그래픽스에서 가장 유명한, 정석 같은 책이에요. 그래픽스의 거의 모든 분야 다룸”)  아직 차례만 보긴 했는데 깊게 알고싶었던 내용 총집합인 것 같아서 아주 마음에 든다Contents1. Introduction  1.1 Graphics Areas  1.2 Major Applications  1.3 Graphics APIs  1.5 Numerical Issues  1.6 Efficiency  1.7 Designing and Coding Graphics Programs 2. Miscellaneous Math  2.1 Sets and Mappings  2.2 Solving Quadratic Equations  2.3 Trigonometry  2.4 Vectors  2.5 Curves and Surfaces  2.6 Linear Interpolation  2.7 Triangles 3. Raster Images  3.1 Raster Devices  3.2 Images, Pixels, and Geometry  3.3 RGB Color  3.4 Alpha Compositing 4. Ray Tracing  4.1 The Basic Ray-Tracing Algorithm  4.2 Perspective  4.3 Computing Viewing Rays  4.4 Ray-Object Intersection  4.5 Shading  4.6 A Ray-Tracing Program  4.7 Shadows  4.8 Ideal Specular Reflection  4.9 Historical Notes5. Linear Algebra  5.1 Determinants  5.2 Matrices  5.3 Computing with Matrices and Determinants  5.4 Eigenvalues and Matrix Diagonalization 6 Transformation Matrices  6.1 2D Linear Transformations  6.2 3D Linear Transformations  6.3 Translation and Affine Transformations  6.4 Inverses of Transformation Matrices  6.5 Coordinate Transformations7 Viewing  7.1 Viewing Transformations  7.2 Projective Transformations  7.3 Perspective Projection  7.4 Some Properties of the Perspective Transform  7.5 Field-of-View8. The Graphics Pipeline  8.1 Rasterization  8.2 Operations Before and After Rasterization  8.3 Simple Antialiasing  8.4 Culling Primitives for Efficiency9. Signal Processing  9.1 Digital Audio: Sampling in 1D  9.2 Convolution  9.3 Convolution Filters  9.4 Signal Processing for Images  9.5 Sampling Theory10. Surface Shading  10.1 Diffuse Shading  10.2 Phong Shading  10.3 Artistic Shading11. Texture Mapping  11.1 Looking Up Texture Values  11.2 Texture Coordinate Functions  11.3 Antialiasing Texture Lookups  11.4 Applications of Texture Mapping  11.5 Procedural 3D Textures12 Data Structures for Graphics  12.1 Triangle Meshes  12.2 Scene Graphs  12.3 Spatial Data Structures  12.4 BSP Trees for Visibility  12.5 Tiling Multidimensional Arrays20단원까지 있는데 일단 리버털 끝나고나서 2월 말까지 여기까지 1회독 목표 학부 때 배운 패턴인식이랑 컴비때 배운 내용이 포함된게 많아서 복습할겸 모르는 내용 위주 공부해야겠다."
  },
  
  {
    "title": "MarchingCubes, SIGGRAPH 1987",
    "url": "/posts/MarchingCubes/",
    "categories": "Paper Review",
    "tags": "Mesh extraction, Graphics",
    "date": "2025-01-21 11:00:00 +0900",
    





    
    "snippet": "Abstract  output = 연속적인 density값을 갖는 surface의 triangle model  알고리즘 큰 흐름 : 3D medical data를 scan-line order에 처리한 후, 선형 보간(linear interpolation)을 이용해서 삼각형의 vertices(꼭짓점)을 구하는 것Introduction  mesh extr...",
    "content": "Abstract  output = 연속적인 density값을 갖는 surface의 triangle model  알고리즘 큰 흐름 : 3D medical data를 scan-line order에 처리한 후, 선형 보간(linear interpolation)을 이용해서 삼각형의 vertices(꼭짓점)을 구하는 것Introduction  mesh extraction하는 것은 medical image에서 굉장히 유용하게 많이 쓰임  새로운 3D surface construction 알고리즘인 “Marching Cube”는 연속적인 density surface를 가지는 물체에 대한 vertices를 추출함으로써 고해상도의 메쉬 만듦  3D Medical 알고리즘 흐름 :                            Data Acquistion          MR, CT, SPECT같은 기기로 환자 데이터 얻음                                      Image Processing          3D data의 전체적인 구조를 파악할 수 있는 기법 사용                                      Surface Construction          본 논문의 주제, 적절한 알고리즘 사용                    Display      Related Work      기존의 연구 (1) Connected Contour 알고리즘 : surface의 윤곽(contour)에서 시작하여 그것들이 서로 연속적인 삼각형이 되게 연결하는 접근법   -&gt; slice에 하나 이상의 contour가 존재해야하는데 이때 ambiguity(모호성)이 발생하여 정확도 떨어짐   -&gt; 원데이터의 inter-slice의 연결성을 무시함    (2) Cuberille 접근법 : cuberilles(작은 큐브인 voxel형태로 표현하는 구조)로부터 surface 구축하는 접근법   -&gt; 이 구조에서 gradient를 계산했을 때 그 값이 shading(그림자)영역의 지점을 찾는데에 이용되는데, 이게 정확하기가 쉽지 않음   -&gt; thresholding해서 3D space를 블록 단위 voxel처럼 표현하고 surface을 표현    (3) Ray casting   -&gt; 3차원 sensation을 생산하기 위해 motion에 의존함(?)   Marching Cubbe Algorithm (Method)  크게 2가지의 주요한 단계로 구성됨  divide-and-conquer 접근법  3D space상의 큐브 하나에서 다음 큐브로 넘어갈 때, surface가 어떻게 교차하는지를 찾는 것이 목표      이웃한 8개 픽셀(두 Slice 면의 4개 꼭짓점)로부터 logical한 cube가 Figure-2처럼 위치하게 세팅,        큐브의 꼭짓점(vertex) 데이터 값이 우리가 구성하는 surface의 vertex value값을 초과하면 1을 부여  마찬가지로 큐브 vertex value가 surface vertex value값보다 아래이면 0을 부여=&gt; surface의 외부에 존재하는 점 대략 이러한 과정으로 교차점들을 통해 surface를 복원하게 된다. 큐브마다 8개의 vertices &amp; 2개의 state(inside, outside)가 존재하기 때문에, 표면이 꼭짓점 1개 당 $2^8$가지 경우의 수로 교차될 수 있다.  해당 256가지 경우의 수에 대한 table을 만들 수 있으나 매우 따분하고 에러가 발생하기 쉽기 때문에 , 256가지 경우의 수를 14가지 패턴으로 줄일 수 있는 다음의 두 가지 대칭 속성을 이용함          큐브가 reverse로 뒤집혀도, surface value들은 그대로 동일하게 바뀌지 않음              Rotational 대칭                        ‘’‘(ex) 0번 패턴 : 모든 vertices가 0으로 선택된 케이스(혹은 모두 1) =&gt; 삼각형을 생산하지 않게 됨          1번 패턴 : surface가 1개의 vertex를 나머지 7개의 vertices에 대해 분리시킨 상태 =&gt; 작은 삼각형 1개           ….  이 때, 윗 그림처럼 8개 vertices(v1~v8)와 각 vertex 사이의 bit index 12개(e1~e12)를 numbering하여 edge intersection을 고려한다.  그 후 surface와 맞닿는 edge가 어떤 것인지 알았으면, 이 edge들 사이에서 linear interpolation(선형 보간)을 수행한다.  마지막 단계로 각 triangle vertex에 대한 unit normal(단위 법선벡터)를 계산한다.          이렇게 구한 normal로 Gouraud-shaded image를 렌더링할 때 사용됨, 즉 명암 넣기 단계임                  고러드 쉐이딩(Gouraud Shading)                    Details :                  surface 의 normal은, surface의 접선 방향에 대한 gradient vector이다.                      direction of gradient vector를 $\\vec{g}$로 표기\\[\\vec{g}(x,y,z) = \\Delta{\\vec{f}(x, y,z)}\\]                                    $\\Delta{\\vec{f}(x,y,z)}$ 를 구하기 위해 3방향에서의 gradient vector를 아래처럼 계산한 후에 선형보간을 하여 surface 복원    \\(G_x(i,j,k) = \\frac{D(i+1, j, k) - D(i-1, j, k)}{\\Delta{x}}\\)     \\(G_x(i,j,k) = \\frac{D(i, j+1, k) - D(i, j-1, k)}{\\Delta{x}}\\)    \\(G_x(i,j,k) = \\frac{D(i, j, k+1) - D(i, j, k-1)}{\\Delta{x}}\\)        In summary, marching cubes creates a surface from a three-dimensional set of data as follows: (논문 표현)          Read four slices into memory.      Scan two slices and create a cube from four neighbors on one slice and four neighbors on the next slice.      Calculate an index for the cube by comparing the eight density values at the cube vertices with the surface con- stant.      Using the index, look up the list of edges from a precal- culated table.      Using the densities at each edge vertex, find the surface- edge intersection via linear interpolation.      Calculate a unit normal at each cube vertex using central differences. Interpolate the normal to each triangle ver- tex.      Output the triangle vertices and vertex normals      Enhancements to the Basic Algorithm  "
  },
  
  {
    "title": "SuGaR, CVPR 2024",
    "url": "/posts/SuGaR/",
    "categories": "Paper Review",
    "tags": "Mesh reconstruction, 3D Gaussian Splatting",
    "date": "2025-01-20 11:00:00 +0900",
    





    
    "snippet": "Abstract  precise and extremely fast mesh extraction from 3DGS representation  state-of-the-art method on SDFs, while providing a better rendering qualityContributions  Regularization term         ...",
    "content": "Abstract  precise and extremely fast mesh extraction from 3DGS representation  state-of-the-art method on SDFs, while providing a better rendering qualityContributions  Regularization term          Gaussian Splatting 최적화 중의 loss term에 추가시켜서 3D Gaussian들이 표면에 잘 정렬되게 하는 역할      3D Gaussian의 surface geometry(density &amp; SDF)를 이용함        Refinement strategy          mesh에 있는 gaussian들 삼각형으로 묶어주면서 refinement함      Introduction  mesh extraction task는 3DGS representation의 explicit한 특성 때문에 더 어려움  3DGS가 잘 최적화되었으면, 가우시안들이 평평하고 표면에 잘 분포되어있다는 가정을 얻을 수 있음          이때 gaussian density와 관련된 geometry를 이용해서 Loss term에 추가하면서 gaussian으로부터 mesh추출이 더 쉬워지게 만듦      volumne density 사용?        Marching Cube 알고리즘 대신 Poisson Reconstruction 알고리즘 사용해서 point cloud로부터 mesh extraction을 했다Method1. Aligning the Gaussians with the Surface (=Regularization)  목표 : 3DGS가 잘 최적화되어있다는 가정 하에, Gaussian의 SDF(Signed Distance Function)를 이끌어내는 것      optimized된 가우시안으로부터 예측된 SDF와 실제의 SDF간의 차이를 최소화함으로써 가우시안들이 평평하게 surface align된 특성을 갖을 수 있도록 encourage        최적화된 Gaussian Splatting Scene이 주어져있는 상황에서 시작    Gaussian의 density function $d(p)$ :\\[d(p) = \\sum_g \\alpha_g exp(-\\frac{1}{2}(p-\\mu_g)^T\\sigma_g^{-1}(p-\\mu_g))\\]  &lt; Property 1 &gt;1️⃣surface에 가까운 point $p$에 가까이 위치한 Gaussian $g^*$이 density function $d(p)$에 기여하는 정도가 크다\\[g^* = \\arg\\min_g(p-\\mu_g)^T\\sigma_g^{-1}(p-\\mu_g)\\]해석 = g*말고 나머지 가우시안들, g들에 대한 밀도가 최소가 되게 하면 된다.⇒ 이 성질을 만족하면, 가우시안들이 scene에서 잘 spread되어있다, 즉 scene에 가우시안들이 잘 퍼져있다는 가정을 만족&lt; Property 2 &gt;2️⃣잘 최적화된 3DGS scene에서는 Gaussian들이 평평하다 = scaling factor 3방향 벡터 중에서 하나는 0에 가까워야 함, (길이 짧아야 함)\\[(p-\\mu_g)^T\\Sigma_g^{-1}(p-\\mu_g) \\approx \\frac{1}{s_g^2}&lt;p-\\mu_g, n_g&gt;\\][notation]  $s_g$: 가장 짧은 scaling factor  $n_g :$ scaling factor에 대응되는 축에 대한 방향, normal(법선 벡터)처럼 생각해도 됨⇒ 결과적으로 surface-align한 density function $\\overline{d}(p)$\\[\\overline{d}(p) = exp(-\\frac{1}{2s_{g^*}^2} &lt;p-\\mu_{g^*}, n_{g^*}&gt;^2)\\]  A : d(p) 밀도함수를 따르는 가우시안,  B : $\\overline{d}(p)$ 밀도함수를 따르는 가우시안&lt; Optimize Term &gt;  $|d(p) - \\overline{d}(p)|$: 위의 density volume을 이용한 optimization term 을 3DGS loss에 추가          밀도를 이용하는 지금 optimize term 일단 좋긴 좋은데,,,density말고 SDF(Signed Distance Function) 활용하는 것도 추가시키면 surface-align Gaussian을 얻는 것에 더 좋다고 함    - 평평한 가우시안이 주어졌을 때, 즉 Gaussian $g$의 scaling factor들이 $s_g$ = 0 인 상황에서, point $p$와 true surface와의 거리 : $|&lt;p-\\mu_{g’}, n_{g’}&gt;|$      \\[SDF : \\overline{f}(p) = \\pm s_{g^*}\\sqrt{-2log(\\overline{d}(p))}\\]\\[ideal-SDF : {f}(p) = \\pm s_{g^*}\\sqrt{-2log({d}(p))}\\]   $|\\overline{f}(p)-f(p)|$ : 위의 SDF를 이용한 optimization term을 통해 표면에 더 잘 정렬된 가우시안들을 얻을 수 있었다.   Regularization term $R$ :\\[R =  \\frac{1}{|P|} \\sum_{p \\in P} |\\overline{f}(p) - f(p)|\\]      SDF의 normal(법선 벡터)에 대한 regularization term도 있음\\[R_{Norm} = \\frac{1}{|P|}  \\sum_{p\\in P} || \\frac{\\nabla f(p)}{|| \\nabla f(p) ||^2} - n_{g^*} ||_2^2\\]  2. Efficient Mesh Extraction (=Poisson Reconstruction)  최적화된 3DGS scene에서 계산된 가우시안들의 density로부터 3D points를 일정 level set에 대하여 샘플링함 =&gt; point clouds 구함                  이 때, level set은 level parameter인 $\\lambda$에 의해 결정됨                          샘플링된 Points 기반으로 Poisson reconstruction 수행하여 mesh 추출    참고. Poisson Reconstruction 정리 글  3. Binding New Gaussians to the Mesh (=Refinement)  Barycentric 좌표계 :  삼각형 또는 다면체 내부의 점을 해당 도형의 꼭짓점에 대한 가중치로 표현하는 좌표계          (ex) 삼각형의 경우:                              삼각형의 꼭짓점을 A, B, C라 할 때, 내부의 임의의 점 P는 다음과 같이 표현됩니다:            $P = \\alpha A + \\beta B + \\gamma C$                                여기서 α,β,γ는 가중치로, 아래 조건을 만족합니다:            $\\alpha + \\beta + \\gamma = 1$                                    논문 표현 :    Also, the Gaussians have only 2 learnable scaling factors instead  of 3 and only 1 learnable 2D rotation encoded with a complex number rather than a quaternion, to keep the Gaussians flat and aligned with the mesh triangles.        quaternion이란: 3D 회전을 표현하기 위해 사용되는 수학적 구조로, 복소수의 확장된 형태    $q=w+xi+yj+zk$  Experiment  single GPU Nvidia Tesla V100 SXM2 32 Go느낀점  3DGS에서 거의 최초로 mesh reconstruction태스크를 수행해서 성능이 좋게 나왔다는 것이 의의  요즘 모델들의 거의 baseline 시초급(?)으로 봐도 무방하다  포아송 재건방법이 아직 뭔지 잘 모르겠다. marching cube공부할 때 같이 공부  어렵긴 한데 재밌다."
  },
  
  {
    "title": "GS2Mesh, ECCV 2024",
    "url": "/posts/GS2Mesh/",
    "categories": "Paper Review",
    "tags": "Mesh reconstruction, Surface reconstruction, 3D Gaussian Splatting",
    "date": "2025-01-19 15:00:00 +0900",
    





    
    "snippet": "GS2Mesh :Surface Reconstruction from Gaussian Splatting via Novel Stereo Views&lt; 선정 이유 &gt;  SuGaR (CVPR 2024)논문 이후로 유의미하게 3dgs의 mesh recon 태스크에서 성능이 sota달성하였다는 점,  baseline에 SuGaR가 존재한다는 점Abstra...",
    "content": "GS2Mesh :Surface Reconstruction from Gaussian Splatting via Novel Stereo Views&lt; 선정 이유 &gt;  SuGaR (CVPR 2024)논문 이후로 유의미하게 3dgs의 mesh recon 태스크에서 성능이 sota달성하였다는 점,  baseline에 SuGaR가 존재한다는 점Abstract  noisy한 3DGS representation으로부터 smooth한 3D mesh representation을 얻는 것 어려움  pre-trained stereo-matching model사용해서 scene에 대한 geometry 활용함          stereo-aligned 되어있는 image pair를 얻고, 이를 앞선 stereo matching모델에 넣어서 depth추출하여 geometry 이용함        more smoother, more accurate mesh extraction 가능          Mesh Extraction 알고리즘 : TSDF(Truncated Signed Distance Function) 이용한 Marching cube      Contributions  pre-trained stereo-matching 모델을 통해 Image pair의 geometry 활용해서 mesh reconstruction 태스크의 sota성능을 달성하였다.          사용한 스테레오 매칭 모델 : DLNR                  stereo calibrated image pair를 모델 input으로 사용하여 이미지 쌍에 대한  correspondence 문제를 해결 + depth 추출                      TSDF(Truncated Signed Distance Function)와 depth 정보를 활용해서 mesh recon하는 알고리즘 (marching cube기반) 사용하였다. Introduction  Gaussian의 explicit한 요소만으로 geometrically consistent한 surface를 추출하는 것은 어려움          image plane(2D)에 back projected 되었을때 best matching되게끔 최적화된 가우시안이기 때문에, mesh reconstruction 에서는 오히려 가우시안 representation이 단점이 됨            stereo-aligned된 이미지 쌍에서 stereo-matching 모델(DLNR)을 사용해서 정확한 depth 측정한 후, TSDF 활용한 Depth-fusion기반 mesh extraction 알고리즘 (Marching cube) 사용하는 파이프라인    데이터셋 : TnT(Tanks and Temples) &amp; DTU(작은 object mesh 데이터셋임) 에서 sota성능을 보여주었다.MethodStep 1. Scene Capture &amp; Pose Estimation  COLMAP의 SFM(Structure From Motion)을 통해 카메라 파라미터들을 얻어내고, sparse한 3D point cloud를 재건한다.Step 2. Stereo-aligned Image Pairs 생성  3DGS에서 photometric loss를 통해 최적화되어서 image plane에 back-projected된 가우시안들 기반으로 stereo-aligned(같은 베이스라인b 선상에 존재하는, 카메라 포즈(rotation, translation)은 그대로에 baseline b길이만큼만 떨어진) 한 쌍으로 만드는 과정을 수행한다.          \\[R_R = R_L\\]\\[T_R = T_L + (R_L \\times [b, 0, 0])\\]   수식 Notation 설명   - $R_R$: 오른쪽 카메라의 rotation matrix    - $R_L$: 왼쪽 카메라의 rotation matrix    - $T_R$: 오른쪽 카메라의 translation matrix    - $T_L$: 왼쪽 카메라의 translation matrix  Step 3. Stereo Depth Estimation  input : a pair of stereo-calibrated cameras      DLNR(High Frequency Stereo matching Network) 모델 이용        DLNR pipeline          multiscale decouple LSTM 구조를 따름 + Disparity Normalization 수행하는 것이 핵심      논문까지 자세힌 아직 안읽어봄, 걍 stereo matching model 성능 중에 sota라고 함            이 모델 output에다가 아래의 mask 2개 정도 추가시켜 reconstruction성능 향상              occlusion mask                  left-to-right disparity랑 right-to-left disparity 차이 이 두개 사이에서 thresholding해서 구해짐                    depth-shading                  stereo-matching error $\\epsilon(Z)$        \\[\\epsilon(Z) = \\frac{\\epsilon(d)}{f_x \\cdot B} Z^2\\]                  $Z$ :  ground-truth depth          $d$ : disparity          $f_x$ : 수평축 카메라 focal length       - error가 baseline B 길이 값이 커질수록, 즉 stereo-paired image 사이의 간격이 클수록 에러가 작아지는 반면, occlusion이 심해질 수 있음          4B ≤ Z ≤ 20B 에 속하는 깊이만을 고려함                    Step 4. Depth Fusion into Triangulated Surface (mesh)  추출된 depth 정보들을 TSDF(Truncated Signed Distance Function) 기반 mesh reconstrucction 방법에 통합시킨다                  TSDF Cube Model이란?        : 깊이 영상 으로부터 3차원 공간 표면을 효과적으로 표현하기 위해,        전체 공간을 일정한 크기의 정육면체 복셀(voxel)들로 구성된 커다란 하나의 큐브(cube)로 표현하고, 각 복셀에는 물체 표면과의 거리를 나타내는 TSDF값과 그 값의 신뢰도를 나타 내는 가중치(weight)를 함께 저장하는 방식                    etc에 TSDF 개념 간단하게 추가              Marching cube : mesh 만들어주는 알고리즘          sdf나 tsdf같은 함수 활용되며, surface representation 방식 기반으로 mesh 뽑아냄      Experiment  ground truth point cloud랑 reconstructed point cloud간의 Chamfer Distance(CD)계산을 통해 evaluation  Baseline : SuGaR(CVPR 2024), BakedSDF, Neuralangelo, VolSDF, NeuS, MVSformer,  데이터별로 실험 진행 얘기  평가 지표 : Chamfer-Distance(CD): 두 point cloud 집합간의 거리 측정, F1, AccuracyLimitation  오른쪽 스테레오 매칭 모델은 투명한 표면에서 어려움을 겪는다.  (왼쪽) 원래 학습 이미지에서 충분히 다루어지지 않은 영역에서 floater를 생성한다.  TSDF 퓨전은 큰 장면(넓은 baseline B)에 맞게 확장되지 않는다.etc., (Preliminaries)기초개념 정리기초 개념      Mesh란?    : 3D 공간상에 존재하는 점들(Vertex/ Point) 과 그 점 3 개의 집합인 면(Polygon/face)들로 이루어진 3D 공간 표현방법        Voxel이란?    :이미지의 pixel 처럼 3D 공간을 표현하기 위해서 3D공간을 작은 단위 공간으로 쪼갠 것 –&gt; 3차원 공간을 grid로 쪼갰다고 보면 편함  TSDF (Truncated Signed Distance Function)  3D scene reconstruction의 목적은 Surface 를 찾아 recon 하는 것인데 이때 surface 를 표현하는 함수를 SDF(Signed Distance Function) 라고 함      Voxel 형태로 단위공간을 나누어 surface 라고 판단되는 곳은 0, Surface 안쪽은 음수 , Surface 바깥쪽은 양수로 표현하는 방식      Marching Cube (SIGGRAPH 1987)  과정 다 생략하고 한 줄 요약 : 3D point cloud로부터 Mesh 생성 알고리즘2차원 image plane에서 물체가 빨간색 선처럼 생겼다고 생각해보자, (3차원에서는 voxel임)이것을 2차원 도트로 표현하면 아래와 같음⇒ 원형의 물체랑 너무 달라짐, 해상도 차이 발생, 모양 이상해지는 결과따라서, 마칭 큐브 알고리즘으로 surface reconstruction을 진행한다. (…TLDR)  About Marching Cube Algorithm, tistory=&gt; 마칭 큐브 알고리즘 다음 게시물에 정리 이어서,, 느낀점  Marching Cube 알고리즘부터 완벽하게 이해를 해보자          항상 간단한 정리글로만 읽고 넘어가니까 그냥 point cloud넣고 mesh뽑아주는 그래픽스 알고리즘이다정도라고만 알고 넘어가서 모호하다, 알짜배기를 모르는 느낌      SuGaR에서는 Poisson reconstruction기반의 mesh recon알고리즘을 썼다고 되어있었는데, 이게 나는 마칭 큐브랑 완전 다른 건 줄 알았는데 또 읽다보니 포아송재건도 마칭큐브기반이라는 소리도 있고 출처가 정확하지 않으니까 혼동된다, 그래서 Poisson Reconstruction 논문도 읽어야겠다        eccv논문인데 생각보다 노벨티가 뭐가 없다          그냥 stereo-matching 모델 써서 depth추출하고 이거 기반으로 point cloud를 더 정확하고 밀도있게 뽑아내고 그 후로는 그냥 알고리즘 사용해서 메쉬추출한건데,, 성능이 좋았다는게 신기하다.      대신 단점이 명확하다, stereo 기법이다보니 위의 triangulation사진을 보다시피 베이스라인 길이에 한정된 씬만 사용될 것이므로 넓은 즉 큰 반경의 scene에 대한 mesh recon은 잘 안될 것이다 (실제로 사용한 데이터셋들도 다 작은 object based 벤치마크들이다)      3DGS도 그렇고, mesh recon도 그렇고 고질적인 문제가 투명한 transparent한 물체가 잘 복원이 어렵다는 점인데, 이부분의 개선은 왜 안되고 있는지 렌더링 측면에서 공부를 좀 더 해봐야겠다.      "
  },
  
  {
    "title": "블로그 오픈",
    "url": "/posts/First_Blog/",
    "categories": "Blogging, Tutorial",
    "tags": "personal",
    "date": "2025-01-18 02:34:00 +0900",
    





    
    "snippet": "First Blog안녕~! 데스크탑 운영체제가 리눅스여서 도커 연동이 생각만큼 잘 되지 않아 깃헙 블로그를 만드는 걸 계속 미뤄왔었다. 오늘 맥북이 새로 와서 심심해가지고 블로그 미뤄뒀던 걸 다시 도전해봤다. 아직 favicon은 반영은 뭐땜시 아직 안되고 있고,, 그대로 테마 갖다 쓰는건데도 생각보다 뭐가 잘 안되서..생각보다 오래걸렸다. 글 쓰고 ...",
    "content": "First Blog안녕~! 데스크탑 운영체제가 리눅스여서 도커 연동이 생각만큼 잘 되지 않아 깃헙 블로그를 만드는 걸 계속 미뤄왔었다. 오늘 맥북이 새로 와서 심심해가지고 블로그 미뤄뒀던 걸 다시 도전해봤다. 아직 favicon은 반영은 뭐땜시 아직 안되고 있고,, 그대로 테마 갖다 쓰는건데도 생각보다 뭐가 잘 안되서..생각보다 오래걸렸다. 글 쓰고 올리는 건 자동화가 잘돼있어서 괜찮을 것 같아 꾸준히 스터디 겸 근황을 이곳에 올리도록 하겠다.맥도 아직 익숙하지 않아서 다소 헤맸지만 좀 더 익숙해지면 활용도가 매우 높을 것 같아 만족한다!  WELCOME MY GITHUB BLOG, I am a Master student in Computer Vision Lab, Korea UniversityNext time you visit our site, this place will be much more developed and awesome."
  }
  
]

